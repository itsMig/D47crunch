<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>D47crunch API documentation</title>
<meta name="description" content="Standardization and analytical error propagation of Δ47 and Δ48 clumped-isotope measurements …" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Fira+Mono&display=swap" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:1em;line-height:1.4em}code{font-family:'Fira Mono',monospace;background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}.css{line-height:1em}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML'></script>
<style>
pre code {
font-size: .75em;
}
pre code.csv {
padding: 0;
}
pre .csv {
line-height: 1.25;
}
code .hljs-selector-tag,
code .hljs-selector-class {
font-weight: normal;
}
code.csv .hljs-string,
code.csv .hljs-number {
color: #333;
}
code.csv .hljs-keyword,
code.csv .hljs-selector-tag,
code.csv .hljs-subst {
font-weight: normal;
}
h1, h2, h3, h4, h5 {
font-weight: 600;
}
blockquote {
border-left: 3px solid #DDD;
padding: 1em;
color : #AAA
}
</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>D47crunch</code></h1>
</header>
<section id="section-intro">
<p>Standardization and analytical error propagation of Δ<sub>47</sub> and Δ48 clumped-isotope measurements</p>
<p>Process and standardize carbonate and/or CO<sub>2</sub> clumped-isotope analyses,
from low-level data out of a dual-inlet mass spectrometer to final, “absolute”
Δ<sub>47</sub> and Δ<sub>48</sub> values with fully propagated analytical error estimates.</p>
<h2 id="usage">Usage</h2>
<h3 id="1-create-d47data-object">1. Create <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> object</h3>
<p>Start by creating a <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> object which will store and process your data.</p>
<pre><code class="language-python">import D47crunch 
foo = D47crunch.D47data(verbose = True)
</code></pre>
<p>The <code>verbose</code> keyword specifies whether to print out extra information when calling <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> methods.
This property may be arbitrarily changed using the <code>verbose</code> attribute of the resulting object:</p>
<pre><code class="language-python">foo.verbose = False
</code></pre>
<p>Even before importing any analyses, our <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> object has properties which may be inspected and/or edited:</p>
<h4 id="11-nominal-13cvpdb-18ovpdb-and-47-values-of-carbonate-standards">1.1 Nominal δ<sup>13</sup>C<sub>VPDB</sub>, δ<sup>18</sup>O<sub>VPDB</sub>, and Δ<sub>47</sub> values of carbonate standards</h4>
<p><code>foo.Nominal_d13C_VPDB</code> and <code>foo.Nominal_d18O_VPDB</code> are dictionaries storing the δ<sup>13</sup>C<sub>VPDB</sub> and δ<sup>18</sup>O<sub>VPDB</sub> values of carbonate standards. You may freely edit these values and/or which standards to consider:</p>
<pre><code class="language-python">print(foo.Nominal_d13C_VPDB)
# output: {'ETH-1': 2.02, 'ETH-2': -10.17, 'ETH-3': 1.71}

print(foo.Nominal_d18O_VPDB)
# {'ETH-1': -2.19, 'ETH-2': -18.69, 'ETH-3': -1.78}

foo.Nominal_d13C_VPDB['ETH-4'] = -10.20
foo.Nominal_d18O_VPDB['ETH-4'] = -18.81

print(foo.Nominal_d13C_VPDB)
# output: {'ETH-1': 2.02, 'ETH-2': -10.17, 'ETH-3': 1.71, 'ETH-4': -10.2}

print(foo.Nominal_d18O_VPDB)
# {'ETH-1': -2.19, 'ETH-2': -18.69, 'ETH-3': -1.78, 'ETH-4': -18.81}
</code></pre>
<p><code>foo.Nominal_D47</code> is another dictionary, storing the absolute Δ<sub>47</sub> values of the standards used to anchor your measurements to an absolute Δ<sub>47</sub> reference frame.
As above, you may feely edit these values:</p>
<pre><code class="language-python">print(foo.Nominal_D47)
# output: {'ETH-1': 0.258, 'ETH-2': 0.256, 'ETH-3': 0.691}

foo.Nominal_D47['ETH-4'] = 0.507

print(foo.Nominal_D47)
# output: {'ETH-1': 0.258, 'ETH-2': 0.256, 'ETH-3': 0.691, 'ETH-4': 0.507}
</code></pre>
<h4 id="12-oxygen-17-correction-parameters">1.2 Oxygen-17 correction parameters</h4>
<p>The oxygen-17 correction parameters used by <code><a title="D47crunch.D47data.crunch" href="#D47crunch.D4xdata.crunch">D4xdata.crunch()</a></code> (see below) are specified by <code>foo.R13_VPDB</code>, <code>foo.R17_VSMOW</code>, <code>foo.R18_VSMOW</code> and <code>foo.lambda_17</code>. Default values correspond to the IUPAC values as recommended by <a href="https://dx.doi.org/10.1016/j.chemgeo.2016.08.014">Daëron et al. (2016)</a> and
<a href="https://dx.doi.org/10.1002/rcm.7743">Schauer et al. (2016)</a>.</p>
<pre><code class="language-python">print(foo.R13_VPDB)  # -&gt; 0.01118    (Chang &amp; Li, 1990)
print(foo.R17_VSMOW) # -&gt; 0.00038475 (Assonov &amp; Brenninkmeijer, 2003, rescaled to R13_VPDB)
print(foo.R18_VSMOW) # -&gt; 0.0020052  (Baertschi, 1976)
print(foo.lambda_17) # -&gt; 0.528      (Barkan &amp; Luz, 2005)
</code></pre>
<p>As above, the values for these parameters may be arbitrarily redefined:</p>
<pre><code class="language-python"># to change the lambda value to 0.5164, leaving the other parameters unchanged:
foo.lambda_17 = 0.5164
</code></pre>
<h4 id="13-default-method-for-carbon-13-and-oxygen-18-standardization">1.3 Default method for carbon-13 and oxygen-18 standardization</h4>
<p>By default, bulk isotopic compositions are standardized using a “two-point” affine transformation (correcting for small offsets and stretching effects) based on the carbonate standards defined in <code>foo.Nominal_d13C_VPDB</code> and <code>foo.Nominal_d18O_VPDB</code> (see above).</p>
<p>Optionally, you may opt instead for a “single-point” standardization approach not correcting for strecthing effects, for instance if the cabonate standards in <code>foo.Nominal_d13C_VPDB</code> and <code>foo.Nominal_d18O_VPDB</code> cover only a small fraction of the full isotopic range of your measurements.</p>
<p>Finally, you may also opt to perform no <em>a posteriori</em> standardization of bulk isotopic compositions, which implies that the quality of your final δ<sup>13</sup>C and δ<sup>18</sup>O values will depend strongly on the accuracy of your working gas composition and the linearity of your instrument.</p>
<p>Switching betwwen these three options can be achieved by setting <code>foo.d13C_STANDARDIZATION_METHOD</code> and <code>foo.d18O_STANDARDIZATION_METHOD</code> to <code>'2pt'</code>, <code>'1pt'</code>, and <code>'none'</code>, respectively. Note that you may later override this default behavior on a per-session basis.</p>
<h4 id="14-oxygen-18-acid-fractionation-factor">1.4 Oxygen-18 acid fractionation factor</h4>
<p><code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> processing methods always return δ<sup>18</sup>O values of CO<sub>2</sub> analytes relative to VSMOW rather than carbonate δ<sup>18</sup>O<sub>VPDB</sub> values (which depend on sample mineralogies and acid reaction temperature). However, when using single-point or two-point δ<sup>18</sup>O standardization or when computing the bulk isotope composition of working gases based on carbonate standards (using <code><a title="D47crunch.D47data.wg" href="#D47crunch.D4xdata.wg">D4xdata.wg()</a></code>), it is necessary to specify the oxygen-18 fractionation factor associated with the phosphoric acid reaction, by setting the value of <code>foo.ALPHA_18O_ACID_REACTION</code>.</p>
<h3 id="2-import-data">2. Import data</h3>
<p>It's time to add some analyses to out <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> object.</p>
<p>Start with some raw data stored as CSV in a file named <code>rawdata.csv</code> (spaces after commas are optional). Each line corresponds to a single analysis.</p>
<p>The only required fields are a sample identifier (<code>Sample</code>), and the working-gas delta values <code>d45</code>, <code>d46</code>, <code>d47</code>. If no session information is provided, all analyses will be treated as belonging to a single analytical session. Alternatively, to group analyses into sessions, provide session identifiers in a <code>Session</code> field. If not specified by the user, a unique identifier (<code>UID</code>) will be assigned automatically to each analysis. Independently known oxygen-17 anomalies may be provided as <code>D17O</code> (in ‰ relative to VSMOW, with λ equal to <code><a title="D47crunch.D47data.lambda_17" href="#D47crunch.D4xdata.lambda_17">D47data.lambda_17</a></code>), and are assumed to be zero otherwise. Working-gas deltas <code>d48</code> and <code>d49</code> may also be provided, and are otherwise treated as <code>nan</code>.</p>
<p>Example <code>rawdata.csv</code> file:</p>
<pre><code class="language-csv">UID,  Session,  Sample,       d45,      d46,       d47,       d48,      d49
A01, Session1,   ETH-1,   5.79502, 11.62767,  16.89351,  24.56708,  0.79486
A02, Session1, IAEA-C1,   6.21907, 11.49107,  17.27749,  24.58270,  1.56318
A03, Session1,   ETH-2,  -6.05868, -4.81718, -11.63506, -10.32578,  0.61352
A04, Session1, IAEA-C2,  -3.86184,  4.94184,   0.60612,  10.52732,  0.57118
A05, Session1,   ETH-3,   5.54365, 12.05228,  17.40555,  25.96919,  0.74608
A06, Session1,   ETH-2,  -6.06706, -4.87710, -11.69927, -10.64421,  1.61234
A07, Session1,   ETH-1,   5.78821, 11.55910,  16.80191,  24.56423,  1.47963
A08, Session1, IAEA-C2,  -3.87692,  4.86889,   0.52185,  10.40390,  1.07032
A09, Session1,   ETH-3,   5.53984, 12.01344,  17.36863,  25.77145,  0.53264
A10, Session1, IAEA-C1,   6.21905, 11.44785,  17.23428,  24.30975,  1.05702
A11, Session2,   ETH-1,   5.79958, 11.63130,  16.91766,  25.12232,  1.25904
A12, Session2, IAEA-C1,   6.22514, 11.51264,  17.33588,  24.92770,  2.54331
A13, Session2,   ETH-2,  -6.03042, -4.74644, -11.52551, -10.55907,  0.04024
A14, Session2, IAEA-C2,  -3.83702,  4.99278,   0.67529,  10.73885,  0.70929
A15, Session2,   ETH-3,   5.53700, 12.04892,  17.42023,  26.21793,  2.16400
A16, Session2,   ETH-2,  -6.06820, -4.84004, -11.68630, -10.72563,  0.04653
A17, Session2,   ETH-1,   5.78263, 11.57182,  16.83519,  25.09964,  1.26283
A18, Session2, IAEA-C2,  -3.85355,  4.91943,   0.58463,  10.56221,  0.71245
A19, Session2,   ETH-3,   5.52227, 12.01174,  17.36841,  26.19829,  1.03740
A20, Session2, IAEA-C1,   6.21937, 11.44701,  17.26426,  24.84678,  0.76866
</code></pre>
<p>Reading data from <code>rawdata.csv</code> can be done with <code>foo.read()</code>:</p>
<pre><code class="language-python">foo.read('rawdata.csv')

print('foo contains:')
print(f'{len(foo)} analyses')
print(f'{len({r[&quot;Sample&quot;] for r in foo})} samples')
print(f'{len({r[&quot;Session&quot;] for r in foo})} sessions')

# output:
# foo contains:
# 20 analyses
# 5 samples
# 2 sessions
</code></pre>
<p>At this stage, <code>foo</code> behaves like a <code>list</code> object. Yoy may slice it in the usual way (<code>foo[:10]</code> returns a list of the first 10 analyses) and use built-in methods in the expected way (e.g., <code>len(foo)</code> is equal to 20).</p>
<p>We can inspect the first record now stored in <code>foo</code>, corresponding to a single analysis:</p>
<pre><code class="language-python">r = foo[0]
for k in r:
    print(f'r[&quot;{k}&quot;] = {repr(r[k])}')

# output:
# r[&quot;UID&quot;] = 'A01'
# r[&quot;Session&quot;] = 'Session1'
# r[&quot;Sample&quot;] = 'ETH-1'
# r[&quot;d45&quot;] = 5.79502
# r[&quot;d46&quot;] = 11.62767
# r[&quot;d47&quot;] = 16.89351
# r[&quot;d48&quot;] = 24.56708
# r[&quot;d49&quot;] = 0.79486
</code></pre>
<h4 id="21-sessions">2.1 Sessions</h4>
<p>After importing records from <code>rawdata.csv</code>, our <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> object now has a new dictionary attribute, <code>foo.sessions</code>:</p>
<pre><code class="language-python">for session in foo.sessions:
    print(f&quot;{session:&gt;28}:&quot;)
    for k in foo.sessions[session]:
        if k == 'data':
            print(f&quot;{k:&gt;28}: [...] (too large to print)&quot;)
        else:
            print(f&quot;{k:&gt;28}: {foo.sessions[session][k]}&quot;)
    print()
# output:
#                     Session1:
#                         data: [...] (too large to print)
#             scrambling_drift: False
#                  slope_drift: False
#                     wg_drift: False
#  d13C_standardization_method: 2pt
#  d18O_standardization_method: 2pt
# 
#                     Session2:
#                         data: [...] (too large to print)
#             scrambling_drift: False
#                  slope_drift: False
#                     wg_drift: False
#  d13C_standardization_method: 2pt
#  d18O_standardization_method: 2pt
</code></pre>
<p>Each session in <code>foo.sessions</code> has the following attributes at this stage:</p>
<ul>
<li><code>data</code>: list of all the analyses in this session</li>
<li><code>scrambling_drift</code>, <code>slope_drift</code>, <code>wg_drift</code>: whether parameters <code>a</code>, <code>b</code>,<code>c</code> of the Δ<sub>47</sub> standardization model are allowed to drift (change linearly with with time).</li>
<li><code>d13C_standardization_method</code>, <code>d18O_standardization_method</code>: which method to use for this session.</li>
</ul>
<p>You may arbitrarily edit the values of <code>d13C_standardization_method</code>, <code>d18O_standardization_method</code> for any session, which will affect the results of <code>foo.crunch()</code>.</p>
<p>Similarly, you may arbitrarily edit the values of <code>scrambling_drift</code>, <code>slope_drift</code>, <code>wg_drift</code> for any session, which will affect the results of <code>foo.standardize()</code>.</p>
<h4 id="22-samples-anchors-and-unknowns">2.2 Samples, anchors and unknowns</h4>
<h3 id="3-working-gas-composition">3. Working gas composition</h3>
<p>There are two ways to define the isotpic composition of the working gas.</p>
<h4 id="31-option-1-explicit-definition">3.1 Option 1: explicit definition</h4>
<p>Directly writing to fields <code>d13Cwg_VPDB</code> and <code>d18Owg_VSMOW</code>:</p>
<pre><code class="language-python">for r in foo:
    if r['Session'] == 'Session1':
        r['d13Cwg_VPDB'] = -3.75
        r['d18Owg_VSMOW'] = 25.14
    elif r['Session'] == 'Session2':
        r['d13Cwg_VPDB'] = -3.74
        r['d18Owg_VSMOW'] = 25.17
</code></pre>
<h4 id="32-option-2-based-on-the-known-composition-of-a-sample">3.2 Option 2: based on the known composition of a sample:</h4>
<pre><code class="language-python"># The 2 code lines below are the default settings. It is thus not
# necessary to include them unless you wish to use different values.

foo.SAMPLE_CONSTRAINING_WG_COMPOSITION = ('ETH-3', 1.71, -1.78)
foo.ALPHA_18O_ACID_REACTION = 1.00813 # (Kim et al., 2007), calcite at 90 °C

# Compute the WG composition for each session:
foo.wg()

</code></pre>
<h3 id="4-crunch-the-data">4. Crunch the data</h3>
<p>Now compute δ<sup>13</sup>C, δ<sup>18</sup>Ο, and raw Δ<sub>47</sub>, Δ<sub>48</sub>, Δ<sub>49</sub> values. Note that δ<sup>18</sup>Ο is the CO<sub>2</sub> composition. The user is responsible for any acid fractionation correction.</p>
<pre><code class="language-python">foo.crunch()

r = foo[0]
for k in r:
    print(f'r[&quot;{k}&quot;] = {r[k]}')

# output:
# r[&quot;UID&quot;] = A01
# r[&quot;Session&quot;] = Session1
# r[&quot;Sample&quot;] = ETH-1
# r[&quot;d45&quot;] = 5.79502
# r[&quot;d46&quot;] = 11.62767
# r[&quot;d47&quot;] = 16.89351
# r[&quot;d48&quot;] = 24.56708
# r[&quot;d49&quot;] = 0.79486
# r[&quot;d13Cwg_VPDB&quot;] = -3.7555729459832765
# r[&quot;d18Owg_VSMOW&quot;] = 25.1145492463934
# r[&quot;D17O&quot;] = 0.0
# r[&quot;d13C_VPDB&quot;] = 1.9948594073404546
# r[&quot;d18O_VSMOW&quot;] = 37.03357105550355
# r[&quot;D47raw&quot;] = -0.5746856128030498
# r[&quot;D48raw&quot;] = 1.1496833191546596
# r[&quot;D49raw&quot;] = -27.690248970251407
</code></pre>
<h3 id="6-standardization">6. Standardization</h3>
<h4 id="61-default-approach-pooled">6.1 Default approach (<span style="text-transform:lowercase"><code>pooled</code></span>)</h4>
<p>The default standardization approach computes the best-fit standardization parameters (a,b,c) for each session, along with the best-fit Δ<sub>47</sub> values of unknown samples, using a pooled regression model taking into account the relative mapping of all samples (anchors and unknowns) in (δ<sub>47</sub>, Δ<sub>47</sub>) space.</p>
<pre><code class="language-python">foo.standardize()
foo.table_of_sessions(verbose = True, save_to_file = False)
foo.table_of_samples(verbose = True, save_to_file = False)

</code></pre>
<p>The following text is output:</p>
<pre><code class="language-csv">[table_of_sessions]
–––––––––––––––––––––––––––––––  –––––––––––
N samples (anchors + unknowns)     5 (3 + 2)
N analyses (anchors + unknowns)  20 (12 + 8)
Repeatability of δ13C_VPDB          13.8 ppm
Repeatability of δ18O_VSMOW         41.9 ppm
Repeatability of Δ47 (anchors)      13.1 ppm
Repeatability of Δ47 (unknowns)      3.4 ppm
Repeatability of Δ47 (all)           9.6 ppm
Model degrees of freedom                  12
Student's 95% t-factor                  2.18
Standardization method                pooled
–––––––––––––––––––––––––––––––  –––––––––––

[table_of_sessions]
––––––––  ––  ––  –––––––––––  ––––––––––––  ––––––  ––––––  ––––––  –––––––––––––  –––––––––––––  ––––––––––––––
Session   Na  Nu  d13Cwg_VPDB  d18Owg_VSMOW  r_d13C  r_d18O   r_D47         a ± SE   1e3 x b ± SE          c ± SE
––––––––  ––  ––  –––––––––––  ––––––––––––  ––––––  ––––––  ––––––  –––––––––––––  –––––––––––––  ––––––––––––––
Session1   6   4       -3.756        25.115  0.0035  0.0415  0.0066  0.838 ± 0.016  3.340 ± 0.247  -0.859 ± 0.007
Session2   6   4       -3.743        25.118  0.0174  0.0490  0.0119  0.815 ± 0.015  4.601 ± 0.246  -0.847 ± 0.007
––––––––  ––  ––  –––––––––––  ––––––––––––  ––––––  ––––––  ––––––  –––––––––––––  –––––––––––––  ––––––––––––––


[table_of_samples] 
–––––––  –  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
Sample   N  d13C_VPDB  d18O_VSMOW     D47      SE    95% CL      SD  p_Levene
–––––––  –  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
ETH-1    4       2.00       37.00  0.2580                    0.0096          
ETH-2    4     -10.03       20.18  0.2560                    0.0154          
ETH-3    4       1.71       37.45  0.6910                    0.0039          
IAEA-C1  4       2.46       36.88  0.3624  0.0061  ± 0.0133  0.0031     0.901
IAEA-C2  4      -8.04       30.19  0.7246  0.0082  ± 0.0178  0.0037     0.825
–––––––  –  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
</code></pre>
<h4 id="62-d47datasessions">6.2 <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a>.sessions</code></h4>
<p>Under the hood, the normalization step does many things. It stores session information in <code>foo.sessions</code>:</p>
<pre><code class="language-python">print([k for k in foo.sessions])
# output: ['Session1', 'Session2']

for k in foo.sessions['Session1']:
    if k == 'data':
        print(f&quot;{k:&gt;16}: [...] (too large to print)&quot;)
    else:
        print(f&quot;{k:&gt;16}: {foo.sessions['Session1'][k]}&quot;)
# output:
#            data: [...] (too large to print)
# scrambling_drift: False
#      slope_drift: False
#         wg_drift: False
#      d13Cwg_VPDB: -3.7555729339153743
#     d18Owg_VSMOW: 25.11497520475171
#               Na: 6
#               Nu: 4
#      r_d13C_VPDB: 0.0035270930676685897
#     r_d18O_VSMOW: 0.04146501520018946
#            r_D47: 0.006638319178058144
#               Np: 3
#                a: 0.8381700110925523
#             SE_a: 0.015603757788793743
#                b: 0.003340175397346955
#             SE_b: 0.0002474062198065805
#                c: -0.8586981978192628
#             SE_c: 0.006737855663518676
#               a2: 0.0
#            SE_a2: 0.0
#               b2: 0.0
#            SE_b2: 0.0
#               c2: 0.0
#            SE_c2: 0.0
#               CM: [...] (6x6 numpy.Array())
</code></pre>
<p>each element of <code>foo.sessions</code> has the following attributes:</p>
<ul>
<li><code>data</code>: list of all the analyses in this session</li>
<li><code>scrambling_drift</code>, <code>slope_drift</code>, <code>wg_drift</code>: whether parameters <code>a</code>, <code>b</code>,<code>c</code> are allowed to drift (change linearly with with time)</li>
<li><code>d13Cwg_VPDB</code>, <code>d18Owg_VSMOW</code>: working gas composition</li>
<li><code>Na</code>: number of anchor analyses in this session</li>
<li><code>Nu</code>: number of unknown analyses in this session</li>
<li><code>r_d13C_VPDB</code>, <code>r_d18O_VSMOW</code>, <code>r_D47</code>: repeatabilities for <code>d13C_VPDB</code>, <code>d18O_VSMOW</code>, <code>D47</code> in this session</li>
<li><code>a</code>,<code>SE_a</code>: best-fit value and model SE of scrambling factor</li>
<li><code>b</code>,<code>SE_b</code>: best-fit value and model SE of compositional slope</li>
<li><code>c</code>,<code>SE_c</code>: best-fit value and model SE of working gas offset</li>
<li><code>a2</code>,<code>b2</code>,<code>c2</code>: drift rates (per unit of <code>TimeTag</code>) of <code>a</code>,<code>b</code>, <code>c</code>. If <code>TimeTag</code> is one of the fields in the raw data, this will be used, otherwise <code>TimeTag</code> starts at 0 for each session and increases by 1 for each analysis, in the listed order (thus beware of datasets ordered by sample name).</li>
<li><code>CM</code>: the covariance matrix of (<code>a</code>, <code>b</code>, <code>c</code>, <code>a2</code>, <code>b2</code>, <code>c2</code>).</li>
</ul>
<h4 id="63-d47datasamples-d47dataanchors-and-d47dataunknowns">6.3 <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a>.samples</code>, <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a>.anchors</code>, and <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a>.unknowns</code></h4>
<p>Additional information about the samples is stored in <code>foo.samples</code> (the same information can also be accessed via <code>foo.anchors</code> and <code>foo.unknowns</code>):</p>
<pre><code class="language-python">print([k for k in foo.samples])
# output:
# ['ETH-1', 'ETH-2', 'ETH-3', 'IAEA-C1', 'IAEA-C2']

for k in foo.samples['IAEA-C1']:
    if k == 'data':
        print(f&quot;{k:&gt;12}: [...] (too large to print)&quot;)
    else:
        print(f&quot;{k:&gt;12}: {foo.samples['IAEA-C1'][k]}&quot;)
# output:
#         data: [...] (too large to print)
#            N: 4
#       SD_D47: 0.0031207941052170305
#    d13C_VPDB: 2.460639104889639
#   d18O_VSMOW: 36.87725533010137
#     p_Levene: 0.901152441112675
#          D47: 0.3624187694150056
#       SE_D47: 0.00610711296513016
</code></pre>
<p>Each element of <code>foo.samples</code> has the following attributes:</p>
<ul>
<li><code>N</code>: total number of analyses in the whole data set</li>
<li><code>SD_D47</code>: the sample SD of Δ<sub>47</sub> for this sample</li>
<li><code>d13C_VPDB</code>, <code>d18O_VSMOW</code>: average δ<sup>13</sup>C, δ<sup>18</sup>Ο values for the analyte CO<sub>2</sub>.</li>
<li><code>D47</code>, <code>SE_D47</code>: best-fit value and model SE for the Δ<sub>47</sub> of this sample</li>
<li><code>p_Levene</code>: p-value for a <a href="https://en.wikipedia.org/wiki/Levene%27s_test">Levene test</a> of whether the observed Δ<sub>47</sub> variance for this sample is significantly larger than that for ETH-3 (to change the reference sample to compare with, e.g. to ETH-1: <code>foo.LEVENE_REF_SAMPLE = 'ETH-1'</code> before calling <code>foo.normalize()</code>).</li>
</ul>
<h4 id="64-d47datarepeatability">6.4 <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a>.()repeatability</code></h4>
<p>The overall analytical repeatabilities are now saved to <code>foo.repeatability</code>:</p>
<pre><code class="language-python">for k in foo.repeatability:
    print(f&quot;{k:&gt;12}: {foo.repeatability[k]}&quot;)

# output:
#  r_d13C_VPDB: 0.013821704833171146
# r_d18O_VSMOW: 0.04191487414887982
#       r_D47a: 0.010690471302409636
#       r_D47u: 0.0034370447628642863
#        r_D47: 0.008561367687546161
</code></pre>
<ul>
<li><code>r_d13C_VPDB</code>: Analytical repeatability of δ<sup>13</sup>C for all samples</li>
<li><code>r_d18O_VSMOW</code>: Analytical repeatability of δ<sup>18</sup>O for all samples (CO<sub>2</sub> values)</li>
<li><code>r_D47a</code>: Analytical repeatability of Δ<sub>47</sub> for anchor samples only</li>
<li><code>r_D47u</code>: Analytical repeatability of Δ<sub>47</sub> for unknown samples only</li>
<li><code>r_D47</code>: Analytical repeatability of Δ<sub>47</sub> for all samples.</li>
</ul>
<h4 id="65-d47dataresult">6.5 <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a>.()result</code></h4>
<p>By default <code>foo.normalize()</code> uses the <a href="https://lmfit.github.io/lmfit-py/fitting.html#lmfit.minimizer.Minimizer"><code>lmfit.Minimizer.leastsq()</code></a> method, which returns an instance of <a href="https://lmfit.github.io/lmfit-py/fitting.html#lmfit.minimizer.MinimizerResult"><code>lmfit.MinimizerResult</code></a>. This <code>MinimizerResult</code>instance is stored in <code>foo.result</code>. A detailed report may be printed using <code>foo.report()</code></p>
<pre><code class="language-python">print(type(foo.normalization))
# output:
# &lt;class 'lmfit.minimizer.MinimizerResult'&gt;
</code></pre>
<h4 id="66-combining-information-from-carbonate-anchors-and-equilibrated-gases">6.6 Combining information from carbonate anchors and equilibrated gases</h4>
<p>The <code>constraints</code> argument to <code><a title="D47crunch.D47data.standardize" href="#D47crunch.D4xdata.standardize">D4xdata.standardize()</a></code> in the pooled regression approach may be used to specify arbitrary constraints between regression model parameters. For instance, if a data set comprises two carbonate standards (<code>ETH-1</code> and <code>ETH-2</code>) and two gas standards (<code>HG-1000C</code> and <code>EG-25C</code>), it is possible to specify the Δ<sub>47</sub> difference between <code>HG-1000C</code> and <code>EG-25C</code> explicitly, essentially constraining the scrambling factor <code>a</code> based on the gas standards while constraining the other parameters based on <code>ETH-1</code> and <code>ETH-2</code>:</p>
<pre><code class="language-python">from D47crunch import D47data, fCO2eqD47_Wang

rawdata = D47data()
rawdata.read('foo.csv') # foo.csv not provided in this example
rawdata.wg()
rawdata.crunch()
constr = {'D47_EG_25C': f'D47_HG_1000C + {fCO2eqD47_Wang(25)-fCO2eqD47_Wang(1000)}')
rawdata.standardize(constraints = constr)
rawdata.table_of_samples()

# outputs something like:
# 
# [table_of_samples] 
# ––––––––  –––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
# Sample      N  d13C_VPDB  d18O_VSMOW     D47      SE    95% CL      SD  p_Levene
# ––––––––  –––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
# ETH-1     205       2.03       37.03  0.2052                    0.0089          
# ETH-2     213     -10.17       19.88  0.2085                    0.0078          
# EG-25C    138     -18.43       40.35  0.9195  0.0000  ± 0.0000  0.0095     1.000
# HG-1000C  180      -7.95       26.50  0.0242  0.0007  ± 0.0014  0.0085     0.367
# ––––––––  –––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
</code></pre>
<p>Note that in the example abobe, because <code>HG-1000C</code> was constrained as a function of <code>EG-25C</code>, the SE in its Δ<sub>47</sub> value is not reported. For now, it must instead be computed based on that of <code>EG-25C</code> (in this simple case, the two standard errors are identical).</p>
<h4 id="67-legacy-standardization-approach-indep_sessions">6.7 Legacy standardization approach (<span style="text-transform:lowercase"><code>indep_sessions</code></span>)</h4>
<p>Following a more traditional approach, <code>foo.standardize(method = 'indep_sessions')</code> computes the best-fit standardization parameters (a,b,c) for each session using independent regression models (one per session) only taking into account the anchor samples (samples defined in <code>foo.Nominal_D47</code>), then computes the Δ<sub>47</sub> value for each analysis and
the weighted average Δ<sub>47</sub> value for each unknown sample.</p>
<h3 id="7-viewing-and-saving-the-results">7. Viewing and saving the results</h3>
<blockquote>
<p>under construction</p>
</blockquote>
<hr>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#! /usr/bin/env python3
&#39;&#39;&#39;
Standardization and analytical error propagation of Δ47 and Δ48 clumped-isotope measurements

Process and standardize carbonate and/or CO&lt;sub&gt;2&lt;/sub&gt; clumped-isotope analyses,
from low-level data out of a dual-inlet mass spectrometer to final, “absolute”
Δ&lt;sub&gt;47&lt;/sub&gt; and Δ&lt;sub&gt;48&lt;/sub&gt; values with fully propagated analytical error estimates.

.. include:: ../docs/documentation.md
&#39;&#39;&#39;

__author__    = &#39;Mathieu Daëron&#39;
__contact__   = &#39;daeron@lsce.ipsl.fr&#39;
__copyright__ = &#39;Copyright (c) 2021 Mathieu Daëron&#39;
__license__   = &#39;Modified BSD License - https://opensource.org/licenses/BSD-3-Clause&#39;
__date__      = &#39;2021-07-21&#39;
__version__   = &#39;2.0-beta&#39;

import os
import numpy as np
from statistics import stdev
from scipy.stats import t as tstudent
from scipy.stats import levene
from scipy.interpolate import interp1d
from numpy import linalg
from lmfit import Minimizer, Parameters, report_fit
from matplotlib import pyplot as ppl
from datetime import datetime as dt
from functools import wraps
from colorsys import hls_to_rgb
from matplotlib import rcParams

rcParams[&#39;font.family&#39;] = &#39;sans-serif&#39;
rcParams[&#39;font.sans-serif&#39;] = &#39;Helvetica&#39;
rcParams[&#39;font.size&#39;] = 10
rcParams[&#39;mathtext.fontset&#39;] = &#39;custom&#39;
rcParams[&#39;mathtext.rm&#39;] = &#39;sans&#39;
rcParams[&#39;mathtext.bf&#39;] = &#39;sans:bold&#39;
rcParams[&#39;mathtext.it&#39;] = &#39;sans:italic&#39;
rcParams[&#39;mathtext.cal&#39;] = &#39;sans:italic&#39;
rcParams[&#39;mathtext.default&#39;] = &#39;rm&#39;
rcParams[&#39;xtick.major.size&#39;] = 4
rcParams[&#39;xtick.major.width&#39;] = 1
rcParams[&#39;ytick.major.size&#39;] = 4
rcParams[&#39;ytick.major.width&#39;] = 1
rcParams[&#39;axes.grid&#39;] = False
rcParams[&#39;axes.linewidth&#39;] = 1
rcParams[&#39;grid.linewidth&#39;] = .75
rcParams[&#39;grid.linestyle&#39;] = &#39;-&#39;
rcParams[&#39;grid.alpha&#39;] = .15
rcParams[&#39;savefig.dpi&#39;] = 150

Petersen_etal_CO2eqD47 = np.array([[-12, 1.147113572], [-11, 1.139961218], [-10, 1.132872856], [-9, 1.125847677], [-8, 1.118884889], [-7, 1.111983708], [-6, 1.105143366], [-5, 1.098363105], [-4, 1.091642182], [-3, 1.084979862], [-2, 1.078375423], [-1, 1.071828156], [0, 1.065337360], [1, 1.058902349], [2, 1.052522443], [3, 1.046196976], [4, 1.039925291], [5, 1.033706741], [6, 1.027540690], [7, 1.021426510], [8, 1.015363585], [9, 1.009351306], [10, 1.003389075], [11, 0.997476303], [12, 0.991612409], [13, 0.985796821], [14, 0.980028975], [15, 0.974308318], [16, 0.968634304], [17, 0.963006392], [18, 0.957424055], [19, 0.951886769], [20, 0.946394020], [21, 0.940945302], [22, 0.935540114], [23, 0.930177964], [24, 0.924858369], [25, 0.919580851], [26, 0.914344938], [27, 0.909150167], [28, 0.903996080], [29, 0.898882228], [30, 0.893808167], [31, 0.888773459], [32, 0.883777672], [33, 0.878820382], [34, 0.873901170], [35, 0.869019623], [36, 0.864175334], [37, 0.859367901], [38, 0.854596929], [39, 0.849862028], [40, 0.845162813], [41, 0.840498905], [42, 0.835869931], [43, 0.831275522], [44, 0.826715314], [45, 0.822188950], [46, 0.817696075], [47, 0.813236341], [48, 0.808809404], [49, 0.804414926], [50, 0.800052572], [51, 0.795722012], [52, 0.791422922], [53, 0.787154979], [54, 0.782917869], [55, 0.778711277], [56, 0.774534898], [57, 0.770388426], [58, 0.766271562], [59, 0.762184010], [60, 0.758125479], [61, 0.754095680], [62, 0.750094329], [63, 0.746121147], [64, 0.742175856], [65, 0.738258184], [66, 0.734367860], [67, 0.730504620], [68, 0.726668201], [69, 0.722858343], [70, 0.719074792], [71, 0.715317295], [72, 0.711585602], [73, 0.707879469], [74, 0.704198652], [75, 0.700542912], [76, 0.696912012], [77, 0.693305719], [78, 0.689723802], [79, 0.686166034], [80, 0.682632189], [81, 0.679122047], [82, 0.675635387], [83, 0.672171994], [84, 0.668731654], [85, 0.665314156], [86, 0.661919291], [87, 0.658546854], [88, 0.655196641], [89, 0.651868451], [90, 0.648562087], [91, 0.645277352], [92, 0.642014054], [93, 0.638771999], [94, 0.635551001], [95, 0.632350872], [96, 0.629171428], [97, 0.626012487], [98, 0.622873870], [99, 0.619755397], [100, 0.616656895], [102, 0.610519107], [104, 0.604459143], [106, 0.598475670], [108, 0.592567388], [110, 0.586733026], [112, 0.580971342], [114, 0.575281125], [116, 0.569661187], [118, 0.564110371], [120, 0.558627545], [122, 0.553211600], [124, 0.547861454], [126, 0.542576048], [128, 0.537354347], [130, 0.532195337], [132, 0.527098028], [134, 0.522061450], [136, 0.517084654], [138, 0.512166711], [140, 0.507306712], [142, 0.502503768], [144, 0.497757006], [146, 0.493065573], [148, 0.488428634], [150, 0.483845370], [152, 0.479314980], [154, 0.474836677], [156, 0.470409692], [158, 0.466033271], [160, 0.461706674], [162, 0.457429176], [164, 0.453200067], [166, 0.449018650], [168, 0.444884242], [170, 0.440796174], [172, 0.436753787], [174, 0.432756438], [176, 0.428803494], [178, 0.424894334], [180, 0.421028350], [182, 0.417204944], [184, 0.413423530], [186, 0.409683531], [188, 0.405984383], [190, 0.402325531], [192, 0.398706429], [194, 0.395126543], [196, 0.391585347], [198, 0.388082324], [200, 0.384616967], [202, 0.381188778], [204, 0.377797268], [206, 0.374441954], [208, 0.371122364], [210, 0.367838033], [212, 0.364588505], [214, 0.361373329], [216, 0.358192065], [218, 0.355044277], [220, 0.351929540], [222, 0.348847432], [224, 0.345797540], [226, 0.342779460], [228, 0.339792789], [230, 0.336837136], [232, 0.333912113], [234, 0.331017339], [236, 0.328152439], [238, 0.325317046], [240, 0.322510795], [242, 0.319733329], [244, 0.316984297], [246, 0.314263352], [248, 0.311570153], [250, 0.308904364], [252, 0.306265654], [254, 0.303653699], [256, 0.301068176], [258, 0.298508771], [260, 0.295975171], [262, 0.293467070], [264, 0.290984167], [266, 0.288526163], [268, 0.286092765], [270, 0.283683684], [272, 0.281298636], [274, 0.278937339], [276, 0.276599517], [278, 0.274284898], [280, 0.271993211], [282, 0.269724193], [284, 0.267477582], [286, 0.265253121], [288, 0.263050554], [290, 0.260869633], [292, 0.258710110], [294, 0.256571741], [296, 0.254454286], [298, 0.252357508], [300, 0.250281174], [302, 0.248225053], [304, 0.246188917], [306, 0.244172542], [308, 0.242175707], [310, 0.240198194], [312, 0.238239786], [314, 0.236300272], [316, 0.234379441], [318, 0.232477087], [320, 0.230593005], [322, 0.228726993], [324, 0.226878853], [326, 0.225048388], [328, 0.223235405], [330, 0.221439711], [332, 0.219661118], [334, 0.217899439], [336, 0.216154491], [338, 0.214426091], [340, 0.212714060], [342, 0.211018220], [344, 0.209338398], [346, 0.207674420], [348, 0.206026115], [350, 0.204393315], [355, 0.200378063], [360, 0.196456139], [365, 0.192625077], [370, 0.188882487], [375, 0.185226048], [380, 0.181653511], [385, 0.178162694], [390, 0.174751478], [395, 0.171417807], [400, 0.168159686], [405, 0.164975177], [410, 0.161862398], [415, 0.158819521], [420, 0.155844772], [425, 0.152936426], [430, 0.150092806], [435, 0.147312286], [440, 0.144593281], [445, 0.141934254], [450, 0.139333710], [455, 0.136790195], [460, 0.134302294], [465, 0.131868634], [470, 0.129487876], [475, 0.127158722], [480, 0.124879906], [485, 0.122650197], [490, 0.120468398], [495, 0.118333345], [500, 0.116243903], [505, 0.114198970], [510, 0.112197471], [515, 0.110238362], [520, 0.108320625], [525, 0.106443271], [530, 0.104605335], [535, 0.102805877], [540, 0.101043985], [545, 0.099318768], [550, 0.097629359], [555, 0.095974915], [560, 0.094354612], [565, 0.092767650], [570, 0.091213248], [575, 0.089690648], [580, 0.088199108], [585, 0.086737906], [590, 0.085306341], [595, 0.083903726], [600, 0.082529395], [605, 0.081182697], [610, 0.079862998], [615, 0.078569680], [620, 0.077302141], [625, 0.076059794], [630, 0.074842066], [635, 0.073648400], [640, 0.072478251], [645, 0.071331090], [650, 0.070206399], [655, 0.069103674], [660, 0.068022424], [665, 0.066962168], [670, 0.065922439], [675, 0.064902780], [680, 0.063902748], [685, 0.062921909], [690, 0.061959837], [695, 0.061016122], [700, 0.060090360], [705, 0.059182157], [710, 0.058291131], [715, 0.057416907], [720, 0.056559120], [725, 0.055717414], [730, 0.054891440], [735, 0.054080860], [740, 0.053285343], [745, 0.052504565], [750, 0.051738210], [755, 0.050985971], [760, 0.050247546], [765, 0.049522643], [770, 0.048810974], [775, 0.048112260], [780, 0.047426227], [785, 0.046752609], [790, 0.046091145], [795, 0.045441581], [800, 0.044803668], [805, 0.044177164], [810, 0.043561831], [815, 0.042957438], [820, 0.042363759], [825, 0.041780573], [830, 0.041207664], [835, 0.040644822], [840, 0.040091839], [845, 0.039548516], [850, 0.039014654], [855, 0.038490063], [860, 0.037974554], [865, 0.037467944], [870, 0.036970054], [875, 0.036480707], [880, 0.035999734], [885, 0.035526965], [890, 0.035062238], [895, 0.034605393], [900, 0.034156272], [905, 0.033714724], [910, 0.033280598], [915, 0.032853749], [920, 0.032434032], [925, 0.032021309], [930, 0.031615443], [935, 0.031216300], [940, 0.030823749], [945, 0.030437663], [950, 0.030057915], [955, 0.029684385], [960, 0.029316951], [965, 0.028955498], [970, 0.028599910], [975, 0.028250075], [980, 0.027905884], [985, 0.027567229], [990, 0.027234006], [995, 0.026906112], [1000, 0.026583445], [1005, 0.026265908], [1010, 0.025953405], [1015, 0.025645841], [1020, 0.025343124], [1025, 0.025045163], [1030, 0.024751871], [1035, 0.024463160], [1040, 0.024178947], [1045, 0.023899147], [1050, 0.023623680], [1055, 0.023352467], [1060, 0.023085429], [1065, 0.022822491], [1070, 0.022563577], [1075, 0.022308615], [1080, 0.022057533], [1085, 0.021810260], [1090, 0.021566729], [1095, 0.021326872], [1100, 0.021090622]])
_fCO2eqD47_Petersen = interp1d(Petersen_etal_CO2eqD47[:,0], Petersen_etal_CO2eqD47[:,1])
def fCO2eqD47_Petersen(T):
        &#39;&#39;&#39;
        CO&lt;sub&gt;2&lt;/sub&gt; equilibrium Δ&lt;sub&gt;47&lt;/sub&gt; value as a function of T (in degrees C)
        according to [Petersen et al. (2019)].

        [Petersen et al. (2019)]: https://doi.org/10.1029/2018GC008127
        &#39;&#39;&#39;
        return float(_fCO2eqD47_Petersen(T))


Wang_etal_CO2eqD47 = np.array([[-83., 1.8954], [-73., 1.7530], [-63., 1.6261], [-53., 1.5126], [-43., 1.4104], [-33., 1.3182], [-23., 1.2345], [-13., 1.1584], [-3., 1.0888], [7., 1.0251], [17., 0.9665], [27., 0.9125], [37., 0.8626], [47., 0.8164], [57., 0.7734], [67., 0.7334], [87., 0.6612], [97., 0.6286], [107., 0.5980], [117., 0.5693], [127., 0.5423], [137., 0.5169], [147., 0.4930], [157., 0.4704], [167., 0.4491], [177., 0.4289], [187., 0.4098], [197., 0.3918], [207., 0.3747], [217., 0.3585], [227., 0.3431], [237., 0.3285], [247., 0.3147], [257., 0.3015], [267., 0.2890], [277., 0.2771], [287., 0.2657], [297., 0.2550], [307., 0.2447], [317., 0.2349], [327., 0.2256], [337., 0.2167], [347., 0.2083], [357., 0.2002], [367., 0.1925], [377., 0.1851], [387., 0.1781], [397., 0.1714], [407., 0.1650], [417., 0.1589], [427., 0.1530], [437., 0.1474], [447., 0.1421], [457., 0.1370], [467., 0.1321], [477., 0.1274], [487., 0.1229], [497., 0.1186], [507., 0.1145], [517., 0.1105], [527., 0.1068], [537., 0.1031], [547., 0.0997], [557., 0.0963], [567., 0.0931], [577., 0.0901], [587., 0.0871], [597., 0.0843], [607., 0.0816], [617., 0.0790], [627., 0.0765], [637., 0.0741], [647., 0.0718], [657., 0.0695], [667., 0.0674], [677., 0.0654], [687., 0.0634], [697., 0.0615], [707., 0.0597], [717., 0.0579], [727., 0.0562], [737., 0.0546], [747., 0.0530], [757., 0.0515], [767., 0.0500], [777., 0.0486], [787., 0.0472], [797., 0.0459], [807., 0.0447], [817., 0.0435], [827., 0.0423], [837., 0.0411], [847., 0.0400], [857., 0.0390], [867., 0.0380], [877., 0.0370], [887., 0.0360], [897., 0.0351], [907., 0.0342], [917., 0.0333], [927., 0.0325], [937., 0.0317], [947., 0.0309], [957., 0.0302], [967., 0.0294], [977., 0.0287], [987., 0.0281], [997., 0.0274], [1007., 0.0268], [1017., 0.0261], [1027., 0.0255], [1037., 0.0249], [1047., 0.0244], [1057., 0.0238], [1067., 0.0233], [1077., 0.0228], [1087., 0.0223], [1097., 0.0218]])
_fCO2eqD47_Wang = interp1d(Wang_etal_CO2eqD47[:,0] - 0.15, Wang_etal_CO2eqD47[:,1])
def fCO2eqD47_Wang(T):
        &#39;&#39;&#39;
        CO&lt;sub&gt;2&lt;/sub&gt; equilibrium Δ&lt;sub&gt;47&lt;/sub&gt; value as a function of `T` (in degrees C)
        according to [Wang et al. (2004)] (supplementary data of [Dennis et al., 2011]).

        [Wang et al. (2004)]: https://doi.org/10.1016/j.gca.2004.05.039
        [Dennis et al., 2011]: https://doi.org/10.1016/j.gca.2011.09.025
        &#39;&#39;&#39;
        return float(_fCO2eqD47_Wang(T))


def correlated_sum(X, C, w = None):
        &#39;&#39;&#39;
        Compute covariance-aware linear combinations

        __Parameters__
        
        + `X`: list or 1-D array of values to sum
        + `C`: covariance matrix for the elements of `X`
        + `w`: list or 1-D array of weights to apply to the elements of `X`
               (all equal to 1 by default)

        Return the sum (and its SE) of the elements of `X`, with optional weights equal
        to the elements of `w`, accounting for covariances between the elements of `X`.
        &#39;&#39;&#39;
        if w is None:
                w = [1 for x in X]
        return np.dot(w,X), (np.dot(w,np.dot(C,w)))**.5


def make_csv(x, hsep = &#39;,&#39;, vsep = &#39;\n&#39;):
        &#39;&#39;&#39;
        Formats a list of lists of strings as a CSV

        __Parameters__

        + `x`: the list of lists of strings to format
        + `hsep`: the field separator (`,` by default)
        + `vsep`: the line-ending convention to use (`\\n` by default)

        __Example__

        ```py
        print(make_csv([[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;d&#39;, &#39;e&#39;, &#39;f&#39;]]))
        ```

        outputs:

        ```py
        a,b,c
        d,e,f
        ```
        &#39;&#39;&#39;
        return vsep.join([hsep.join(l) for l in x])


def pf(txt):
        &#39;&#39;&#39;
        Modify string `txt` to follow `lmfit.Parameter()` naming rules.
        &#39;&#39;&#39;
        return txt.replace(&#39;-&#39;,&#39;_&#39;).replace(&#39;.&#39;,&#39;_&#39;).replace(&#39; &#39;,&#39;_&#39;)


def smart_type(x):
        &#39;&#39;&#39;
        Tries to convert string `x` to a float if it includes a decimal point, or
        to an integer if it does not. If both attempts fail, return the original
        string unchanged.
        &#39;&#39;&#39;
        try:
                y = float(x)
        except ValueError:
                return x
        if &#39;.&#39; not in x:
                return int(y)
        return y


def pretty_table(x, header = 1, hsep = &#39;  &#39;, vsep = &#39;–&#39;, align = &#39;&lt;&#39;):
        &#39;&#39;&#39;
        Reads a list of lists of strings and outputs an ascii table

        __Parameters__

        + `x`: a list of lists of strings
        + `header`: the number of lines to treat as header lines
        + `hsep`: the horizontal separator between columns
        + `vsep`: the character to use as vertical separator
        + `align`: string of left (`&lt;`) or right (`&gt;`) alignment characters.

        __Example__

        ```python
        x = [[&#39;A&#39;,&#39;B&#39;, &#39;C&#39;], [&#39;1&#39;, &#39;1.9999&#39;, &#39;foo&#39;], [&#39;10&#39;, &#39;x&#39;, &#39;bar&#39;]]
        print(pretty_table(x))
        ```

        output:

        ```python
        --  ------  ---
        A        B    C
        --  ------  ---
        1   1.9999  foo
        10       x  bar
        --  ------  ---
        ```
        &#39;&#39;&#39;
        txt = []
        widths = [np.max([len(e) for e in c]) for c in zip(*x)]

        if len(widths) &gt; len(align):
                align += &#39;&gt;&#39; * (len(widths)-len(align))
        sepline = hsep.join([vsep*w for w in widths])
        txt += [sepline]
        for k,l in enumerate(x):
                if k and k == header:
                        txt += [sepline]
                txt += [hsep.join([f&#39;{e:{a}{w}}&#39; for e, w, a in zip(l, widths, align)])]
        txt += [sepline]
        txt += [&#39;&#39;]
        return &#39;\n&#39;.join(txt)


def transpose_table(x):
        &#39;&#39;&#39;
        Transpose a list if lists

        __Parameters__

        + `x`: a list of lists

        __Example__

        ```python
        x = [[1, 2], [3, 4]]
        print(transpose_table(x))
        ```

        outputs:

        ```python
        [[1, 3], [2, 4]]
        ```

        &#39;&#39;&#39;
        return [[e for e in c] for c in zip(*x)]


def w_avg(X, sX) :
        &#39;&#39;&#39;
        Compute variance-weighted average

        Returns the value and SE of the weighted average of the elements of `X`,
        with relative weights equal to their inverse variances (`1/sX**2`).

        __Parameters__

        + `X`: array-like of elements to average
        + `sX`: array-like of the corresponding SE values

        __Tip__

        If `X` and `sX` are initially arranged as a list of `(x, sx)` doublets,
        they may be rearranged using `zip()`:

        ```python
        foo = [(0, 0.1), (1, 0.05), (2, 0.05)]
        print(w_avg(*zip(*foo)))

        # output:
        # (1.3333333333333333, 0.03333333333333334)
        ```
        &#39;&#39;&#39;
        X = [ x for x in X ]
        sX = [ sx for sx in sX ]
        W = [ sx**-2 for sx in sX ]
        W = [ w/sum(W) for w in W ]
        Xavg = sum([ w*x for w,x in zip(W,X) ])
        sXavg = sum([ w**2*sx**2 for w,sx in zip(W,sX) ])**.5
        return Xavg, sXavg


def read_csv(filename, sep = &#39;&#39;):
        &#39;&#39;&#39;
        Read contents of `filename` in csv format and return a list of dictionaries.

        In the csv string, spaces before and after field separators (`&#39;,&#39;` by default)
        are optional.

        __Parameters__

        + `filename`: the csv file to read
        + `sep`: csv separator delimiting the fields. By default, use `,`, `;`, or `\t`,
        whichever appers most often in the contents of `filename`.
        &#39;&#39;&#39;
        with open(filename) as fid:
                txt = fid.read()

        if sep == &#39;&#39;:
                sep = sorted(&#39;,;\t&#39;, key = lambda x: - txt.count(x))[0]
        txt = [[x.strip() for x in l.split(sep)] for l in txt.splitlines() if l.strip()]
        return [{k: smart_type(v) for k,v in zip(txt[0], l) if v} for l in txt[1:]]


class D4xdata(list):
        &#39;&#39;&#39;
        Store and process data for a large set of Δ&lt;sub&gt;47&lt;/sub&gt; and/or Δ&lt;sub&gt;48&lt;/sub&gt;
        analyses, usually comprising more than one analytical session.
        &#39;&#39;&#39;

        ### 17O CORRECTION PARAMETERS
        R13_VPDB = 0.01118  # (Chang &amp; Li, 1990)
        &#39;&#39;&#39;
        Absolute (&lt;sup&gt;13&lt;/sup&gt;C/&lt;sup&gt;12&lt;/sup&gt;C) ratio of VPDB.
        By default equal to 0.01118 ([Chang &amp; Li, 1990])

        [Chang &amp; Li, 1990]: http://www.cnki.com.cn/Article/CJFDTotal-JXTW199004006.htm
        &#39;&#39;&#39;

        R18_VSMOW = 0.0020052  # (Baertschi, 1976)
        &#39;&#39;&#39;
        Absolute (&lt;sup&gt;18&lt;/sup&gt;O/&lt;sup&gt;16&lt;/sup&gt;C) ratio of VSMOW.
        By default equal to 0.0020052 ([Baertschi, 1976])

        [Baertschi, 1976]: https://doi.org/10.1016/0012-821X(76)90115-1
        &#39;&#39;&#39;

        lambda_17 = 0.528  # (Barkan &amp; Luz, 2005)
        &#39;&#39;&#39;
        Mass-dependent exponent for triple oxygen isotopes.
        By default equal to 0.528 ([Barkan &amp; Luz, 2005])

        [Barkan &amp; Luz, 2005]: https://doi.org/10.1002/rcm.2250
        &#39;&#39;&#39;

        R17_VSMOW = 0.00038475  # (Assonov &amp; Brenninkmeijer, 2003, rescaled to R13_VPDB)
        &#39;&#39;&#39;
        Absolute (&lt;sup&gt;17&lt;/sup&gt;O/&lt;sup&gt;16&lt;/sup&gt;C) ratio of VSMOW.
        By default equal to 0.00038475
        ([Assonov &amp; Brenninkmeijer, 2003], rescaled to `R13_VPDB`)

        [Assonov &amp; Brenninkmeijer, 2003]: https://dx.doi.org/10.1002/rcm.1011
        &#39;&#39;&#39;

        R18_VPDB = R18_VSMOW * 1.03092
        &#39;&#39;&#39;
        Absolute (&lt;sup&gt;18&lt;/sup&gt;O/&lt;sup&gt;16&lt;/sup&gt;C) ratio of VPDB.
        By definition equal to `R18_VSMOW * 1.03092`.
        &#39;&#39;&#39;

        R17_VPDB = R17_VSMOW * 1.03092 ** lambda_17
        &#39;&#39;&#39;
        Absolute (&lt;sup&gt;17&lt;/sup&gt;O/&lt;sup&gt;16&lt;/sup&gt;C) ratio of VPDB.
        By definition equal to `R17_VSMOW * 1.03092 ** lambda_17`.
        &#39;&#39;&#39;

        LEVENE_REF_SAMPLE = &#39;ETH-3&#39;
        &#39;&#39;&#39;
        After the Δ&lt;sub&gt;4x&lt;/sub&gt; standardization step, each sample is tested to
        assess whether the Δ&lt;sub&gt;4x&lt;/sub&gt; variance within all analyses for that
        sample differs significantly from that observed for a given reference
        sample (using [Levene&#39;s test], which yields a p-value corresponding to
        the null hypothesis that the underlying variances are equal).

        `LEVENE_REF_SAMPLE` (by default equal to `&#39;ETH-3&#39;`) specifies which
        sample should be used as a reference for this test.

        [Levene&#39;s test]: https://en.wikipedia.org/wiki/Levene%27s_test
        &#39;&#39;&#39;

        ALPHA_18O_ACID_REACTION = round(np.exp(3.59 / (90 + 273.15) - 1.79e-3), 6)  # (Kim et al., 2007, calcite)
        &#39;&#39;&#39;
        Specifies the &lt;sup&gt;18&lt;/sup&gt;O/&lt;sup&gt;16&lt;/sup&gt;O fractionation factor generally applicable
        to acid reactions in the dataset. Currently used by `D4xdata.wg()`,
        `D4xdata.standardize_d13C`, and `D4xdata.standardize_d18O`.

        By default equal to 1.008129 (calcite reacted at 90 °C, [Kim et al., 2007]).

        [Kim et al., 2007]: https://dx.doi.org/10.1016/j.chemgeo.2007.08.005
        &#39;&#39;&#39;

        Nominal_d13C_VPDB = {
                &#39;ETH-1&#39;: 2.02,
                &#39;ETH-2&#39;: -10.17,
                &#39;ETH-3&#39;: 1.71,
                }       # (Bernasconi et al., 2018)
        &#39;&#39;&#39;
        Nominal δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; values assigned to carbonate standards, used by
        `D4xdata.standardize_d13C()`.

        By default equal to `{&#39;ETH-1&#39;: 2.02, &#39;ETH-2&#39;: -10.17, &#39;ETH-3&#39;: 1.71}` after
        [Bernasconi et al. (2018)].

        [Bernasconi et al. (2018)]: https://doi.org/10.1029/2017GC007385
        &#39;&#39;&#39;

        Nominal_d18O_VPDB = {
                &#39;ETH-1&#39;: -2.19,
                &#39;ETH-2&#39;: -18.69,
                &#39;ETH-3&#39;: -1.78,
                }       # (Bernasconi et al., 2018)
        &#39;&#39;&#39;
        Nominal δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VPDB&lt;/sub&gt; values assigned to carbonate standards, used by
        `D4xdata.standardize_d18O()`.

        By default equal to `{&#39;ETH-1&#39;: -2.19, &#39;ETH-2&#39;: -18.69, &#39;ETH-3&#39;: -1.78}` after
        [Bernasconi et al. (2018)].

        [Bernasconi et al. (2018)]: https://doi.org/10.1029/2017GC007385
        &#39;&#39;&#39;

        d13C_STANDARDIZATION_METHOD = &#39;2pt&#39;
        &#39;&#39;&#39;
        Method by which to standardize δ&lt;sup&gt;13&lt;/sup&gt;C values:
        
        + `none`: do not apply any δ&lt;sup&gt;13&lt;/sup&gt;C standardization.
        + `&#39;1pt&#39;`: within each session, offset all initial δ&lt;sup&gt;13&lt;/sup&gt;C values so as to
        minimize the difference between final δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; values and
        `Nominal_d13C_VPDB` (averaged over all analyses for which `Nominal_d13C_VPDB` is defined).
        + `&#39;2pt&#39;`: within each session, apply a affine trasformation to all δ&lt;sup&gt;13&lt;/sup&gt;C
        values so as to minimize the difference between final δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt;
        values and `Nominal_d13C_VPDB` (averaged over all analyses for which `Nominal_d13C_VPDB`
        is defined).
        &#39;&#39;&#39;

        d18O_STANDARDIZATION_METHOD = &#39;2pt&#39;
        &#39;&#39;&#39;
        Method by which to standardize δ&lt;sup&gt;18&lt;/sup&gt;O values:
        
        + `none`: do not apply any δ&lt;sup&gt;18&lt;/sup&gt;O standardization.
        + `&#39;1pt&#39;`: within each session, offset all initial δ&lt;sup&gt;18&lt;/sup&gt;O values so as to
        minimize the difference between final δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VPDB&lt;/sub&gt; values and
        `Nominal_d18O_VPDB` (averaged over all analyses for which `Nominal_d18O_VPDB` is defined).
        + `&#39;2pt&#39;`: within each session, apply a affine trasformation to all δ&lt;sup&gt;18&lt;/sup&gt;O
        values so as to minimize the difference between final δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VPDB&lt;/sub&gt;
        values and `Nominal_d18O_VPDB` (averaged over all analyses for which `Nominal_d18O_VPDB`
        is defined).
        &#39;&#39;&#39;

        def __init__(self, l = [], mass = &#39;47&#39;, logfile = &#39;&#39;, session = &#39;mySession&#39;, verbose = False):
                &#39;&#39;&#39;
                __Parameters__

                + `l`: a list of dictionaries, with each dictionary including at least the keys
                `Sample`, `d45`, `d46`, and `d47` or `d48`.
                + `logfile`: if specified, write detailed logs to this file path when calling `D4xdata` methods.
                + `session`: define session name for analyses without a `Session` key
                + `verbose`: if `True`, print out detailed logs when calling `D4xdata` methods.

                Returns a `D47data` object derived from `list`.
                &#39;&#39;&#39;
                self._4x = mass
                self.verbose = verbose
                self.prefix = &#39;D4xdata&#39;
                self.logfile = logfile
                list.__init__(self, l)
                self.Nf = None
                self.repeatability = {}
                self.refresh(session = session)


        def make_verbal(oldfun):
                &#39;&#39;&#39;
                Decorator: allow temporarily changing `self.prefix` and overriding `self.verbose`.
                &#39;&#39;&#39;
                @wraps(oldfun)
                def newfun(*args, verbose = &#39;&#39;, **kwargs):
                        myself = args[0]
                        oldprefix = myself.prefix
                        myself.prefix = oldfun.__name__
                        if verbose != &#39;&#39;:
                                oldverbose = myself.verbose
                                myself.verbose = verbose
                        out = oldfun(*args, **kwargs)
                        myself.prefix = oldprefix
                        if verbose != &#39;&#39;:
                                myself.verbose = oldverbose
                        return out
                return newfun


        def msg(self, txt):
                &#39;&#39;&#39;
                Log a message to `self.logfile`, and print it out if `verbose = True`
                &#39;&#39;&#39;
                self.log(txt)
                if self.verbose:
                        print(f&#39;{f&#34;[{self.prefix}]&#34;:&lt;16} {txt}&#39;)


        def vmsg(self, txt):
                &#39;&#39;&#39;
                Log a message to `self.logfile` and print it out
                &#39;&#39;&#39;
                self.log(txt)
                print(txt)


        def log(self, *txts):
                &#39;&#39;&#39;
                Log a message to `self.logfile`
                &#39;&#39;&#39;
                if self.logfile:
                        with open(self.logfile, &#39;a&#39;) as fid:
                                for txt in txts:
                                        fid.write(f&#39;\n{dt.now().strftime(&#34;%Y-%m-%d %H:%M:%S&#34;)} {f&#34;[{self.prefix}]&#34;:&lt;16} {txt}&#39;)


        def refresh(self, session = &#39;mySession&#39;):
                &#39;&#39;&#39;
                Update `self.sessions`, `self.samples`, `self.anchors`, and `self.unknowns`.
                &#39;&#39;&#39;
                self.fill_in_missing_info(session = session)
                self.refresh_sessions()
                self.refresh_samples()


        def refresh_sessions(self):
                &#39;&#39;&#39;
                Update `self.sessions` and set `scrambling_drift`, `slope_drift`, and `wg_drift`
                to `False` for all sessions.
                &#39;&#39;&#39;
                self.sessions = {
                        s: {&#39;data&#39;: [r for r in self if r[&#39;Session&#39;] == s]}
                        for s in sorted({r[&#39;Session&#39;] for r in self})
                        }
                for s in self.sessions:
                        self.sessions[s][&#39;scrambling_drift&#39;] = False
                        self.sessions[s][&#39;slope_drift&#39;] = False
                        self.sessions[s][&#39;wg_drift&#39;] = False
                        self.sessions[s][&#39;d13C_standardization_method&#39;] = self.d13C_STANDARDIZATION_METHOD
                        self.sessions[s][&#39;d18O_standardization_method&#39;] = self.d18O_STANDARDIZATION_METHOD


        def refresh_samples(self):
                &#39;&#39;&#39;
                Define `self.samples`, `self.anchors`, and `self.unknowns`.
                &#39;&#39;&#39;
                self.samples = {
                        s: {&#39;data&#39;: [r for r in self if r[&#39;Sample&#39;] == s]}
                        for s in sorted({r[&#39;Sample&#39;] for r in self})
                        }
                self.anchors = {s: self.samples[s] for s in self.samples if s in self.Nominal_D4x}
                self.unknowns = {s: self.samples[s] for s in self.samples if s not in self.Nominal_D4x}


        def read(self, filename, sep = &#39;&#39;, session = &#39;&#39;):
                &#39;&#39;&#39;
                Read file in csv format to load data into a `D47data` object.

                In the csv file, spaces before and after field separators (`&#39;,&#39;` by default)
                are optional. Each line corresponds to a single analysis.

                The required fields are:

                + `UID`: a unique identifier
                + `Session`: an identifier for the analytical session
                + `Sample`: a sample identifier
                + `d45`, `d46`, and at least one of `d47` or `d48`: the working-gas delta values

                Independently known oxygen-17 anomalies may be provided as `D17O` (in ‰ relative to
                VSMOW, λ = `self.lambda_17`), and are otherwise assumed to be zero. Working-gas deltas `d47`, `d48`
                and `d49` are optional, and set to NaN by default.

                __Parameters__

                + `fileneme`: the path of the file to read
                + `sep`: csv separator delimiting the fields
                + `session`: set `Session` field to this string for all analyses
                &#39;&#39;&#39;
                with open(filename) as fid:
                        self.input(fid.read(), sep = sep, session = session)


        def input(self, txt, sep = &#39;&#39;, session = &#39;&#39;):
                &#39;&#39;&#39;
                Read `txt` string in csv format to load analysis data into a `D47data` object.

                In the csv string, spaces before and after field separators (`&#39;,&#39;` by default)
                are optional. Each line corresponds to a single analysis.

                The required fields are:

                + `UID`: a unique identifier
                + `Session`: an identifier for the analytical session
                + `Sample`: a sample identifier
                + `d45`, `d46`, and at least one of `d47` or `d48`: the working-gas delta values

                Independently known oxygen-17 anomalies may be provided as `D17O` (in ‰ relative to
                VSMOW, λ = `self.lambda_17`), and are otherwise assumed to be zero. Working-gas deltas `d47`, `d48`
                and `d49` are optional, and set to NaN by default.

                __Parameters__

                + `txt`: the csv string to read
                + `sep`: csv separator delimiting the fields. By default, use `,`, `;`, or `\t`,
                whichever appers most often in `txt`.
                + `session`: set `Session` field to this string for all analyses
                &#39;&#39;&#39;
                if sep == &#39;&#39;:
                        sep = sorted(&#39;,;\t&#39;, key = lambda x: - txt.count(x))[0]
                txt = [[x.strip() for x in l.split(sep)] for l in txt.splitlines() if l.strip()]
                data = [{k: v if k in [&#39;UID&#39;, &#39;Session&#39;, &#39;Sample&#39;] else smart_type(v) for k,v in zip(txt[0], l) if v != &#39;&#39;} for l in txt[1:]]

                if session != &#39;&#39;:
                        for r in data:
                                r[&#39;Session&#39;] = session

                self += data
                self.refresh()


        @make_verbal
        def wg(self, samples = None, a18_acid = None):
                &#39;&#39;&#39;
                Compute bulk composition of the working gas for each session based on
                the carbonate standards defined in both `self.Nominal_d13C_VPDB` and
                `self.Nominal_d18O_VPDB`.
                &#39;&#39;&#39;

                self.msg(&#39;Computing WG composition:&#39;)

                if a18_acid is None:
                        a18_acid = self.ALPHA_18O_ACID_REACTION
                if samples is None:
                        samples = [s for s in self.Nominal_d13C_VPDB if s in self.Nominal_d18O_VPDB]

                assert a18_acid, f&#39;Acid fractionation factor should not be zero.&#39;

                samples = [s for s in samples if s in self.Nominal_d13C_VPDB and s in self.Nominal_d18O_VPDB]
                R45R46_standards = {}
                for sample in samples:
                        d13C_vpdb = self.Nominal_d13C_VPDB[sample]
                        d18O_vpdb = self.Nominal_d18O_VPDB[sample]
                        R13_s = self.R13_VPDB * (1 + d13C_vpdb / 1000)
                        R17_s = self.R17_VPDB * ((1 + d18O_vpdb / 1000) * a18_acid) ** self.lambda_17
                        R18_s = self.R18_VPDB * (1 + d18O_vpdb / 1000) * a18_acid

                        C12_s = 1 / (1 + R13_s)
                        C13_s = R13_s / (1 + R13_s)
                        C16_s = 1 / (1 + R17_s + R18_s)
                        C17_s = R17_s / (1 + R17_s + R18_s)
                        C18_s = R18_s / (1 + R17_s + R18_s)

                        C626_s = C12_s * C16_s ** 2
                        C627_s = 2 * C12_s * C16_s * C17_s
                        C628_s = 2 * C12_s * C16_s * C18_s
                        C636_s = C13_s * C16_s ** 2
                        C637_s = 2 * C13_s * C16_s * C17_s
                        C727_s = C12_s * C17_s ** 2

                        R45_s = (C627_s + C636_s) / C626_s
                        R46_s = (C628_s + C637_s + C727_s) / C626_s
                        R45R46_standards[sample] = (R45_s, R46_s)
                
                for s in self.sessions:
                        db = [r for r in self.sessions[s][&#39;data&#39;] if r[&#39;Sample&#39;] in samples]
                        assert db, f&#39;No sample from {samples} found in session &#34;{s}&#34;.&#39;
#                       dbsamples = sorted({r[&#39;Sample&#39;] for r in db})

                        X = [r[&#39;d45&#39;] for r in db]
                        Y = [R45R46_standards[r[&#39;Sample&#39;]][0] for r in db]
                        x1, x2 = np.min(X), np.max(X)

                        if x1 &lt; x2:
                                wgcoord = x1/(x1-x2)
                        else:
                                wgcoord = 999

                        if wgcoord &lt; -.5 or wgcoord &gt; 1.5:
                                # unreasonable to extrapolate to d45 = 0
                                R45_wg = np.mean([y/(1+x/1000) for x,y in zip(X,Y)])
                        else :
                                # d45 = 0 is reasonably well bracketed
                                R45_wg = np.polyfit(X, Y, 1)[1]

                        X = [r[&#39;d46&#39;] for r in db]
                        Y = [R45R46_standards[r[&#39;Sample&#39;]][1] for r in db]
                        x1, x2 = np.min(X), np.max(X)

                        if x1 &lt; x2:
                                wgcoord = x1/(x1-x2)
                        else:
                                wgcoord = 999

                        if wgcoord &lt; -.5 or wgcoord &gt; 1.5:
                                # unreasonable to extrapolate to d46 = 0
                                R46_wg = np.mean([y/(1+x/1000) for x,y in zip(X,Y)])
                        else :
                                # d46 = 0 is reasonably well bracketed
                                R46_wg = np.polyfit(X, Y, 1)[1]

                        d13Cwg_VPDB, d18Owg_VSMOW = self.compute_bulk_delta(R45_wg, R46_wg)

                        self.msg(f&#39;Session {s} WG:   δ13C_VPDB = {d13Cwg_VPDB:.3f}   δ18O_VSMOW = {d18Owg_VSMOW:.3f}&#39;)

                        self.sessions[s][&#39;d13Cwg_VPDB&#39;] = d13Cwg_VPDB
                        self.sessions[s][&#39;d18Owg_VSMOW&#39;] = d18Owg_VSMOW
                        for r in self.sessions[s][&#39;data&#39;]:
                                r[&#39;d13Cwg_VPDB&#39;] = d13Cwg_VPDB
                                r[&#39;d18Owg_VSMOW&#39;] = d18Owg_VSMOW


        def compute_bulk_delta(self, R45, R46, D17O = 0):
                &#39;&#39;&#39;
                Compute δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; and δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt;,
                by solving the generalized form of equation (17) from [Brand et al. (2010)],
                assuming that δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt; is not too big (0 ± 50 ‰) and
                solving the corresponding second-order Taylor polynomial.
                (Appendix A of [Daëron et al., 2016])

                [Brand et al. (2010)]: https://doi.org/10.1351/PAC-REP-09-01-05
                [Daëron et al., 2016]: https://doi.org/10.1016/j.chemgeo.2016.08.014
                &#39;&#39;&#39;

                K = np.exp(D17O / 1000) * self.R17_VSMOW * self.R18_VSMOW ** -self.lambda_17

                A = -3 * K ** 2 * self.R18_VSMOW ** (2 * self.lambda_17)
                B = 2 * K * R45 * self.R18_VSMOW ** self.lambda_17
                C = 2 * self.R18_VSMOW
                D = -R46

                aa = A * self.lambda_17 * (2 * self.lambda_17 - 1) + B * self.lambda_17 * (self.lambda_17 - 1) / 2
                bb = 2 * A * self.lambda_17 + B * self.lambda_17 + C
                cc = A + B + C + D

                d18O_VSMOW = 1000 * (-bb + (bb ** 2 - 4 * aa * cc) ** .5) / (2 * aa)

                R18 = (1 + d18O_VSMOW / 1000) * self.R18_VSMOW
                R17 = K * R18 ** self.lambda_17
                R13 = R45 - 2 * R17

                d13C_VPDB = 1000 * (R13 / self.R13_VPDB - 1)

                return d13C_VPDB, d18O_VSMOW


        @make_verbal
        def crunch(self, verbose = &#39;&#39;):
                &#39;&#39;&#39;
                Compute bulk composition and raw clumped isotope anomalies for all analyses.
                &#39;&#39;&#39;
                for r in self:
                        self.compute_bulk_and_clumping_deltas(r)
                self.standardize_d13C()
                self.standardize_d18O()
                self.msg(f&#34;Crunched {len(self)} analyses.&#34;)


        def fill_in_missing_info(self, session = &#39;mySession&#39;):
                &#39;&#39;&#39;
                Fill in optional fields with default values
                &#39;&#39;&#39;
                for i,r in enumerate(self):
                        if &#39;D17O&#39; not in r:
                                r[&#39;D17O&#39;] = 0.
                        if &#39;UID&#39; not in r:
                                r[&#39;UID&#39;] = f&#39;{i+1}&#39;
                        if &#39;Session&#39; not in r:
                                r[&#39;Session&#39;] = session
                        for k in [&#39;d47&#39;, &#39;d48&#39;, &#39;d49&#39;]:
                                if k not in r:
                                        r[k] = np.nan


        def standardize_d13C(self):
                &#39;&#39;&#39;
                Perform δ&lt;sup&gt;13&lt;/sup&gt;C standadization within each session `s` according to
                `self.sessions[s][&#39;d13C_standardization_method&#39;]`, which is defined by default
                by `D47data.refresh_sessions()`as equal to `self.d13C_STANDARDIZATION_METHOD`, but
                may be redefined abitrarily at a later stage.
                &#39;&#39;&#39;
                for s in self.sessions:
                        if self.sessions[s][&#39;d13C_standardization_method&#39;] in [&#39;1pt&#39;, &#39;2pt&#39;]:
                                XY = [(r[&#39;d13C_VPDB&#39;], self.Nominal_d13C_VPDB[r[&#39;Sample&#39;]]) for r in self.sessions[s][&#39;data&#39;] if r[&#39;Sample&#39;] in self.Nominal_d13C_VPDB]
                                X,Y = zip(*XY)
                                if self.sessions[s][&#39;d13C_standardization_method&#39;] == &#39;1pt&#39;:
                                        offset = np.mean(Y) - np.mean(X)
                                        for r in self.sessions[s][&#39;data&#39;]:
                                                r[&#39;d13C_VPDB&#39;] += offset                                
                                elif self.sessions[s][&#39;d13C_standardization_method&#39;] == &#39;2pt&#39;:
                                        a,b = np.polyfit(X,Y,1)
                                        for r in self.sessions[s][&#39;data&#39;]:
                                                r[&#39;d13C_VPDB&#39;] = a * r[&#39;d13C_VPDB&#39;] + b

        def standardize_d18O(self):
                &#39;&#39;&#39;
                Perform δ&lt;sup&gt;18&lt;/sup&gt;O standadization within each session `s` according to
                `self.ALPHA_18O_ACID_REACTION` and `self.sessions[s][&#39;d18O_standardization_method&#39;]`,
                which is defined by default by `D47data.refresh_sessions()`as equal to
                `self.d18O_STANDARDIZATION_METHOD`, but may be redefined abitrarily at a later stage.
                &#39;&#39;&#39;
                for s in self.sessions:
                        if self.sessions[s][&#39;d18O_standardization_method&#39;] in [&#39;1pt&#39;, &#39;2pt&#39;]:
                                XY = [(r[&#39;d18O_VSMOW&#39;], self.Nominal_d18O_VPDB[r[&#39;Sample&#39;]]) for r in self.sessions[s][&#39;data&#39;] if r[&#39;Sample&#39;] in self.Nominal_d18O_VPDB]
                                X,Y = zip(*XY)
                                Y = [(1000+y) * self.R18_VPDB * self.ALPHA_18O_ACID_REACTION / self.R18_VSMOW - 1000 for y in Y]
                                if self.sessions[s][&#39;d18O_standardization_method&#39;] == &#39;1pt&#39;:
                                        offset = np.mean(Y) - np.mean(X)
                                        for r in self.sessions[s][&#39;data&#39;]:
                                                r[&#39;d18O_VSMOW&#39;] += offset                               
                                elif self.sessions[s][&#39;d18O_standardization_method&#39;] == &#39;2pt&#39;:
                                        a,b = np.polyfit(X,Y,1)
                                        for r in self.sessions[s][&#39;data&#39;]:
                                                r[&#39;d18O_VSMOW&#39;] = a * r[&#39;d18O_VSMOW&#39;] + b
        

        def compute_bulk_and_clumping_deltas(self, r):
                &#39;&#39;&#39;
                Compute δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt;, δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt;, and
                raw Δ&lt;sub&gt;47&lt;/sub&gt;, Δ&lt;sub&gt;48&lt;/sub&gt;, Δ&lt;sub&gt;49&lt;/sub&gt; values for an analysis `r`.
                &#39;&#39;&#39;

                # Compute working gas R13, R18, and isobar ratios
                R13_wg = self.R13_VPDB * (1 + r[&#39;d13Cwg_VPDB&#39;] / 1000)
                R18_wg = self.R18_VSMOW * (1 + r[&#39;d18Owg_VSMOW&#39;] / 1000)
                R45_wg, R46_wg, R47_wg, R48_wg, R49_wg = self.compute_isobar_ratios(R13_wg, R18_wg)

                # Compute analyte isobar ratios
                R45 = (1 + r[&#39;d45&#39;] / 1000) * R45_wg
                R46 = (1 + r[&#39;d46&#39;] / 1000) * R46_wg
                R47 = (1 + r[&#39;d47&#39;] / 1000) * R47_wg
                R48 = (1 + r[&#39;d48&#39;] / 1000) * R48_wg
                R49 = (1 + r[&#39;d49&#39;] / 1000) * R49_wg

                r[&#39;d13C_VPDB&#39;], r[&#39;d18O_VSMOW&#39;] = self.compute_bulk_delta(R45, R46, D17O = r[&#39;D17O&#39;])
                R13 = (1 + r[&#39;d13C_VPDB&#39;] / 1000) * self.R13_VPDB
                R18 = (1 + r[&#39;d18O_VSMOW&#39;] / 1000) * self.R18_VSMOW

                # Compute stochastic isobar ratios of the analyte
                R45stoch, R46stoch, R47stoch, R48stoch, R49stoch = self.compute_isobar_ratios(
                        R13, R18, D17O = r[&#39;D17O&#39;]
                )

                # Check that R45/R45stoch and R46/R46stoch are undistinguishable from 1,
                # and raise a warning if the corresponding anomalies exceed 0.02 ppm.
                if (R45 / R45stoch - 1) &gt; 5e-8:
                        self.vmsg(f&#39;This is unexpected: R45/R45stoch - 1 = {1e6 * (R45 / R45stoch - 1):.3f} ppm&#39;)
                if (R46 / R46stoch - 1) &gt; 5e-8:
                        self.vmsg(f&#39;This is unexpected: R46/R46stoch - 1 = {1e6 * (R46 / R46stoch - 1):.3f} ppm&#39;)

                # Compute raw clumped isotope anomalies
                r[&#39;D47raw&#39;] = 1000 * (R47 / R47stoch - 1)
                r[&#39;D48raw&#39;] = 1000 * (R48 / R48stoch - 1)
                r[&#39;D49raw&#39;] = 1000 * (R49 / R49stoch - 1)


        def compute_isobar_ratios(self, R13, R18, D17O=0, D47=0, D48=0, D49=0):
                &#39;&#39;&#39;
                Compute isobar ratios for a sample with isotopic ratios `R13` and `R18`,
                optionally accounting for non-zero values of Δ&lt;sup&gt;17&lt;/sup&gt;O (`D17O`) and clumped isotope
                anomalies (`D47`, `D48`, `D49`), all expressed in permil.
                &#39;&#39;&#39;

                # Compute R17
                R17 = self.R17_VSMOW * np.exp(D17O / 1000) * (R18 / self.R18_VSMOW) ** self.lambda_17

                # Compute isotope concentrations
                C12 = (1 + R13) ** -1
                C13 = C12 * R13
                C16 = (1 + R17 + R18) ** -1
                C17 = C16 * R17
                C18 = C16 * R18

                # Compute stochastic isotopologue concentrations
                C626 = C16 * C12 * C16
                C627 = C16 * C12 * C17 * 2
                C628 = C16 * C12 * C18 * 2
                C636 = C16 * C13 * C16
                C637 = C16 * C13 * C17 * 2
                C638 = C16 * C13 * C18 * 2
                C727 = C17 * C12 * C17
                C728 = C17 * C12 * C18 * 2
                C737 = C17 * C13 * C17
                C738 = C17 * C13 * C18 * 2
                C828 = C18 * C12 * C18
                C838 = C18 * C13 * C18

                # Compute stochastic isobar ratios
                R45 = (C636 + C627) / C626
                R46 = (C628 + C637 + C727) / C626
                R47 = (C638 + C728 + C737) / C626
                R48 = (C738 + C828) / C626
                R49 = C838 / C626

                # Account for stochastic anomalies
                R47 *= 1 + D47 / 1000
                R48 *= 1 + D48 / 1000
                R49 *= 1 + D49 / 1000

                # Return isobar ratios
                return R45, R46, R47, R48, R49


        def split_samples(self, samples_to_split = &#39;all&#39;, grouping = &#39;by_session&#39;):
                &#39;&#39;&#39;
                Split unknown samples by UID (treat all analyses as different samples)
                or by session (treat analyses of a given sample in different sessions as
                different samples).

                __Parameters__

                + `samples_to_split`: a list of samples to split, e.g., `[&#39;IAEA-C1&#39;, &#39;IAEA-C2&#39;]`
                + `grouping`: `by_uid` | `by_session`
                &#39;&#39;&#39;
                if samples_to_split == &#39;all&#39;:
                        samples_to_split = [s for s in self.unknowns]
                gkeys = {&#39;by_uid&#39;:&#39;UID&#39;, &#39;by_session&#39;:&#39;Session&#39;}
                self.grouping = grouping.lower()
                if self.grouping in gkeys:
                        gkey = gkeys[self.grouping]
                for r in self:
                        if r[&#39;Sample&#39;] in samples_to_split:
                                r[&#39;Sample_original&#39;] = r[&#39;Sample&#39;]
                                r[&#39;Sample&#39;] = f&#34;{r[&#39;Sample&#39;]}__{r[gkey]}&#34;
                        elif r[&#39;Sample&#39;] in self.unknowns:
                                r[&#39;Sample_original&#39;] = r[&#39;Sample&#39;]
                self.refresh_samples()


        def unsplit_samples(self, tables = False):
                &#39;&#39;&#39;
                Reverse the effects of `D47data.split_samples()`.
                
                This should only be used after `D4xdata.standardize()` with `method=&#39;pooled&#39;`.
                
                After `D4xdata.standardize()` with `method=&#39;indep_sessions&#39;`, one should
                probably use `D4xdata.combine_samples()` instead to reverse the effects of
                `D47data.split_samples()` with `grouping=&#39;by_uid&#39;`, or `w_avg()` to reverse the
                effects of `D47data.split_samples()` with `grouping=&#39;by_sessions&#39;` (because in
                that case session-averaged Δ&lt;sub&gt;4x&lt;/sub&gt; values are statistically independent).
                &#39;&#39;&#39;
                unknowns_old = sorted({s for s in self.unknowns})
                CM_old = self.standardization.covar[:,:]
                VD_old = self.standardization.params.valuesdict().copy()
                vars_old = self.standardization.var_names

                unknowns_new = sorted({r[&#39;Sample_original&#39;] for r in self if &#39;Sample_original&#39; in r})

                Ns = len(vars_old) - len(unknowns_old)
                vars_new = vars_old[:Ns] + [f&#39;D{self._4x}_{pf(u)}&#39; for u in unknowns_new]
                VD_new = {k: VD_old[k] for k in vars_old[:Ns]}

                W = np.zeros((len(vars_new), len(vars_old)))
                W[:Ns,:Ns] = np.eye(Ns)
                for u in unknowns_new:
                        splits = sorted({r[&#39;Sample&#39;] for r in self if &#39;Sample_original&#39; in r and r[&#39;Sample_original&#39;] == u})
                        if self.grouping == &#39;by_session&#39;:
                                weights = [self.samples[s][f&#39;SE_D{self._4x}&#39;]**-2 for s in splits]
                        elif self.grouping == &#39;by_uid&#39;:
                                weights = [1 for s in splits]
                        sw = sum(weights)
                        weights = [w/sw for w in weights]
                        W[vars_new.index(f&#39;D{self._4x}_{pf(u)}&#39;),[vars_old.index(f&#39;D{self._4x}_{pf(s)}&#39;) for s in splits]] = weights[:]

                CM_new = W @ CM_old @ W.T
                V = W @ np.array([[VD_old[k]] for k in vars_old])
                VD_new = {k:v[0] for k,v in zip(vars_new, V)}

                self.standardization.covar = CM_new
                self.standardization.params.valuesdict = lambda : VD_new
                self.standardization.var_names = vars_new

                for r in self:
                        if r[&#39;Sample&#39;] in self.unknowns:
                                r[&#39;Sample_split&#39;] = r[&#39;Sample&#39;]
                                r[&#39;Sample&#39;] = r[&#39;Sample_original&#39;]

                self.refresh_samples()
                self.consolidate_samples()
                self.repeatabilities()

                if tables:
                        self.table_of_analyses()
                        self.table_of_samples()

        def assign_timestamps(self):
                &#39;&#39;&#39;
                Assign a time field `t` of type `float` to each analysis.

                If `TimeTag` is one of the data fields, `t` is equal within a given session
                to `TimeTag` minus the mean value of `TimeTag` for that session.
                Otherwise, `TimeTag` is by default equal to the index of each analysis
                in the dataset and `t` is defined as above.
                &#39;&#39;&#39;
                for session in self.sessions:
                        sdata = self.sessions[session][&#39;data&#39;]
                        try:
                                t0 = np.mean([r[&#39;TimeTag&#39;] for r in sdata])
                                for r in sdata:
                                        r[&#39;t&#39;] = r[&#39;TimeTag&#39;] - t0
#                               print(&#39;DEBUG - USING TimeTag        &lt;-----------------------------------&#39;)
                        except KeyError:
                                t0 = (len(sdata)-1)/2
                                for t,r in enumerate(sdata):
                                        r[&#39;t&#39;] = t - t0


        def report(self):
                &#39;&#39;&#39;
                Prints a report on the standardization fit.
                &#39;&#39;&#39;
                report_fit(self.standardization)


        def combine_samples(self, sample_groups):
                &#39;&#39;&#39;
                Combine analyses of different samples to compute weighted average Δ&lt;sub&gt;47&lt;/sub&gt;
                and new error (co)variances corresponding to the groups defined by the `sample_groups`
                dictionary.
                
                Caution: samples are weighted by number of replicate analyses, which is a
                reasonable default behavior but is not always optimal (e.g., in the case of strongly
                correlated analytical errors for one or more samples).
                
                Returns a tuplet of:
                
                + the list of group names
                + an array of the corresponding Δ&lt;sub&gt;47&lt;/sub&gt; values
                + the corresponding (co)variance matrix
                
                __Parameters__

                + `sample_groups`: a dictionary of the form:
                ```py
                {&#39;group1&#39;: [&#39;sample_1&#39;, &#39;sample_2&#39;],
                 &#39;group2&#39;: [&#39;sample_3&#39;, &#39;sample_4&#39;, &#39;sample_5&#39;]}
                ```
                &#39;&#39;&#39;
                
                samples = [s for k in sorted(sample_groups.keys()) for s in sorted(sample_groups[k])]
                groups = sorted(sample_groups.keys())
                group_total_weights = {k: sum([self.samples[s][&#39;N&#39;] for s in sample_groups[k]]) for k in groups}
                D4x_old = np.array([[self.samples[x][f&#39;D{self._4x}&#39;]] for x in samples])
                CM_old = np.array([[self.sample_D4x_covar(x,y) for x in samples] for y in samples])
                W = np.array([
                        [self.samples[i][&#39;N&#39;]/group_total_weights[j] if i in sample_groups[j] else 0 for i in samples]
                        for j in groups])
                D4x_new = W @ D4x_old
                CM_new = W @ CM_old @ W.T

                return groups, D4x_new[:,0], CM_new
                

        @make_verbal
        def standardize(self,
                method = &#39;pooled&#39;,
                weighted_sessions = [],
                consolidate = True,
                consolidate_tables = False,
                consolidate_plots = False,
                constraints = {},
                ):
                &#39;&#39;&#39;
                Compute absolute Δ&lt;sub&gt;47&lt;/sub&gt; values for all replicate analyses and for sample averages.
                If `method` argument is set to `&#39;pooled&#39;`, the standardization processes all sessions
                in a single step, assuming that all samples (anchors and unknowns alike) are
                homogeneous (i.e. that their true Δ&lt;sub&gt;47&lt;/sub&gt; value does not change between sessions).
                If `method` argument is set to `&#39;indep_sessions&#39;`, the standardization processes each
                session independently, based only on anchors analyses.
                &#39;&#39;&#39;

                self.standardization_method = method
                self.assign_timestamps()

                if method == &#39;pooled&#39;:
                        if weighted_sessions:
                                for session_group in weighted_sessions:
                                        X = D4xdata([r for r in self if r[&#39;Session&#39;] in session_group], mass = self._4x)
                                        X.Nominal_D4x = self.Nominal_D4x.copy()
                                        X.refresh()
                                        result = X.standardize(method = &#39;pooled&#39;, weighted_sessions = [], consolidate = False)
                                        w = np.sqrt(result.redchi)
                                        self.msg(f&#39;Session group {session_group} MRSWD = {w:.4f}&#39;)
                                        for r in X:
                                                r[f&#39;wD{self._4x}raw&#39;] *= w
                        else:
                                self.msg(f&#39;All D{self._4x}raw weights set to 1 ‰&#39;)
                                for r in self:
                                        r[f&#39;wD{self._4x}raw&#39;] = 1.

                        params = Parameters()
                        for k,session in enumerate(self.sessions):
                                self.msg(f&#34;Session {session}: scrambling_drift is {self.sessions[session][&#39;scrambling_drift&#39;]}.&#34;)
                                self.msg(f&#34;Session {session}: slope_drift is {self.sessions[session][&#39;slope_drift&#39;]}.&#34;)
                                self.msg(f&#34;Session {session}: wg_drift is {self.sessions[session][&#39;wg_drift&#39;]}.&#34;)
                                s = pf(session)
                                params.add(f&#39;a_{s}&#39;, value = 0.9)
                                params.add(f&#39;b_{s}&#39;, value = 0.)
                                params.add(f&#39;c_{s}&#39;, value = -0.9)
                                params.add(f&#39;a2_{s}&#39;, value = 0., vary = self.sessions[session][&#39;scrambling_drift&#39;])
                                params.add(f&#39;b2_{s}&#39;, value = 0., vary = self.sessions[session][&#39;slope_drift&#39;])
                                params.add(f&#39;c2_{s}&#39;, value = 0., vary = self.sessions[session][&#39;wg_drift&#39;])
                        for sample in self.unknowns:
                                params.add(f&#39;D{self._4x}_{pf(sample)}&#39;, value = 0.5)

                        for k in constraints:
                                params[k].expr = constraints[k]

                        def residuals(p):
                                R = []
                                for r in self:
                                        session = pf(r[&#39;Session&#39;])
                                        sample = pf(r[&#39;Sample&#39;])
                                        if r[&#39;Sample&#39;] in self.Nominal_D4x:
                                                R += [ (
                                                        r[f&#39;D{self._4x}raw&#39;] - (
                                                                p[f&#39;a_{session}&#39;] * self.Nominal_D4x[r[&#39;Sample&#39;]]
                                                                + p[f&#39;b_{session}&#39;] * r[f&#39;d{self._4x}&#39;]
                                                                +       p[f&#39;c_{session}&#39;]
                                                                + r[&#39;t&#39;] * (
                                                                        p[f&#39;a2_{session}&#39;] * self.Nominal_D4x[r[&#39;Sample&#39;]]
                                                                        + p[f&#39;b2_{session}&#39;] * r[f&#39;d{self._4x}&#39;]
                                                                        +       p[f&#39;c2_{session}&#39;]
                                                                        )
                                                                )
                                                        ) / r[f&#39;wD{self._4x}raw&#39;] ]
                                        else:
                                                R += [ (
                                                        r[f&#39;D{self._4x}raw&#39;] - (
                                                                p[f&#39;a_{session}&#39;] * p[f&#39;D{self._4x}_{sample}&#39;]
                                                                + p[f&#39;b_{session}&#39;] * r[f&#39;d{self._4x}&#39;]
                                                                +       p[f&#39;c_{session}&#39;]
                                                                + r[&#39;t&#39;] * (
                                                                        p[f&#39;a2_{session}&#39;] * p[f&#39;D{self._4x}_{sample}&#39;]
                                                                        + p[f&#39;b2_{session}&#39;] * r[f&#39;d{self._4x}&#39;]
                                                                        +       p[f&#39;c2_{session}&#39;]
                                                                        )
                                                                )
                                                        ) / r[f&#39;wD{self._4x}raw&#39;] ]
                                return R

                        M = Minimizer(residuals, params)
                        result = M.least_squares()
                        self.Nf = result.nfree
                        self.t95 = tstudent.ppf(1 - 0.05/2, self.Nf)
#                       if self.verbose:
#                               report_fit(result)

                        for r in self:
                                s = pf(r[&#34;Session&#34;])
                                a = result.params.valuesdict()[f&#39;a_{s}&#39;]
                                b = result.params.valuesdict()[f&#39;b_{s}&#39;]
                                c = result.params.valuesdict()[f&#39;c_{s}&#39;]
                                a2 = result.params.valuesdict()[f&#39;a2_{s}&#39;]
                                b2 = result.params.valuesdict()[f&#39;b2_{s}&#39;]
                                c2 = result.params.valuesdict()[f&#39;c2_{s}&#39;]
                                r[f&#39;D{self._4x}&#39;] = (r[f&#39;D{self._4x}raw&#39;] - c - b * r[f&#39;d{self._4x}&#39;] - c2 * r[&#39;t&#39;] - b2 * r[&#39;t&#39;] * r[f&#39;d{self._4x}&#39;]) / (a + a2 * r[&#39;t&#39;])

                        self.standardization = result

                        for session in self.sessions:
                                self.sessions[session][&#39;Np&#39;] = 3
                                for k in [&#39;scrambling&#39;, &#39;slope&#39;, &#39;wg&#39;]:
                                        if self.sessions[session][f&#39;{k}_drift&#39;]:
                                                self.sessions[session][&#39;Np&#39;] += 1

                        if consolidate:
                                self.consolidate(tables = consolidate_tables, plots = consolidate_plots)
                        return result


                elif method == &#39;indep_sessions&#39;:

                        if weighted_sessions:
                                for session_group in weighted_sessions:
                                        X = D4xdata([r for r in self if r[&#39;Session&#39;] in session_group], mass = self._4x)
                                        X.Nominal_D4x = self.Nominal_D4x.copy()
                                        X.refresh()
                                        # This is only done to assign r[&#39;wD47raw&#39;] for r in X:
                                        X.standardize(method = method, weighted_sessions = [], consolidate = False)
                                        self.msg(f&#39;D{self._4x}raw weights set to {1000*X[0][f&#34;wD{self._4x}raw&#34;]:.1f} ppm for sessions in {session_group}&#39;)
                        else:
                                self.msg(&#39;All weights set to 1 ‰&#39;)
                                for r in self:
                                        r[f&#39;wD{self._4x}raw&#39;] = 1

                        for session in self.sessions:
                                s = self.sessions[session]
                                p_names = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;a2&#39;, &#39;b2&#39;, &#39;c2&#39;]
                                p_active = [True, True, True, s[&#39;scrambling_drift&#39;], s[&#39;slope_drift&#39;], s[&#39;wg_drift&#39;]]
                                s[&#39;Np&#39;] = sum(p_active)
                                sdata = s[&#39;data&#39;]

                                A = np.array([
                                        [
                                                self.Nominal_D4x[r[&#39;Sample&#39;]] / r[f&#39;wD{self._4x}raw&#39;],
                                                r[f&#39;d{self._4x}&#39;] / r[f&#39;wD{self._4x}raw&#39;],
                                                1 / r[f&#39;wD{self._4x}raw&#39;],
                                                self.Nominal_D4x[r[&#39;Sample&#39;]] * r[&#39;t&#39;] / r[f&#39;wD{self._4x}raw&#39;],
                                                r[f&#39;d{self._4x}&#39;] * r[&#39;t&#39;] / r[f&#39;wD{self._4x}raw&#39;],
                                                r[&#39;t&#39;] / r[f&#39;wD{self._4x}raw&#39;]
                                                ]
                                        for r in sdata if r[&#39;Sample&#39;] in self.anchors
                                        ])[:,p_active] # only keep columns for the active parameters
                                Y = np.array([[r[f&#39;D{self._4x}raw&#39;] / r[f&#39;wD{self._4x}raw&#39;]] for r in sdata if r[&#39;Sample&#39;] in self.anchors])
                                s[&#39;Na&#39;] = Y.size
                                CM = linalg.inv(A.T @ A)
                                bf = (CM @ A.T @ Y).T[0,:]
                                k = 0
                                for n,a in zip(p_names, p_active):
                                        if a:
                                                s[n] = bf[k]
#                                               self.msg(f&#39;{n} = {bf[k]}&#39;)
                                                k += 1
                                        else:
                                                s[n] = 0.
#                                               self.msg(f&#39;{n} = 0.0&#39;)

                                for r in sdata :
                                        a, b, c, a2, b2, c2 = s[&#39;a&#39;], s[&#39;b&#39;], s[&#39;c&#39;], s[&#39;a2&#39;], s[&#39;b2&#39;], s[&#39;c2&#39;]
                                        r[f&#39;D{self._4x}&#39;] = (r[f&#39;D{self._4x}raw&#39;] - c - b * r[f&#39;d{self._4x}&#39;] - c2 * r[&#39;t&#39;] - b2 * r[&#39;t&#39;] * r[f&#39;d{self._4x}&#39;]) / (a + a2 * r[&#39;t&#39;])
                                        r[f&#39;wD{self._4x}&#39;] /= (a + a2 * r[&#39;t&#39;])

                                s[&#39;CM&#39;] = np.zeros((6,6))
                                i = 0
                                k_active = [j for j,a in enumerate(p_active) if a]
                                for j,a in enumerate(p_active):
                                        if a:
                                                s[&#39;CM&#39;][j,k_active] = CM[i,:]
                                                i += 1

                        if not weighted_sessions:
                                w = self.rmswd()[&#39;rmswd&#39;]
                                for r in self:
                                                r[f&#39;wD{self._4x}&#39;] *= w
                                                r[f&#39;wD{self._4x}raw&#39;] *= w
                                for session in self.sessions:
                                        self.sessions[session][&#39;CM&#39;] *= w**2

                        for session in self.sessions:
                                s = self.sessions[session]
                                s[&#39;SE_a&#39;] = s[&#39;CM&#39;][0,0]**.5
                                s[&#39;SE_b&#39;] = s[&#39;CM&#39;][1,1]**.5
                                s[&#39;SE_c&#39;] = s[&#39;CM&#39;][2,2]**.5
                                s[&#39;SE_a2&#39;] = s[&#39;CM&#39;][3,3]**.5
                                s[&#39;SE_b2&#39;] = s[&#39;CM&#39;][4,4]**.5
                                s[&#39;SE_c2&#39;] = s[&#39;CM&#39;][5,5]**.5

                        if not weighted_sessions:
                                self.Nf = len(self) - len(self.unknowns) - np.sum([self.sessions[s][&#39;Np&#39;] for s in self.sessions])
                        else:
                                self.Nf = 0
                                for sg in weighted_sessions:
                                        self.Nf += self.rmswd(sessions = sg)[&#39;Nf&#39;]

                        self.t95 = tstudent.ppf(1 - 0.05/2, self.Nf)

                        avgD4x = {
                                sample: np.mean([r[f&#39;D{self._4x}&#39;] for r in self if r[&#39;Sample&#39;] == sample])
                                for sample in self.samples
                                }
                        chi2 = np.sum([(r[f&#39;D{self._4x}&#39;] - avgD4x[r[&#39;Sample&#39;]])**2 for r in self])
                        rD4x = (chi2/self.Nf)**.5
                        self.repeatability[f&#39;sigma_{self._4x}&#39;] = rD4x

                        if consolidate:
                                self.consolidate(tables = consolidate_tables, plots = consolidate_plots)


        def standardization_error(self, session, d4x, D4x, t = 0):
                &#39;&#39;&#39;
                Compute standardization error for a given session and
                (δ&lt;sub&gt;47&lt;/sub&gt;, Δ&lt;sub&gt;47&lt;/sub&gt;) composition.
                &#39;&#39;&#39;
                a = self.sessions[session][&#39;a&#39;]
                b = self.sessions[session][&#39;b&#39;]
                c = self.sessions[session][&#39;c&#39;]
                a2 = self.sessions[session][&#39;a2&#39;]
                b2 = self.sessions[session][&#39;b2&#39;]
                c2 = self.sessions[session][&#39;c2&#39;]
                CM = self.sessions[session][&#39;CM&#39;]

                x, y = D4x, d4x
                z = a * x + b * y + c + a2 * x * t + b2 * y * t + c2 * t
#               x = (z - b*y - b2*y*t - c - c2*t) / (a+a2*t)
                dxdy = -(b+b2*t) / (a+a2*t)
                dxdz = 1. / (a+a2*t)
                dxda = -x / (a+a2*t)
                dxdb = -y / (a+a2*t)
                dxdc = -1. / (a+a2*t)
                dxda2 = -x * a2 / (a+a2*t)
                dxdb2 = -y * t / (a+a2*t)
                dxdc2 = -t / (a+a2*t)
                V = np.array([dxda, dxdb, dxdc, dxda2, dxdb2, dxdc2])
                sx = (V @ CM @ V.T) ** .5
                return sx


        @make_verbal
        def summary(self,
                dir = &#39;output&#39;,
                filename = None,
                save_to_file = True,
                print_out = True,
                ):
                &#39;&#39;&#39;
                Print out an/or save to disk a summary of the standardization results.

                __Parameters__

                + `dir`: the directory in which to save the table
                + `filename`: the name to the csv file to write to
                + `save_to_file`: whether to save the table to disk
                + `print_out`: whether to print out the table
                &#39;&#39;&#39;

                out = []
                out += [[&#39;N samples (anchors + unknowns)&#39;, f&#34;{len(self.samples)} ({len(self.anchors)} + {len(self.unknowns)})&#34;]]
                out += [[&#39;N analyses (anchors + unknowns)&#39;, f&#34;{len(self)} ({len([r for r in self if r[&#39;Sample&#39;] in self.anchors])} + {len([r for r in self if r[&#39;Sample&#39;] in self.unknowns])})&#34;]]
                out += [[&#39;Repeatability of δ13C_VPDB&#39;, f&#34;{1000 * self.repeatability[&#39;r_d13C_VPDB&#39;]:.1f} ppm&#34;]]
                out += [[&#39;Repeatability of δ18O_VSMOW&#39;, f&#34;{1000 * self.repeatability[&#39;r_d18O_VSMOW&#39;]:.1f} ppm&#34;]]
                out += [[f&#39;Repeatability of Δ{self._4x} (anchors)&#39;, f&#34;{1000 * self.repeatability[f&#39;r_D{self._4x}a&#39;]:.1f} ppm&#34;]]
                out += [[f&#39;Repeatability of Δ{self._4x} (unknowns)&#39;, f&#34;{1000 * self.repeatability[f&#39;r_D{self._4x}u&#39;]:.1f} ppm&#34;]]
                out += [[f&#39;Repeatability of Δ{self._4x} (all)&#39;, f&#34;{1000 * self.repeatability[f&#39;r_D{self._4x}&#39;]:.1f} ppm&#34;]]
                out += [[&#39;Model degrees of freedom&#39;, f&#34;{self.Nf}&#34;]]
                out += [[&#39;Student\&#39;s 95% t-factor&#39;, f&#34;{self.t95:.2f}&#34;]]
                out += [[&#39;Standardization method&#39;, self.standardization_method]]

                if save_to_file:
                        if not os.path.exists(dir):
                                os.makedirs(dir)
                        if filename is None:
                                filename = f&#39;D{self._4x}_summary.csv&#39;
                        with open(f&#39;{dir}/{filename}&#39;, &#39;w&#39;) as fid:
                                fid.write(make_csv(out))
                if print_out:
                        self.msg(&#39;\n&#39; + pretty_table(out, header = 0))


        @make_verbal
        def table_of_sessions(self,
                dir = &#39;output&#39;,
                filename = None,
                save_to_file = True,
                print_out = True,
                ):
                &#39;&#39;&#39;
                Print out an/or save to disk a table of sessions.

                __Parameters__

                + `dir`: the directory in which to save the table
                + `filename`: the name to the csv file to write to
                + `save_to_file`: whether to save the table to disk
                + `print_out`: whether to print out the table
                &#39;&#39;&#39;
                include_a2 = any([self.sessions[session][&#39;scrambling_drift&#39;] for session in self.sessions])
                include_b2 = any([self.sessions[session][&#39;slope_drift&#39;] for session in self.sessions])
                include_c2 = any([self.sessions[session][&#39;wg_drift&#39;] for session in self.sessions])

                out = [[&#39;Session&#39;,&#39;Na&#39;,&#39;Nu&#39;,&#39;d13Cwg_VPDB&#39;,&#39;d18Owg_VSMOW&#39;,&#39;r_d13C&#39;,&#39;r_d18O&#39;,f&#39;r_D{self._4x}&#39;,&#39;a ± SE&#39;,&#39;1e3 x b ± SE&#39;,&#39;c ± SE&#39;]]
                if include_a2:
                        out[-1] += [&#39;a2 ± SE&#39;]
                if include_b2:
                        out[-1] += [&#39;b2 ± SE&#39;]
                if include_c2:
                        out[-1] += [&#39;c2 ± SE&#39;]
                for session in self.sessions:
                        out += [[
                                session,
                                f&#34;{self.sessions[session][&#39;Na&#39;]}&#34;,
                                f&#34;{self.sessions[session][&#39;Nu&#39;]}&#34;,
                                f&#34;{self.sessions[session][&#39;d13Cwg_VPDB&#39;]:.3f}&#34;,
                                f&#34;{self.sessions[session][&#39;d18Owg_VSMOW&#39;]:.3f}&#34;,
                                f&#34;{self.sessions[session][&#39;r_d13C_VPDB&#39;]:.4f}&#34;,
                                f&#34;{self.sessions[session][&#39;r_d18O_VSMOW&#39;]:.4f}&#34;,
                                f&#34;{self.sessions[session][f&#39;r_D{self._4x}&#39;]:.4f}&#34;,
                                f&#34;{self.sessions[session][&#39;a&#39;]:.3f} ± {self.sessions[session][&#39;SE_a&#39;]:.3f}&#34;,
                                f&#34;{1e3*self.sessions[session][&#39;b&#39;]:.3f} ± {1e3*self.sessions[session][&#39;SE_b&#39;]:.3f}&#34;,
                                f&#34;{self.sessions[session][&#39;c&#39;]:.3f} ± {self.sessions[session][&#39;SE_c&#39;]:.3f}&#34;,
                                ]]
                        if include_a2:
                                if self.sessions[session][&#39;scrambling_drift&#39;]:
                                        out[-1] += [f&#34;{self.sessions[session][&#39;a2&#39;]:.1e} ± {self.sessions[session][&#39;SE_a2&#39;]:.1e}&#34;]
                                else:
                                        out[-1] += [&#39;&#39;]
                        if include_b2:
                                if self.sessions[session][&#39;slope_drift&#39;]:
                                        out[-1] += [f&#34;{self.sessions[session][&#39;b2&#39;]:.1e} ± {self.sessions[session][&#39;SE_b2&#39;]:.1e}&#34;]
                                else:
                                        out[-1] += [&#39;&#39;]
                        if include_c2:
                                if self.sessions[session][&#39;wg_drift&#39;]:
                                        out[-1] += [f&#34;{self.sessions[session][&#39;c2&#39;]:.1e} ± {self.sessions[session][&#39;SE_c2&#39;]:.1e}&#34;]
                                else:
                                        out[-1] += [&#39;&#39;]

                if save_to_file:
                        if not os.path.exists(dir):
                                os.makedirs(dir)
                        if filename is None:
                                filename = f&#39;D{self._4x}_sessions.csv&#39;
                        with open(f&#39;{dir}/{filename}&#39;, &#39;w&#39;) as fid:
                                fid.write(make_csv(out))
                if print_out:
                        self.msg(&#39;\n&#39; + pretty_table(out))
                return out


        @make_verbal
        def table_of_analyses(
                self,
                dir = &#39;output&#39;,
                filename = None,
                save_to_file = True,
                print_out = True,
                ):
                &#39;&#39;&#39;
                Print out an/or save to disk a table of analyses.

                __Parameters__

                + `dir`: the directory in which to save the table
                + `filename`: the name to the csv file to write to
                + `save_to_file`: whether to save the table to disk
                + `print_out`: whether to print out the table
                &#39;&#39;&#39;

                out = [[&#39;UID&#39;,&#39;Session&#39;,&#39;Sample&#39;]]
                extra_fields = [f for f in [(&#39;SampleMass&#39;,&#39;.2f&#39;),(&#39;ColdFingerPressure&#39;,&#39;.1f&#39;),(&#39;AcidReactionYield&#39;,&#39;.3f&#39;)] if f[0] in {k for r in self for k in r}]
                for f in extra_fields:
                        out[-1] += [f[0]]
                out[-1] += [&#39;d13Cwg_VPDB&#39;,&#39;d18Owg_VSMOW&#39;,&#39;d45&#39;,&#39;d46&#39;,&#39;d47&#39;,&#39;d48&#39;,&#39;d49&#39;,&#39;d13C_VPDB&#39;,&#39;d18O_VSMOW&#39;,&#39;D47raw&#39;,&#39;D48raw&#39;,&#39;D49raw&#39;,f&#39;D{self._4x}&#39;]
                for r in self:
                        out += [[f&#34;{r[&#39;UID&#39;]}&#34;,f&#34;{r[&#39;Session&#39;]}&#34;,f&#34;{r[&#39;Sample&#39;]}&#34;]]
                        for f in extra_fields:
                                out[-1] += [f&#34;{r[f[0]]:{f[1]}}&#34;]
                        out[-1] += [
                                f&#34;{r[&#39;d13Cwg_VPDB&#39;]:.3f}&#34;,
                                f&#34;{r[&#39;d18Owg_VSMOW&#39;]:.3f}&#34;,
                                f&#34;{r[&#39;d45&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;d46&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;d47&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;d48&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;d49&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;d13C_VPDB&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;d18O_VSMOW&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;D47raw&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;D48raw&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;D49raw&#39;]:.6f}&#34;,
                                f&#34;{r[f&#39;D{self._4x}&#39;]:.6f}&#34;
                                ]
                if save_to_file:
                        if not os.path.exists(dir):
                                os.makedirs(dir)
                        if filename is None:
                                filename = f&#39;D{self._4x}_analyses.csv&#39;
                        with open(f&#39;{dir}/{filename}&#39;, &#39;w&#39;) as fid:
                                fid.write(make_csv(out))
                if print_out:
                        self.msg(&#39;\n&#39; + pretty_table(out))
                return out


        @make_verbal
        def table_of_samples(
                self,
                dir = &#39;output&#39;,
                filename = None,
                save_to_file = True,
                print_out = True,
                ):
                &#39;&#39;&#39;
                Print out an/or save to disk a table of samples.

                __Parameters__

                + `dir`: the directory in which to save the table
                + `filename`: the name to the csv file to write to
                + `save_to_file`: whether to save the table to disk
                + `print_out`: whether to print out the table
                &#39;&#39;&#39;

                out = [[&#39;Sample&#39;,&#39;N&#39;,&#39;d13C_VPDB&#39;,&#39;d18O_VSMOW&#39;,f&#39;D{self._4x}&#39;,&#39;SE&#39;,&#39;95% CL&#39;,&#39;SD&#39;,&#39;p_Levene&#39;]]
                for sample in self.anchors:
                        out += [[
                                f&#34;{sample}&#34;,
                                f&#34;{self.samples[sample][&#39;N&#39;]}&#34;,
                                f&#34;{self.samples[sample][&#39;d13C_VPDB&#39;]:.2f}&#34;,
                                f&#34;{self.samples[sample][&#39;d18O_VSMOW&#39;]:.2f}&#34;,
                                f&#34;{self.samples[sample][f&#39;D{self._4x}&#39;]:.4f}&#34;,&#39;&#39;,&#39;&#39;,
                                f&#34;{self.samples[sample][f&#39;SD_D{self._4x}&#39;]:.4f}&#34; if self.samples[sample][&#39;N&#39;] &gt; 1 else &#39;&#39;, &#39;&#39;
                                ]]
                for sample in self.unknowns:
                        out += [[
                                f&#34;{sample}&#34;,
                                f&#34;{self.samples[sample][&#39;N&#39;]}&#34;,
                                f&#34;{self.samples[sample][&#39;d13C_VPDB&#39;]:.2f}&#34;,
                                f&#34;{self.samples[sample][&#39;d18O_VSMOW&#39;]:.2f}&#34;,
                                f&#34;{self.samples[sample][f&#39;D{self._4x}&#39;]:.4f}&#34;,
                                f&#34;{self.samples[sample][f&#39;SE_D{self._4x}&#39;]:.4f}&#34;,
                                f&#34;± {self.samples[sample][f&#39;SE_D{self._4x}&#39;] * self.t95:.4f}&#34;,
                                f&#34;{self.samples[sample][f&#39;SD_D{self._4x}&#39;]:.4f}&#34; if self.samples[sample][&#39;N&#39;] &gt; 1 else &#39;&#39;,
                                f&#34;{self.samples[sample][&#39;p_Levene&#39;]:.3f}&#34; if self.samples[sample][&#39;N&#39;] &gt; 2 else &#39;&#39;
                                ]]
                if save_to_file:
                        if not os.path.exists(dir):
                                os.makedirs(dir)
                        if filename is None:
                                filename = f&#39;D{self._4x}_samples.csv&#39;
                        with open(f&#39;{dir}/{filename}&#39;, &#39;w&#39;) as fid:
                                fid.write(make_csv(out))
                if print_out:
                        self.msg(&#39;\n&#39;+pretty_table(out))


        def plot_sessions(self, dir = &#39;output&#39;, figsize = (8,8)):
                &#39;&#39;&#39;
                Generate session plots and save them to disk.

                __Parameters__

                + `dir`: the directory in which to save the plots
                + `figsize`: the width and height (in inches) of each plot
                &#39;&#39;&#39;
                if not os.path.exists(dir):
                        os.makedirs(dir)

                for session in self.sessions:
                        sp = self.plot_single_session(session, xylimits = &#39;constant&#39;)
                        ppl.savefig(f&#39;{dir}/D{self._4x}_plot_{session}.pdf&#39;)
                        ppl.close(sp.fig)


        @make_verbal
        def consolidate_samples(self):
                &#39;&#39;&#39;
                Compile various statistics for each sample.

                For each anchor sample:

                + `D47`: the nominal Δ&lt;sub&gt;47&lt;/sub&gt; value for this anchor, specified by `self.Nominal_D47`
                + `SE_D47`: set to zero by definition

                For each unknown sample:

                + `D47`: the standardized Δ&lt;sub&gt;47&lt;/sub&gt; value for this unknown
                + `SE_D47`: the standard error of Δ&lt;sub&gt;47&lt;/sub&gt; for this unknown

                For each anchor and unknown:

                + `N`: the total number of analyses of this sample
                + `SD_D47`: the “sample” (in the statistical sense) standard deviation for this sample
                + `d13C_VPDB`: the average δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; value for this sample
                + `d18O_VSMOW`: the average δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt; value for this sample (as CO&lt;sub&gt;2&lt;/sub&gt;)
                + `p_Levene`: the p-value from a [Levene test] of equal variance, indicating whether
                the Δ&lt;sub&gt;47&lt;/sub&gt; repeatability this sample differs significantly from that observed
                for the reference sample specified by `self.LEVENE_REF_SAMPLE`.

                [Levene test]: https://en.wikipedia.org/wiki/Levene%27s_test
                &#39;&#39;&#39;
                D4x_ref_pop = [r[f&#39;D{self._4x}&#39;] for r in self.samples[self.LEVENE_REF_SAMPLE][&#39;data&#39;]]
                for sample in self.samples:
                        self.samples[sample][&#39;N&#39;] = len(self.samples[sample][&#39;data&#39;])
                        if self.samples[sample][&#39;N&#39;] &gt; 1:
                                self.samples[sample][f&#39;SD_D{self._4x}&#39;] = stdev([r[f&#39;D{self._4x}&#39;] for r in self.samples[sample][&#39;data&#39;]])

                        self.samples[sample][&#39;d13C_VPDB&#39;] = np.mean([r[&#39;d13C_VPDB&#39;] for r in self.samples[sample][&#39;data&#39;]])
                        self.samples[sample][&#39;d18O_VSMOW&#39;] = np.mean([r[&#39;d18O_VSMOW&#39;] for r in self.samples[sample][&#39;data&#39;]])

                        D4x_pop = [r[f&#39;D{self._4x}&#39;] for r in self.samples[sample][&#39;data&#39;]]
                        if len(D4x_pop) &gt; 2:
                                self.samples[sample][&#39;p_Levene&#39;] = levene(D4x_ref_pop, D4x_pop, center = &#39;median&#39;)[1]

                if self.standardization_method == &#39;pooled&#39;:
                        for sample in self.anchors:
                                self.samples[sample][f&#39;D{self._4x}&#39;] = self.Nominal_D4x[sample]
                                self.samples[sample][f&#39;SE_D{self._4x}&#39;] = 0.
                        for sample in self.unknowns:
                                self.samples[sample][f&#39;D{self._4x}&#39;] = self.standardization.params.valuesdict()[f&#39;D{self._4x}_{pf(sample)}&#39;]
                                try:
                                        self.samples[sample][f&#39;SE_D{self._4x}&#39;] = self.sample_D4x_covar(sample)**.5
                                except ValueError:
                                        # when `sample` is constrained by self.standardize(constraints = {...}),
                                        # it is no longer listed in self.standardization.var_names.
                                        # Temporary fix: define SE as zero for now
                                        self.samples[sample][f&#39;SE_D4{self._4x}&#39;] = 0.

                elif self.standardization_method == &#39;indep_sessions&#39;:
                        for sample in self.anchors:
                                self.samples[sample][f&#39;D{self._4x}&#39;] = self.Nominal_D4x[sample]
                                self.samples[sample][f&#39;SE_D{self._4x}&#39;] = 0.
                        for sample in self.unknowns:
                                self.msg(f&#39;Consolidating sample {sample}&#39;)
                                self.unknowns[sample][f&#39;session_D{self._4x}&#39;] = {}
                                session_avg = []
                                for session in self.sessions:
                                        sdata = [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]
                                        if sdata:
                                                self.msg(f&#39;{sample} found in session {session}&#39;)
                                                avg_D4x = np.mean([r[f&#39;D{self._4x}&#39;] for r in sdata])
                                                avg_d4x = np.mean([r[f&#39;d{self._4x}&#39;] for r in sdata])
                                                # !! TODO: sigma_s below does not account for temporal changes in standardization error
                                                sigma_s = self.standardization_error(session, avg_d4x, avg_D4x)
                                                sigma_u = sdata[0][f&#39;wD{self._4x}raw&#39;] / self.sessions[session][&#39;a&#39;] / len(sdata)**.5
                                                session_avg.append([avg_D4x, (sigma_u**2 + sigma_s**2)**.5])
                                                self.unknowns[sample][f&#39;session_D{self._4x}&#39;][session] = session_avg[-1]
                                self.samples[sample][f&#39;D{self._4x}&#39;], self.samples[sample][f&#39;SE_D{self._4x}&#39;] = w_avg(*zip(*session_avg))
                                weights = {s: self.unknowns[sample][f&#39;session_D{self._4x}&#39;][s][1]**-2 for s in self.unknowns[sample][f&#39;session_D{self._4x}&#39;]}
                                wsum = sum([weights[s] for s in weights])
                                for s in weights:
                                        self.unknowns[sample][f&#39;session_D{self._4x}&#39;][s] += [self.unknowns[sample][f&#39;session_D{self._4x}&#39;][s][1]**-2 / wsum]


        def consolidate_sessions(self):
                &#39;&#39;&#39;
                Compile various statistics for each session.

                + `Na`: Number of anchor analyses in the session
                + `Nu`: Number of unknown analyses in the session
                + `r_d13C_VPDB`: δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; repeatability of analyses within the session
                + `r_d18O_VSMOW`: δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt; repeatability of analyses within the session
                + `r_D47`: Δ&lt;sub&gt;47&lt;/sub&gt; repeatability of analyses within the session
                + `a`: scrambling factor
                + `b`: compositional slope
                + `c`: WG offset
                + `SE_a`: Model stadard erorr of `a`
                + `SE_b`: Model stadard erorr of `b`
                + `SE_c`: Model stadard erorr of `c`
                + `scrambling_drift` (boolean): whether to allow a temporal drift in the scrambling factor (`a`)
                + `slope_drift` (boolean): whether to allow a temporal drift in the compositional slope (`b`)
                + `wg_drift` (boolean): whether to allow a temporal drift in the WG offset (`c`)
                + `a2`: scrambling factor drift
                + `b2`: compositional slope drift
                + `c2`: WG offset drift
                + `Np`: Number of standardization parameters to fit
                + `CM`: model covariance matrix for (`a`, `b`, `c`, `a2`, `b2`, `c2`)
                + `d13Cwg_VPDB`: δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; of WG
                + `d18Owg_VSMOW`: δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt; of WG
                &#39;&#39;&#39;
                for session in self.sessions:
                        if &#39;d13Cwg_VPDB&#39; not in self.sessions[session]:
                                self.sessions[session][&#39;d13Cwg_VPDB&#39;] = self.sessions[session][&#39;data&#39;][0][&#39;d13Cwg_VPDB&#39;]
                        if &#39;d18Owg_VSMOW&#39; not in self.sessions[session]:
                                self.sessions[session][&#39;d18Owg_VSMOW&#39;] = self.sessions[session][&#39;data&#39;][0][&#39;d18Owg_VSMOW&#39;]
                        self.sessions[session][&#39;Na&#39;] = len([r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.anchors])
                        self.sessions[session][&#39;Nu&#39;] = len([r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.unknowns])

                        self.msg(f&#39;Computing repeatabilities for session {session}&#39;)
                        self.sessions[session][&#39;r_d13C_VPDB&#39;] = self.compute_r(&#39;d13C_VPDB&#39;, samples = &#39;anchors&#39;, sessions = [session])
                        self.sessions[session][&#39;r_d18O_VSMOW&#39;] = self.compute_r(&#39;d18O_VSMOW&#39;, samples = &#39;anchors&#39;, sessions = [session])
                        self.sessions[session][f&#39;r_D{self._4x}&#39;] = self.compute_r(f&#39;D{self._4x}&#39;, sessions = [session])

                if self.standardization_method == &#39;pooled&#39;:
                        for session in self.sessions:

                                self.sessions[session][&#39;a&#39;] = self.standardization.params.valuesdict()[f&#39;a_{pf(session)}&#39;]
                                i = self.standardization.var_names.index(f&#39;a_{pf(session)}&#39;)
                                self.sessions[session][&#39;SE_a&#39;] = self.standardization.covar[i,i]**.5

                                self.sessions[session][&#39;b&#39;] = self.standardization.params.valuesdict()[f&#39;b_{pf(session)}&#39;]
                                i = self.standardization.var_names.index(f&#39;b_{pf(session)}&#39;)
                                self.sessions[session][&#39;SE_b&#39;] = self.standardization.covar[i,i]**.5

                                self.sessions[session][&#39;c&#39;] = self.standardization.params.valuesdict()[f&#39;c_{pf(session)}&#39;]
                                i = self.standardization.var_names.index(f&#39;c_{pf(session)}&#39;)
                                self.sessions[session][&#39;SE_c&#39;] = self.standardization.covar[i,i]**.5

                                self.sessions[session][&#39;a2&#39;] = self.standardization.params.valuesdict()[f&#39;a2_{pf(session)}&#39;]
                                if self.sessions[session][&#39;scrambling_drift&#39;]:
                                        i = self.standardization.var_names.index(f&#39;a2_{pf(session)}&#39;)
                                        self.sessions[session][&#39;SE_a2&#39;] = self.standardization.covar[i,i]**.5
                                else:
                                        self.sessions[session][&#39;SE_a2&#39;] = 0.

                                self.sessions[session][&#39;b2&#39;] = self.standardization.params.valuesdict()[f&#39;b2_{pf(session)}&#39;]
                                if self.sessions[session][&#39;slope_drift&#39;]:
                                        i = self.standardization.var_names.index(f&#39;b2_{pf(session)}&#39;)
                                        self.sessions[session][&#39;SE_b2&#39;] = self.standardization.covar[i,i]**.5
                                else:
                                        self.sessions[session][&#39;SE_b2&#39;] = 0.

                                self.sessions[session][&#39;c2&#39;] = self.standardization.params.valuesdict()[f&#39;c2_{pf(session)}&#39;]
                                if self.sessions[session][&#39;wg_drift&#39;]:
                                        i = self.standardization.var_names.index(f&#39;c2_{pf(session)}&#39;)
                                        self.sessions[session][&#39;SE_c2&#39;] = self.standardization.covar[i,i]**.5
                                else:
                                        self.sessions[session][&#39;SE_c2&#39;] = 0.

                                i = self.standardization.var_names.index(f&#39;a_{pf(session)}&#39;)
                                j = self.standardization.var_names.index(f&#39;b_{pf(session)}&#39;)
                                k = self.standardization.var_names.index(f&#39;c_{pf(session)}&#39;)
                                CM = np.zeros((6,6))
                                CM[:3,:3] = self.standardization.covar[[i,j,k],:][:,[i,j,k]]
                                try:
                                        i2 = self.standardization.var_names.index(f&#39;a2_{pf(session)}&#39;)
                                        CM[3,[0,1,2,3]] = self.standardization.covar[i2,[i,j,k,i2]]
                                        CM[[0,1,2,3],3] = self.standardization.covar[[i,j,k,i2],i2]
                                        try:
                                                j2 = self.standardization.var_names.index(f&#39;b2_{pf(session)}&#39;)
                                                CM[3,4] = self.standardization.covar[i2,j2]
                                                CM[4,3] = self.standardization.covar[j2,i2]
                                        except ValueError:
                                                pass
                                        try:
                                                k2 = self.standardization.var_names.index(f&#39;c2_{pf(session)}&#39;)
                                                CM[3,5] = self.standardization.covar[i2,k2]
                                                CM[5,3] = self.standardization.covar[k2,i2]
                                        except ValueError:
                                                pass
                                except ValueError:
                                        pass
                                try:
                                        j2 = self.standardization.var_names.index(f&#39;b2_{pf(session)}&#39;)
                                        CM[4,[0,1,2,4]] = self.standardization.covar[j2,[i,j,k,j2]]
                                        CM[[0,1,2,4],4] = self.standardization.covar[[i,j,k,j2],j2]
                                        try:
                                                k2 = self.standardization.var_names.index(f&#39;c2_{pf(session)}&#39;)
                                                CM[4,5] = self.standardization.covar[j2,k2]
                                                CM[5,4] = self.standardization.covar[k2,j2]
                                        except ValueError:
                                                pass
                                except ValueError:
                                        pass
                                try:
                                        k2 = self.standardization.var_names.index(f&#39;c2_{pf(session)}&#39;)
                                        CM[5,[0,1,2,5]] = self.standardization.covar[k2,[i,j,k,k2]]
                                        CM[[0,1,2,5],5] = self.standardization.covar[[i,j,k,k2],k2]
                                except ValueError:
                                        pass

                                self.sessions[session][&#39;CM&#39;] = CM

                elif self.standardization_method == &#39;indep_sessions&#39;:
                        pass # Not implemented yet


        @make_verbal
        def repeatabilities(self):
                &#39;&#39;&#39;
                Compute analytical repeatabilities for δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt;,
                δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt;, Δ&lt;sub&gt;47&lt;/sub&gt; (for all samples, for anchors,
                and for unknowns).
                &#39;&#39;&#39;
                self.msg(&#39;Computing reproducibilities for all sessions&#39;)

                self.repeatability[&#39;r_d13C_VPDB&#39;] = self.compute_r(&#39;d13C_VPDB&#39;, samples = &#39;anchors&#39;)
                self.repeatability[&#39;r_d18O_VSMOW&#39;] = self.compute_r(&#39;d18O_VSMOW&#39;, samples = &#39;anchors&#39;)
                self.repeatability[f&#39;r_D{self._4x}a&#39;] = self.compute_r(f&#39;D{self._4x}&#39;, samples = &#39;anchors&#39;)
                self.repeatability[f&#39;r_D{self._4x}u&#39;] = self.compute_r(f&#39;D{self._4x}&#39;, samples = &#39;unknowns&#39;)
                self.repeatability[f&#39;r_D{self._4x}&#39;] = self.compute_r(f&#39;D{self._4x}&#39;, samples = &#39;all samples&#39;)


        @make_verbal
        def consolidate(self, tables = True, plots = True):
                &#39;&#39;&#39;
                Collect information about samples, sessions and repeatabilities.
                &#39;&#39;&#39;
                self.consolidate_samples()
                self.consolidate_sessions()
                self.repeatabilities()

                if tables:
                        self.summary()
                        self.table_of_sessions()
                        self.table_of_analyses()
                        self.table_of_samples()

                if plots:
                        self.plot_sessions()


        @make_verbal
        def rmswd(self,
                samples = &#39;all samples&#39;,
                sessions = &#39;all sessions&#39;,
                ):
                &#39;&#39;&#39;
                Compute the root mean squared weighted deviation, χ&lt;sup&gt;2&lt;/sup&gt; and
                corresponding degrees of freedom of `[r[&#39;D47&#39;] for r in self]`.
                
                Currently used only in `D4xdata.standardize(method = &#39;indep_sessions&#39;)`
                &#39;&#39;&#39;
                if samples == &#39;all samples&#39;:
                        mysamples = [k for k in self.samples]
                elif samples == &#39;anchors&#39;:
                        mysamples = [k for k in self.anchors]
                elif samples == &#39;unknowns&#39;:
                        mysamples = [k for k in self.unknowns]
                else:
                        mysamples = samples

                if sessions == &#39;all sessions&#39;:
                        sessions = [k for k in self.sessions]

                chisq, Nf = 0, 0
                for sample in mysamples :
                        G = [ r for r in self if r[&#39;Sample&#39;] == sample and r[&#39;Session&#39;] in sessions ]
                        if len(G) &gt; 1 :
                                X, sX = w_avg([r[f&#39;D{self._4x}&#39;] for r in G], [r[f&#39;wD{self._4x}&#39;] for r in G])
                                Nf += (len(G) - 1)
                                chisq += np.sum([ ((r[f&#39;D{self._4x}&#39;]-X)/r[f&#39;wD{self._4x}&#39;])**2 for r in G])
                r = (chisq / Nf)**.5 if Nf &gt; 0 else 0
                self.msg(f&#39;RMSWD of r[&#34;D{self._4x}&#34;] is {r:.6f} for {samples}.&#39;)
                return {&#39;rmswd&#39;: r, &#39;chisq&#39;: chisq, &#39;Nf&#39;: Nf}

        
        @make_verbal
        def compute_r(self, key, samples = &#39;all samples&#39;, sessions = &#39;all sessions&#39;):
                &#39;&#39;&#39;
                Compute the repeatability of `[r[key] for r in self]`
                &#39;&#39;&#39;
                # NB: it&#39;s debatable whether rD47 should be computed
                # with Nf = len(self)-len(self.samples) instead of
                # Nf = len(self) - len(self.unknwons) - 3*len(self.sessions)

                if samples == &#39;all samples&#39;:
                        mysamples = [k for k in self.samples]
                elif samples == &#39;anchors&#39;:
                        mysamples = [k for k in self.anchors]
                elif samples == &#39;unknowns&#39;:
                        mysamples = [k for k in self.unknowns]
                else:
                        mysamples = samples

                if sessions == &#39;all sessions&#39;:
                        sessions = [k for k in self.sessions]

                if key in [&#39;D47&#39;, &#39;D48&#39;]:
                        chisq, Nf = 0, 0
                        for sample in mysamples :
                                X = [ r[key] for r in self if r[&#39;Sample&#39;] == sample and r[&#39;Session&#39;] in sessions ]
                                if len(X) &gt; 1 :
                                        chisq += np.sum([ (x-self.samples[sample][key])**2 for x in X ])
                                        if sample in self.unknowns:
                                                Nf += len(X) - 1
                                        else:
                                                Nf += len(X)
                        if samples in [&#39;anchors&#39;, &#39;all samples&#39;]:
                                Nf -= sum([self.sessions[s][&#39;Np&#39;] for s in sessions])
                        r = (chisq / Nf)**.5 if Nf &gt; 0 else 0

                else: # if key not in [&#39;D47&#39;, &#39;D48&#39;]
                        chisq, Nf = 0, 0
                        for sample in mysamples :
                                X = [ r[key] for r in self if r[&#39;Sample&#39;] == sample and r[&#39;Session&#39;] in sessions ]
                                if len(X) &gt; 1 :
                                        Nf += len(X) - 1
                                        chisq += np.sum([ (x-np.mean(X))**2 for x in X ])
                        r = (chisq / Nf)**.5 if Nf &gt; 0 else 0

                self.msg(f&#39;Repeatability of r[&#34;{key}&#34;] is {1000*r:.1f} ppm for {samples}.&#39;)
                return r

        def sample_average(self, samples, weights = &#39;equal&#39;, normalize = True):
                &#39;&#39;&#39;
                Weighted average Δ&lt;sub&gt;47&lt;/sub&gt; value of a group of samples, accounting for covariance.

                Returns the weighed average Δ47 value and associated SE
                of a group of samples. Weights are equal by default. If `normalize` is
                true, `weights` will be rescaled so that their sum equals 1.

                __Examples__

                ```python
                self.sample_average([&#39;X&#39;,&#39;Y&#39;], [1, 2])
                ```

                returns the value and SE of [Δ&lt;sub&gt;47&lt;/sub&gt;(X) + 2 Δ&lt;sub&gt;47&lt;/sub&gt;(Y)]/3,
                where Δ&lt;sub&gt;47&lt;/sub&gt;(X) and Δ&lt;sub&gt;47&lt;/sub&gt;(Y) are the average Δ&lt;sub&gt;47&lt;/sub&gt;
                values of samples X and Y, respectively.

                ```python
                self.sample_average([&#39;X&#39;,&#39;Y&#39;], [1, -1], normalize = False)
                ```

                returns the value and SE of the difference Δ&lt;sub&gt;47&lt;/sub&gt;(X) - Δ&lt;sub&gt;47&lt;/sub&gt;(Y).
                &#39;&#39;&#39;
                if weights == &#39;equal&#39;:
                        weights = [1/len(samples)] * len(samples)

                if normalize:
                        s = sum(weights)
                        if s:
                                weights = [w/s for w in weights]

                try:
#                       indices = [self.standardization.var_names.index(f&#39;D47_{pf(sample)}&#39;) for sample in samples]
#                       C = self.standardization.covar[indices,:][:,indices]
                        C = np.array([[self.sample_D4x_covar(x, y) for x in samples] for y in samples])
                        X = [self.samples[sample][f&#39;D{self._4x}&#39;] for sample in samples]
                        return correlated_sum(X, C, weights)
                except ValueError:
                        return (0., 0.)


        def sample_D4x_covar(self, sample1, sample2 = None):
                &#39;&#39;&#39;
                Covariance between Δ&lt;sub&gt;47&lt;/sub&gt; values of samples

                Returns the error covariance between the average Δ&lt;sub&gt;47&lt;/sub&gt; values of two
                samples. If if only `sample_1` is specified, or if `sample_1 == sample_2`),
                returns the Δ&lt;sub&gt;47&lt;/sub&gt; variance for that sample.
                &#39;&#39;&#39;
                if sample2 is None:
                        sample2 = sample1
                if self.standardization_method == &#39;pooled&#39;:
                        i = self.standardization.var_names.index(f&#39;D{self._4x}_{pf(sample1)}&#39;)
                        j = self.standardization.var_names.index(f&#39;D{self._4x}_{pf(sample2)}&#39;)
                        return self.standardization.covar[i, j]
                elif self.standardization_method == &#39;indep_sessions&#39;:
                        if sample1 == sample2:
                                return self.samples[sample1][f&#39;SE_D{self._4x}&#39;]**2
                        else:
                                c = 0
                                for session in self.sessions:
                                        sdata1 = [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample1]
                                        sdata2 = [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample2]
                                        if sdata1 and sdata2:
                                                a = self.sessions[session][&#39;a&#39;]
                                                # !! TODO: CM below does not account for temporal changes in standardization parameters
                                                CM = self.sessions[session][&#39;CM&#39;][:3,:3]
                                                avg_D4x_1 = np.mean([r[f&#39;D{self._4x}&#39;] for r in sdata1])
                                                avg_d4x_1 = np.mean([r[f&#39;d{self._4x}&#39;] for r in sdata1])
                                                avg_D4x_2 = np.mean([r[f&#39;D{self._4x}&#39;] for r in sdata2])
                                                avg_d4x_2 = np.mean([r[f&#39;d{self._4x}&#39;] for r in sdata2])
                                                c += (
                                                        self.unknowns[sample1][f&#39;session_D{self._4x}&#39;][session][2]
                                                        * self.unknowns[sample2][f&#39;session_D{self._4x}&#39;][session][2]
                                                        * np.array([[avg_D4x_1, avg_d4x_1, 1]])
                                                        @ CM
                                                        @ np.array([[avg_D4x_2, avg_d4x_2, 1]]).T
                                                        ) / a**2
                                return float(c)

        def sample_D4x_correl(self, sample1, sample2 = None):
                &#39;&#39;&#39;
                Correlation between Δ&lt;sub&gt;47&lt;/sub&gt; errors of samples

                Returns the error correlation between the average Δ47 values of two samples.
                &#39;&#39;&#39;
                if sample2 is None or sample2 == sample1:
                        return 1.
                return (
                        self.sample_D4x_covar(sample1, sample2)
                        / self.unknowns[sample1][f&#39;SE_D{self._4x}&#39;]
                        / self.unknowns[sample2][f&#39;SE_D{self._4x}&#39;]
                        )

        def plot_single_session(self,
                session,
                kw_plot_anchors = dict(ls=&#39;None&#39;, marker=&#39;x&#39;, mec=(.75, 0, 0), mew = .75, ms = 4),
                kw_plot_unknowns = dict(ls=&#39;None&#39;, marker=&#39;x&#39;, mec=(0, 0, .75), mew = .75, ms = 4),
                kw_plot_anchor_avg = dict(ls=&#39;-&#39;, marker=&#39;None&#39;, color=(.75, 0, 0), lw = .75),
                kw_plot_unknown_avg = dict(ls=&#39;-&#39;, marker=&#39;None&#39;, color=(0, 0, .75), lw = .75),
                kw_contour_error = dict(colors = [[0, 0, 0]], alpha = .5, linewidths = 0.75),
                xylimits = &#39;free&#39;, # | &#39;constant&#39;
                x_label = None,
                y_label = None,
                error_contour_interval = &#39;auto&#39;,
                fig = &#39;new&#39;,
                ):
                &#39;&#39;&#39;
                Generate plot for a single session
                &#39;&#39;&#39;
                if x_label is None:
                        x_label = f&#39;δ$_{{{self._4x}}}$ (‰)&#39;,
                if y_label is None:
                        y_label = f&#39;Δ$_{{{self._4x}}}$ (‰)&#39;,

                out = SessionPlot()
                anchors = [a for a in self.anchors if [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == a]]
                unknowns = [u for u in self.unknowns if [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == u]]
                
                if fig == &#39;new&#39;:
                        out.fig = ppl.figure(figsize = (6,6))
                        ppl.subplots_adjust(.1,.1,.9,.9)

                out.anchor_analyses, = ppl.plot(
                        [r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.anchors],
                        [r[f&#39;D{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.anchors],
                        **kw_plot_anchors)
                out.unknown_analyses, = ppl.plot(
                        [r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.unknowns],
                        [r[f&#39;D{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.unknowns],
                        **kw_plot_unknowns)
                out.anchor_avg = ppl.plot(
                        np.array([ np.array([
                                np.min([r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]) - 1,
                                np.max([r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]) + 1
                                ]) for sample in anchors]).T,
                        np.array([ np.array([0, 0]) + self.Nominal_D4x[sample] for sample in anchors]).T,
                        &#39;-&#39;, **kw_plot_anchor_avg)
                out.unknown_avg = ppl.plot(
                        np.array([ np.array([
                                np.min([r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]) - 1,
                                np.max([r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]) + 1
                                ]) for sample in unknowns]).T,
                        np.array([ np.array([0, 0]) + self.unknowns[sample][f&#39;D{self._4x}&#39;] for sample in unknowns]).T,
                        &#39;-&#39;, **kw_plot_unknown_avg)
                if xylimits == &#39;constant&#39;:
                        x = [r[f&#39;d{self._4x}&#39;] for r in self]
                        y = [r[f&#39;D{self._4x}&#39;] for r in self]
                        x1, x2, y1, y2 = np.min(x), np.max(x), np.min(y), np.max(y)
                        w, h = x2-x1, y2-y1
                        x1 -= w/20
                        x2 += w/20
                        y1 -= h/20
                        y2 += h/20
                        ppl.axis([x1, x2, y1, y2])
                elif xylimits == &#39;free&#39;:
                        x1, x2, y1, y2 = ppl.axis()
                else:
                        x1, x2, y1, y2 = ppl.axis(xylimits)
                                
                if error_contour_interval != &#39;none&#39;:
                        xi, yi = np.linspace(x1, x2), np.linspace(y1, y2)
                        XI,YI = np.meshgrid(xi, yi)
                        SI = np.array([[self.standardization_error(session, x, y) for x in xi] for y in yi])
                        if error_contour_interval == &#39;auto&#39;:
                                rng = np.max(SI) - np.min(SI)
                                if rng &lt;= 0.01:
                                        cinterval = 0.001
                                elif rng &lt;= 0.03:
                                        cinterval = 0.004
                                elif rng &lt;= 0.1:
                                        cinterval = 0.01
                                elif rng &lt;= 0.3:
                                        cinterval = 0.03
                                elif rng &lt;= 1.:
                                        cinterval = 0.1
                                else:
                                        cinterval = 0.5
                        else:
                                cinterval = error_contour_interval

                        cval = np.arange(np.ceil(SI.min() / .001) * .001, np.ceil(SI.max() / .001 + 1) * .001, cinterval)
                        out.contour = ppl.contour(XI, YI, SI, cval, **kw_contour_error)
                        out.clabel = ppl.clabel(out.contour)

                ppl.xlabel(x_label)
                ppl.ylabel(y_label)
                ppl.title(session, weight = &#39;bold&#39;)
                ppl.grid(alpha = .2)
                out.ax = ppl.gca()              

                return out

        def plot_residuals(self, dir = &#39;output&#39;, filename = None, highlight = [], colors = None):
                &#39;&#39;&#39;
                Plot residuals of each analysis as a function of time (actually, as a function of
                the order of analyses in the D47data() object)

                + `dir`: the directory in which to save the plot
                + `highlight`: a list of samples to highlight
                + `colors`: a dict of {&lt;sample&gt;: &lt;color&gt;} for all samples
                &#39;&#39;&#39;
                fig = ppl.figure(figsize = (8,4))
                ppl.subplots_adjust(.1,.05,.78,.8)
                N = len(self.anchors)
                if colors is None:
                        if len(highlight) &gt; 0:
                                Nh = len(highlight)
                                if Nh == 1:
                                        colors = {highlight[0]: (0,0,0)}
                                elif Nh == 3:
                                        colors = {a: c for a,c in zip(highlight, [(0,0,1), (1,0,0), (0,2/3,0)])}
                                elif Nh == 4:
                                        colors = {a: c for a,c in zip(highlight, [(0,0,1), (1,0,0), (0,2/3,0), (.75,0,.75)])}
                                else:
                                        colors = {a: hls_to_rgb(k/Nh, .4, 1) for k,a in enumerate(highlight)}
                        else:
                                if N == 3:
                                        colors = {a: c for a,c in zip(self.anchors, [(0,0,1), (1,0,0), (0,2/3,0)])}
                                elif N == 4:
                                        colors = {a: c for a,c in zip(self.anchors, [(0,0,1), (1,0,0), (0,2/3,0), (.75,0,.75)])}
                                else:
                                        colors = {a: hls_to_rgb(k/N, .4, 1) for k,a in enumerate(self.anchors)}
                session = self[0][&#39;Session&#39;]
                x1 = 0
#               ymax = np.max([1e3 * (r[&#39;D47&#39;] - self.samples[r[&#39;Sample&#39;]][&#39;D47&#39;]) for r in self])
                x_sessions = {}
                one_or_more_singlets = False
                one_or_more_multiplets = False
                for k,r in enumerate(self):
                        if r[&#39;Session&#39;] != session:
                                x2 = k-1
                                x_sessions[session] = (x1+x2)/2
                                ppl.axvline(k - 0.5, color = &#39;k&#39;, lw = .5)
                                session = r[&#39;Session&#39;]
                                x1 = k
                        singlet = len(self.samples[r[&#39;Sample&#39;]][&#39;data&#39;]) == 1
                        if r[&#39;Sample&#39;] in self.unknowns:
                                if singlet:
                                        one_or_more_singlets = True
                                else:
                                        one_or_more_multiplets = True
                        kw = dict(
                                marker = &#39;x&#39; if singlet else &#39;+&#39;,
                                ms = 4 if singlet else 5,
                                ls = &#39;None&#39;,
                                mec = colors[r[&#39;Sample&#39;]] if r[&#39;Sample&#39;] in colors else (0,0,0),
                                mew = 1,
                                alpha = 0.2 if singlet else 1,
                                )
                        if highlight and r[&#39;Sample&#39;] not in highlight:
                                kw[&#39;alpha&#39;] = 0.2
                        ppl.plot(k, 1e3 * (r[&#39;D47&#39;] - self.samples[r[&#39;Sample&#39;]][&#39;D47&#39;]), **kw)
                x2 = k
                x_sessions[session] = (x1+x2)/2

                ppl.axhspan(-self.repeatability[&#39;r_D47&#39;]*1000, self.repeatability[&#39;r_D47&#39;]*1000, color = &#39;k&#39;, alpha = .05, lw = 1)
                ppl.text(len(self), self.repeatability[&#39;r_D47&#39;]*1000, f&#34;   SD = {self.repeatability[&#39;r_D47&#39;]*1000:.1f} ppm&#34;, size = 9, alpha = .75, va = &#39;center&#39;)
                ppl.axhspan(-self.repeatability[&#39;r_D47&#39;]*1000*self.t95, self.repeatability[&#39;r_D47&#39;]*1000*self.t95, color = &#39;k&#39;, alpha = .05, lw = 1)
                ppl.text(len(self), self.repeatability[&#39;r_D47&#39;]*1000*self.t95, f&#34;   95% CL: ± {self.repeatability[&#39;r_D47&#39;]*1000*self.t95:.1f} ppm&#34;, size = 9, alpha = .75, va = &#39;center&#39;)

                ymax = ppl.axis()[3]
                for s in x_sessions:
                        ppl.text(
                                x_sessions[s],
                                ymax +1,
                                s,
                                va = &#39;bottom&#39;,
                                **(
                                        dict(ha = &#39;center&#39;)
                                        if len(self.sessions[s][&#39;data&#39;]) &gt; (0.15 * len(self))
                                        else dict(ha = &#39;left&#39;, rotation = 45)
                                        )
                                )

                for s in colors:
                        kw[&#39;marker&#39;] = &#39;+&#39;
                        kw[&#39;ms&#39;] = 5
                        kw[&#39;mec&#39;] = colors[s]
                        kw[&#39;label&#39;] = s
                        kw[&#39;alpha&#39;] = 1
                        ppl.plot([], [], **kw)

                kw[&#39;mec&#39;] = (0,0,0)

                if one_or_more_singlets:
                        kw[&#39;marker&#39;] = &#39;x&#39;
                        kw[&#39;ms&#39;] = 4
                        kw[&#39;alpha&#39;] = .2
                        kw[&#39;label&#39;] = &#39;other (N$\\,$=$\\,$1)&#39; if one_or_more_multiplets else &#39;other&#39;
                        ppl.plot([], [], **kw)

                if one_or_more_multiplets:
                        kw[&#39;marker&#39;] = &#39;+&#39;
                        kw[&#39;ms&#39;] = 4
                        kw[&#39;alpha&#39;] = 1
                        kw[&#39;label&#39;] = &#39;other (N$\\,$&gt;$\\,$1)&#39; if one_or_more_singlets else &#39;other&#39;
                        ppl.plot([], [], **kw)

                ppl.legend(loc = &#39;lower left&#39;, bbox_to_anchor = (1.03, 0), borderaxespad = 0)
                ppl.xticks([])
                ppl.ylabel(&#39;Δ$_{47}$ residuals (ppm)&#39;)
                ppl.axis([-1, len(self), None, None])

                if not os.path.exists(dir):
                        os.makedirs(dir)
                if filename is None:
                        return fig
                elif filename == &#39;&#39;:
                        filename = f&#39;D{self._4x}_residuals.pdf&#39;
                ppl.savefig(f&#39;{dir}/{filename}&#39;)
                ppl.close(fig)

        def simulate(self,
                samples = None,
                a = 1.,
                b = 0.,
                c = -0.9,
                rD4x = 0.015,
                seed = 0,
                ):
                &#39;&#39;&#39;
                Populate `D47data` instance with simulated analyses from a single session.
                
                __Parameters__

                + `samples`: a list of entries; each entry is a dictionary with the following fields:
                    * `Sample`: the name of the sample
                    * either `d47` (the δ&lt;sub&gt;47&lt;/sub&gt; value of this sample), or `d13C_VPDB` and `d18O_VPDB` (its δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; and δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VPDB&lt;/sub&gt; values)
                    * `D47`: the absolute Δ&lt;sub&gt;47&lt;/sub&gt; value of this sample
                    * `N`: how many analyses of this sample should be generated
                + `a`: scrambling factor)
                + `b`: compositional nonlinearity
                + `c`: working gas offset
                + `rD47`: Δ&lt;sub&gt;47&lt;/sub&gt; repeatability
                + `seed`: explicitly set to a non-zero value to achieve random but repeatable simulations
                
                Beware that `d47` values computed from `d13C_VPDB` and `d18O_VPDB` are calculated assuming
                a working gas with δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;0 and δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;0.
                In the unusual case where simulating a different working gas composition is necessary, `d47` must be specified explicitly.
                
                Samples already defined in `D47data.Nominal_d13C_VPDB`, `D47data.Nominal_d18O_VPDB`, and `D47data.Nominal_D47`
                do not require explicit `d47`, `D47`, `d13C_VPDB` nor `d18O_VPDB` (the nominal values will be used by default).
                
                Here is an example of using this method to simulate a given combination of anchors and unknowns:

                ````py
                import D47crunch
                D = D47crunch.D47data()
                D.simulate([
                    dict(Sample = &#39;ETH-1&#39;, N = 6),
                    dict(Sample = &#39;ETH-2&#39;, N = 6),
                    dict(Sample = &#39;ETH-3&#39;, N = 12),
                    dict(Sample = &#39;FOO&#39;, d13C_VPDB = -5., d18O_VPDB = -10., D47 = 0.3, N = 4),
                    ], rD47 = 0.010)
                D.standardize()
                D.plot_sessions()
                D.verbose = True
                D.table_of_samples()
                ````
                
                This should output something like:
                
                ````
                [table_of_samples] 
                ––––––  ––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
                Sample   N  d13C_VPDB  d18O_VSMOW     D47      SE    95% CL      SD  p_Levene
                ––––––  ––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
                ETH-1    6        nan         nan  0.2052                    0.0076          
                ETH-2    6        nan         nan  0.2085                    0.0089          
                ETH-3   12        nan         nan  0.6132                    0.0118          
                FOO      4        nan         nan  0.3031  0.0057  ± 0.0118  0.0104     0.572
                ––––––  ––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
                ````
                &#39;&#39;&#39;
                from numpy import random as nprandom
                if seed:
                        rng = nprandom.default_rng(seed)
                else:
                        rng = nprandom.default_rng()
                
                if samples is None:
                        samples = [dict(Sample = s, N = 4) for s in self.Nominal_D4x]
                        samples += [{
                                &#39;Sample&#39;: &#39;FOO&#39;,
                                f&#39;d{self._4x}&#39;: 0.,
                                f&#39;D{self._4x}&#39;: 0.5 if self._4x == &#39;47&#39; else 0.2,
                                &#39;N&#39;: 4,
                                }]

                N = sum([s[&#39;N&#39;] for s in samples])
                errors = rng.normal(loc = 0, scale = 1, size = N) # generate random measurement errors
                errors *= rD4x / stdev(errors) # scale errors to rD47
                
                k = 0
                for s in samples:
                
                        if f&#39;D{self._4x}&#39; not in s:
                                if s[&#39;Sample&#39;] not in self.Nominal_D4x:
                                        raise KeyError(f&#34;Sample {s[&#39;Sample&#39;]} is missing a D{self._4x} value and it is not defined in Nominal_D{self._4x}&#34;)
                                else:
                                        s[f&#39;D{self._4x}&#39;] = self.Nominal_D4x[s[&#39;Sample&#39;]]                                       

                        if f&#39;d{self._4x}&#39; not in s:

                                if &#39;d13C_VPDB&#39; not in s:
                                        if s[&#39;Sample&#39;] not in self.Nominal_d13C_VPDB:
                                                raise KeyError(f&#34;Sample {s[&#39;Sample&#39;]} is missing d{self._4x} and d13C_VPDB values, and it is not defined in Nominal_d13C_VPDB.&#34;)
                                        else:
                                                s[&#39;d13C_VPDB&#39;] = self.Nominal_d13C_VPDB[s[&#39;Sample&#39;]]

                                if &#39;d18O_VPDB&#39; not in s:
                                        if s[&#39;Sample&#39;] not in self.Nominal_d18O_VPDB:
                                                raise KeyError(f&#34;Sample {s[&#39;Sample&#39;]} is missing d{self._4x} and d18O_VPDB values, and it is not defined in Nominal_d18O_VPDB.&#34;)
                                        else:
                                                s[&#39;d18O_VPDB&#39;] = self.Nominal_d18O_VPDB[s[&#39;Sample&#39;]]

                                i = 2 if self._4x == &#39;47&#39; else 3
                                R4xwg = self.compute_isobar_ratios(self.R13_VPDB, self.R18_VPDB * self.ALPHA_18O_ACID_REACTION)[i]
                                R4xs = self.compute_isobar_ratios(
                                        self.R13_VPDB * (1 + s[&#39;d13C_VPDB&#39;]/1000),
                                        self.R18_VPDB * (1 + s[&#39;d18O_VPDB&#39;]/1000) * self.ALPHA_18O_ACID_REACTION,
                                        )[i]*(1+s[f&#39;D{self._4x}&#39;]/1000)
                                s[f&#39;d{self._4x}&#39;] = (R4xs/R4xwg-1)*1000
                                        
                        while s[&#39;N&#39;]:
                                self.append({
                                        &#39;Sample&#39;: s[&#39;Sample&#39;],
                                        &#39;d13Cwg_VPDB&#39;: 0.,
                                        &#39;d18Owg_VSMOW&#39;: (self.R18_VSMOW * self.ALPHA_18O_ACID_REACTION - 1) * 1000,
                                        &#39;d13C_VPDB&#39;: self[&#39;d13C_VPDB&#39;],
                                        &#39;d18O_VSMOW&#39;: self[&#39;d18O_VPDB&#39;],
                                        f&#39;d{self._4x}&#39;: s[f&#39;d{self._4x}&#39;],
                                        f&#39;D{self._4x}raw&#39;: a * (s[f&#39;D{self._4x}&#39;] + errors[k]) + b * s[f&#39;d{self._4x}&#39;] + c,
                                        })
                                s[&#39;N&#39;] -= 1
                                k += 1

                self.refresh()

        def plot_distribution_of_analyses(self, dir = &#39;output&#39;, filename = None, vs_time = False, output = None):
                &#39;&#39;&#39;
                Plot temporal distribution of all analyses.
                
                __Parameters__

                + `vs_time`: if `True`, plot as a function of `TimeTag` rather than sequentially.
                &#39;&#39;&#39;

                asamples = [s for s in self.anchors]
                usamples = [s for s in self.unknowns]
                if output is None or output == &#39;fig&#39;:
                        fig = ppl.figure(figsize = (6,4))
                        ppl.subplots_adjust(0.02, 0.03, 0.9, 0.8)
                Xmax = max([r[&#39;TimeTag&#39;] if vs_time else j for j,r in enumerate(self)])
                for k, s in enumerate(asamples + usamples):
                        if vs_time:
                                X = [r[&#39;TimeTag&#39;] for r in self if r[&#39;Sample&#39;] == s]
                        else:
                                X = [x for x,r in enumerate(self) if r[&#39;Sample&#39;] == s]
                        Y = [k for x in X]
                        ppl.plot(X, Y, &#39;o&#39;, mec = None, mew = 0, mfc = &#39;b&#39; if s in usamples else &#39;r&#39;, ms = 3, alpha = .5)
                        ppl.axhline(k, color = &#39;b&#39; if s in usamples else &#39;r&#39;, lw = .5, alpha = .25)
                        ppl.text(Xmax, k, f&#39;  {s}&#39;, va = &#39;center&#39;, ha = &#39;left&#39;, size = 7)
                if vs_time:
                        t = [r[&#39;TimeTag&#39;] for r in self]
                        t1, t2 = min(t), max(t)
                        tspan = t2 - t1
                        t1 -= tspan / len(self)
                        t2 += tspan / len(self)
                        ppl.axis([t1, t2, -1, k+1])
                else:
                        ppl.axis([-1, len(self), -1, k+1])
                        

                x2 = 0
                for session in self.sessions:
                        x1 = min([r[&#39;TimeTag&#39;] if vs_time else j for j,r in enumerate(self) if r[&#39;Session&#39;] == session])
                        if vs_time:
                                ppl.axvline(x1, color = &#39;k&#39;, lw = .75)
                        if k:
                                if vs_time:
                                        ppl.axvspan(x1,x2,color = &#39;k&#39;, zorder = -100, alpha = .2)
                                else:
                                        ppl.axvline((x1+x2)/2, color = &#39;k&#39;, lw = .75)
                        x2 = max([r[&#39;TimeTag&#39;] if vs_time else j for j,r in enumerate(self) if r[&#39;Session&#39;] == session])
#                       from xlrd import xldate_as_datetime
#                       print(session, xldate_as_datetime(x1, 0), xldate_as_datetime(x2, 0))
                        if vs_time:
                                ppl.axvline(x2, color = &#39;k&#39;, lw = .75)
                        ppl.text((2*x1+x2)/3, k+1, session, ha = &#39;left&#39;, va = &#39;bottom&#39;, rotation = 45, size = 8)

                ppl.xticks([])
                ppl.yticks([])

                if output is None:
                        if not os.path.exists(dir):
                                os.makedirs(dir)
                        if filename == None:
                                filename = f&#39;D{self._4x}_distribution_of_analyses.pdf&#39;
                        ppl.savefig(f&#39;{dir}/{filename}&#39;)
                        ppl.close(fig)
                elif output == &#39;ax&#39;:
                        return ppl.gca()
                elif output == &#39;fig&#39;:
                        return fig


class D47data(D4xdata):
        &#39;&#39;&#39;
        Store and process data for a large set of Δ&lt;sub&gt;47&lt;/sub&gt; analyses,
        usually comprising more than one analytical session.
        &#39;&#39;&#39;

        Nominal_D4x = {
                &#39;ETH-1&#39;:   0.2052,
                &#39;ETH-2&#39;:   0.2085,
                &#39;ETH-3&#39;:   0.6132,
                &#39;ETH-4&#39;:   0.4511,
                &#39;IAEA-C1&#39;: 0.3018,
                &#39;IAEA-C2&#39;: 0.6409,
                &#39;MERCK&#39;:   0.5135,
                } # I-CDES (Bernasconi et al., 2021)
        &#39;&#39;&#39;
        Nominal Δ&lt;sub&gt;47&lt;/sub&gt; values assigned to the Δ&lt;sub&gt;47&lt;/sub&gt; anchor samples, used by
        `D47data.standardize_D47()` to normalize unknown samples to an absolute Δ&lt;sub&gt;47&lt;/sub&gt;
        reference frame.

        By default equal to (after [Bernasconi et al. (2021)]):
        ````py
        {
                &#39;ETH-1&#39;   : 0.2052,
                &#39;ETH-2&#39;   : 0.2085,
                &#39;ETH-3&#39;   : 0.6132,
                &#39;ETH-4&#39;   : 0.4511,
                &#39;IAEA-C1&#39; : 0.3018,
                &#39;IAEA-C2&#39; : 0.6409,
                &#39;MERCK&#39;   : 0.5135,
        }
        ````

        [Bernasconi et al. (2021)]: https://doi.org/10.1029/2020GC009588
        &#39;&#39;&#39;


        @property
        def Nominal_D47(self):
                return self.Nominal_D4x
        

        @Nominal_D47.setter
        def Nominal_D47(self, new):
                self.Nominal_D4x = dict(**new)
                self.refresh()


        def __init__(self, l = [], **kwargs):
                D4xdata.__init__(self, l = l, mass = &#39;47&#39;, **kwargs)


        def D47fromTeq(self, fCo2eqD47 = &#39;petersen&#39;, priority = &#39;new&#39;):
                &#39;&#39;&#39;
                Find all samples for which `Teq` is specified, compute equilibrium Δ&lt;sub&gt;47&lt;/sub&gt;
                value for that temperature, and add treat these samples as additional anchors.

                __Parameters__

                + `fCo2eqD47`: Which CO&lt;sub&gt;2&lt;/sub&gt; equilibrium law to use
                (`petersen`: [Petersen et al. (2019)];
                `wang`: [Wang et al. (2019)]).
                + `priority`: if `replace`: forget old anchors and only use the new ones;
                if `new`: keep pre-existing anchors but update them in case of conflict
                between old and new Δ&lt;sub&gt;47&lt;/sub&gt; values;
                if `old`: keep pre-existing anchors but preserve their original Δ&lt;sub&gt;47&lt;/sub&gt;
                values in case of conflict.

                [Petersen et al. (2019)]: https://doi.org/10.1029/2018GC008127
                [Wang et al. (2019)]: https://doi.org/10.1016/j.gca.2004.05.039
                &#39;&#39;&#39;
                f = {
                        &#39;petersen&#39;: fCO2eqD47_Petersen,
                        &#39;wang&#39;: fCO2eqD47_Wang,
                        }[fCo2eqD47]
                foo = {}
                for r in self:
                        if &#39;Teq&#39; in r:
                                if r[&#39;Sample&#39;] in foo:
                                        assert foo[r[&#39;Sample&#39;]] == f(r[&#39;Teq&#39;]), f&#39;Different values of `Teq` provided for sample `{r[&#34;Sample&#34;]}`.&#39;
                                else:
                                        foo[r[&#39;Sample&#39;]] = f(r[&#39;Teq&#39;])
                        else:
                                        assert r[&#39;Sample&#39;] not in foo, f&#39;`Teq` is inconsistently specified for sample `{r[&#34;Sample&#34;]}`.&#39;

                if priority == &#39;replace&#39;:
                        self.Nominal_D47 = {}
                for s in foo:
                        if priority != &#39;old&#39; or s not in self.Nominal_D47:
                                self.Nominal_D47[s] = foo[s]
        



class D48data(D4xdata):
        &#39;&#39;&#39;
        Store and process data for a large set of Δ&lt;sub&gt;48&lt;/sub&gt; analyses,
        usually comprising more than one analytical session.
        &#39;&#39;&#39;

        Nominal_D4x = {
                &#39;ETH-1&#39;:  0.138,
                &#39;ETH-2&#39;:  0.138,
                &#39;ETH-3&#39;:  0.270,
                &#39;ETH-4&#39;:  0.223,
                &#39;GU-1&#39;:  -0.419,
                } # (Fiebig et al., 2019, 2021)
        &#39;&#39;&#39;
        Nominal Δ&lt;sub&gt;48&lt;/sub&gt; values assigned to the Δ&lt;sub&gt;48&lt;/sub&gt; anchor samples, used by
        `D47data.standardize_D48()` to normalize unknown samples to an absolute Δ&lt;sub&gt;48&lt;/sub&gt;
        reference frame.

        By default equal to (after [Fiebig et al. (2019)], Fiebig et al. (in press)):
        ````py
        {
                &#39;ETH-1&#39; :  0.138,
                &#39;ETH-2&#39; :  0.138,
                &#39;ETH-3&#39; :  0.270,
                &#39;ETH-4&#39; :  0.223,
                &#39;GU-1&#39;  : -0.419,
        }
        ````

        [Fiebig et al. (2019)]: https://doi.org/10.1016/j.chemgeo.2019.05.019
        &#39;&#39;&#39;


        @property
        def Nominal_D48(self):
                return self.Nominal_D4x

        
        @Nominal_D48.setter
        def Nominal_D48(self, new):
                self.Nominal_D4x = dict(**new)
                self.refresh()


        def __init__(self, l = [], **kwargs):
                D4xdata.__init__(self, l = l, mass = &#39;48&#39;, **kwargs)


class SessionPlot():
        def __init__(self):
                pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="D47crunch.correlated_sum"><code class="name flex">
<span>def <span class="ident">correlated_sum</span></span>(<span>X, C, w=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute covariance-aware linear combinations</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>X</code>: list or 1-D array of values to sum</li>
<li><code>C</code>: covariance matrix for the elements of <code>X</code></li>
<li><code>w</code>: list or 1-D array of weights to apply to the elements of <code>X</code>
(all equal to 1 by default)</li>
</ul>
<p>Return the sum (and its SE) of the elements of <code>X</code>, with optional weights equal
to the elements of <code>w</code>, accounting for covariances between the elements of <code>X</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def correlated_sum(X, C, w = None):
        &#39;&#39;&#39;
        Compute covariance-aware linear combinations

        __Parameters__
        
        + `X`: list or 1-D array of values to sum
        + `C`: covariance matrix for the elements of `X`
        + `w`: list or 1-D array of weights to apply to the elements of `X`
               (all equal to 1 by default)

        Return the sum (and its SE) of the elements of `X`, with optional weights equal
        to the elements of `w`, accounting for covariances between the elements of `X`.
        &#39;&#39;&#39;
        if w is None:
                w = [1 for x in X]
        return np.dot(w,X), (np.dot(w,np.dot(C,w)))**.5</code></pre>
</details>
</dd>
<dt id="D47crunch.fCO2eqD47_Petersen"><code class="name flex">
<span>def <span class="ident">fCO2eqD47_Petersen</span></span>(<span>T)</span>
</code></dt>
<dd>
<div class="desc"><p>CO<sub>2</sub> equilibrium Δ<sub>47</sub> value as a function of T (in degrees C)
according to <a href="https://doi.org/10.1029/2018GC008127">Petersen et al. (2019)</a>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fCO2eqD47_Petersen(T):
        &#39;&#39;&#39;
        CO&lt;sub&gt;2&lt;/sub&gt; equilibrium Δ&lt;sub&gt;47&lt;/sub&gt; value as a function of T (in degrees C)
        according to [Petersen et al. (2019)].

        [Petersen et al. (2019)]: https://doi.org/10.1029/2018GC008127
        &#39;&#39;&#39;
        return float(_fCO2eqD47_Petersen(T))</code></pre>
</details>
</dd>
<dt id="D47crunch.fCO2eqD47_Wang"><code class="name flex">
<span>def <span class="ident">fCO2eqD47_Wang</span></span>(<span>T)</span>
</code></dt>
<dd>
<div class="desc"><p>CO<sub>2</sub> equilibrium Δ<sub>47</sub> value as a function of <code>T</code> (in degrees C)
according to <a href="https://doi.org/10.1016/j.gca.2004.05.039">Wang et al. (2004)</a> (supplementary data of <a href="https://doi.org/10.1016/j.gca.2011.09.025">Dennis et al., 2011</a>).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fCO2eqD47_Wang(T):
        &#39;&#39;&#39;
        CO&lt;sub&gt;2&lt;/sub&gt; equilibrium Δ&lt;sub&gt;47&lt;/sub&gt; value as a function of `T` (in degrees C)
        according to [Wang et al. (2004)] (supplementary data of [Dennis et al., 2011]).

        [Wang et al. (2004)]: https://doi.org/10.1016/j.gca.2004.05.039
        [Dennis et al., 2011]: https://doi.org/10.1016/j.gca.2011.09.025
        &#39;&#39;&#39;
        return float(_fCO2eqD47_Wang(T))</code></pre>
</details>
</dd>
<dt id="D47crunch.make_csv"><code class="name flex">
<span>def <span class="ident">make_csv</span></span>(<span>x, hsep=',', vsep='\n')</span>
</code></dt>
<dd>
<div class="desc"><p>Formats a list of lists of strings as a CSV</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>x</code>: the list of lists of strings to format</li>
<li><code>hsep</code>: the field separator (<code>,</code> by default)</li>
<li><code>vsep</code>: the line-ending convention to use (<code>\n</code> by default)</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="language-py">print(make_csv([['a', 'b', 'c'], ['d', 'e', 'f']]))
</code></pre>
<p>outputs:</p>
<pre><code class="language-py">a,b,c
d,e,f
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_csv(x, hsep = &#39;,&#39;, vsep = &#39;\n&#39;):
        &#39;&#39;&#39;
        Formats a list of lists of strings as a CSV

        __Parameters__

        + `x`: the list of lists of strings to format
        + `hsep`: the field separator (`,` by default)
        + `vsep`: the line-ending convention to use (`\\n` by default)

        __Example__

        ```py
        print(make_csv([[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;d&#39;, &#39;e&#39;, &#39;f&#39;]]))
        ```

        outputs:

        ```py
        a,b,c
        d,e,f
        ```
        &#39;&#39;&#39;
        return vsep.join([hsep.join(l) for l in x])</code></pre>
</details>
</dd>
<dt id="D47crunch.pf"><code class="name flex">
<span>def <span class="ident">pf</span></span>(<span>txt)</span>
</code></dt>
<dd>
<div class="desc"><p>Modify string <code>txt</code> to follow <code>lmfit.Parameter()</code> naming rules.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pf(txt):
        &#39;&#39;&#39;
        Modify string `txt` to follow `lmfit.Parameter()` naming rules.
        &#39;&#39;&#39;
        return txt.replace(&#39;-&#39;,&#39;_&#39;).replace(&#39;.&#39;,&#39;_&#39;).replace(&#39; &#39;,&#39;_&#39;)</code></pre>
</details>
</dd>
<dt id="D47crunch.pretty_table"><code class="name flex">
<span>def <span class="ident">pretty_table</span></span>(<span>x, header=1, hsep='
', vsep='–', align=&#x27;&lt;&#x27;)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads a list of lists of strings and outputs an ascii table</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>x</code>: a list of lists of strings</li>
<li><code>header</code>: the number of lines to treat as header lines</li>
<li><code>hsep</code>: the horizontal separator between columns</li>
<li><code>vsep</code>: the character to use as vertical separator</li>
<li><code>align</code>: string of left (<code>&lt;</code>) or right (<code>&gt;</code>) alignment characters.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="language-python">x = [['A','B', 'C'], ['1', '1.9999', 'foo'], ['10', 'x', 'bar']]
print(pretty_table(x))
</code></pre>
<p>output:</p>
<pre><code class="language-python">--  ------  ---
A        B    C
--  ------  ---
1   1.9999  foo
10       x  bar
--  ------  ---
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pretty_table(x, header = 1, hsep = &#39;  &#39;, vsep = &#39;–&#39;, align = &#39;&lt;&#39;):
        &#39;&#39;&#39;
        Reads a list of lists of strings and outputs an ascii table

        __Parameters__

        + `x`: a list of lists of strings
        + `header`: the number of lines to treat as header lines
        + `hsep`: the horizontal separator between columns
        + `vsep`: the character to use as vertical separator
        + `align`: string of left (`&lt;`) or right (`&gt;`) alignment characters.

        __Example__

        ```python
        x = [[&#39;A&#39;,&#39;B&#39;, &#39;C&#39;], [&#39;1&#39;, &#39;1.9999&#39;, &#39;foo&#39;], [&#39;10&#39;, &#39;x&#39;, &#39;bar&#39;]]
        print(pretty_table(x))
        ```

        output:

        ```python
        --  ------  ---
        A        B    C
        --  ------  ---
        1   1.9999  foo
        10       x  bar
        --  ------  ---
        ```
        &#39;&#39;&#39;
        txt = []
        widths = [np.max([len(e) for e in c]) for c in zip(*x)]

        if len(widths) &gt; len(align):
                align += &#39;&gt;&#39; * (len(widths)-len(align))
        sepline = hsep.join([vsep*w for w in widths])
        txt += [sepline]
        for k,l in enumerate(x):
                if k and k == header:
                        txt += [sepline]
                txt += [hsep.join([f&#39;{e:{a}{w}}&#39; for e, w, a in zip(l, widths, align)])]
        txt += [sepline]
        txt += [&#39;&#39;]
        return &#39;\n&#39;.join(txt)</code></pre>
</details>
</dd>
<dt id="D47crunch.read_csv"><code class="name flex">
<span>def <span class="ident">read_csv</span></span>(<span>filename, sep='')</span>
</code></dt>
<dd>
<div class="desc"><p>Read contents of <code>filename</code> in csv format and return a list of dictionaries.</p>
<p>In the csv string, spaces before and after field separators (<code>','</code> by default)
are optional.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>filename</code>: the csv file to read</li>
<li><code>sep</code>: csv separator delimiting the fields. By default, use <code>,</code>, <code>;</code>, or <code>
</code>,
whichever appers most often in the contents of <code>filename</code>.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_csv(filename, sep = &#39;&#39;):
        &#39;&#39;&#39;
        Read contents of `filename` in csv format and return a list of dictionaries.

        In the csv string, spaces before and after field separators (`&#39;,&#39;` by default)
        are optional.

        __Parameters__

        + `filename`: the csv file to read
        + `sep`: csv separator delimiting the fields. By default, use `,`, `;`, or `\t`,
        whichever appers most often in the contents of `filename`.
        &#39;&#39;&#39;
        with open(filename) as fid:
                txt = fid.read()

        if sep == &#39;&#39;:
                sep = sorted(&#39;,;\t&#39;, key = lambda x: - txt.count(x))[0]
        txt = [[x.strip() for x in l.split(sep)] for l in txt.splitlines() if l.strip()]
        return [{k: smart_type(v) for k,v in zip(txt[0], l) if v} for l in txt[1:]]</code></pre>
</details>
</dd>
<dt id="D47crunch.smart_type"><code class="name flex">
<span>def <span class="ident">smart_type</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>Tries to convert string <code>x</code> to a float if it includes a decimal point, or
to an integer if it does not. If both attempts fail, return the original
string unchanged.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def smart_type(x):
        &#39;&#39;&#39;
        Tries to convert string `x` to a float if it includes a decimal point, or
        to an integer if it does not. If both attempts fail, return the original
        string unchanged.
        &#39;&#39;&#39;
        try:
                y = float(x)
        except ValueError:
                return x
        if &#39;.&#39; not in x:
                return int(y)
        return y</code></pre>
</details>
</dd>
<dt id="D47crunch.transpose_table"><code class="name flex">
<span>def <span class="ident">transpose_table</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>Transpose a list if lists</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>x</code>: a list of lists</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="language-python">x = [[1, 2], [3, 4]]
print(transpose_table(x))
</code></pre>
<p>outputs:</p>
<pre><code class="language-python">[[1, 3], [2, 4]]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transpose_table(x):
        &#39;&#39;&#39;
        Transpose a list if lists

        __Parameters__

        + `x`: a list of lists

        __Example__

        ```python
        x = [[1, 2], [3, 4]]
        print(transpose_table(x))
        ```

        outputs:

        ```python
        [[1, 3], [2, 4]]
        ```

        &#39;&#39;&#39;
        return [[e for e in c] for c in zip(*x)]</code></pre>
</details>
</dd>
<dt id="D47crunch.w_avg"><code class="name flex">
<span>def <span class="ident">w_avg</span></span>(<span>X, sX)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute variance-weighted average</p>
<p>Returns the value and SE of the weighted average of the elements of <code>X</code>,
with relative weights equal to their inverse variances (<code>1/sX**2</code>).</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>X</code>: array-like of elements to average</li>
<li><code>sX</code>: array-like of the corresponding SE values</li>
</ul>
<p><strong>Tip</strong></p>
<p>If <code>X</code> and <code>sX</code> are initially arranged as a list of <code>(x, sx)</code> doublets,
they may be rearranged using <code>zip()</code>:</p>
<pre><code class="language-python">foo = [(0, 0.1), (1, 0.05), (2, 0.05)]
print(w_avg(*zip(*foo)))

# output:
# (1.3333333333333333, 0.03333333333333334)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def w_avg(X, sX) :
        &#39;&#39;&#39;
        Compute variance-weighted average

        Returns the value and SE of the weighted average of the elements of `X`,
        with relative weights equal to their inverse variances (`1/sX**2`).

        __Parameters__

        + `X`: array-like of elements to average
        + `sX`: array-like of the corresponding SE values

        __Tip__

        If `X` and `sX` are initially arranged as a list of `(x, sx)` doublets,
        they may be rearranged using `zip()`:

        ```python
        foo = [(0, 0.1), (1, 0.05), (2, 0.05)]
        print(w_avg(*zip(*foo)))

        # output:
        # (1.3333333333333333, 0.03333333333333334)
        ```
        &#39;&#39;&#39;
        X = [ x for x in X ]
        sX = [ sx for sx in sX ]
        W = [ sx**-2 for sx in sX ]
        W = [ w/sum(W) for w in W ]
        Xavg = sum([ w*x for w,x in zip(W,X) ])
        sXavg = sum([ w**2*sx**2 for w,sx in zip(W,sX) ])**.5
        return Xavg, sXavg</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="D47crunch.D47data"><code class="flex name class">
<span>class <span class="ident">D47data</span></span>
<span>(</span><span>l=[], **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Store and process data for a large set of Δ<sub>47</sub> analyses,
usually comprising more than one analytical session.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>l</code>: a list of dictionaries, with each dictionary including at least the keys
<code>Sample</code>, <code>d45</code>, <code>d46</code>, and <code>d47</code> or <code>d48</code>.</li>
<li><code>logfile</code>: if specified, write detailed logs to this file path when calling <code><a title="D47crunch.D4xdata" href="#D47crunch.D4xdata">D4xdata</a></code> methods.</li>
<li><code>session</code>: define session name for analyses without a <code>Session</code> key</li>
<li><code>verbose</code>: if <code>True</code>, print out detailed logs when calling <code><a title="D47crunch.D4xdata" href="#D47crunch.D4xdata">D4xdata</a></code> methods.</li>
</ul>
<p>Returns a <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> object derived from <code>list</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class D47data(D4xdata):
        &#39;&#39;&#39;
        Store and process data for a large set of Δ&lt;sub&gt;47&lt;/sub&gt; analyses,
        usually comprising more than one analytical session.
        &#39;&#39;&#39;

        Nominal_D4x = {
                &#39;ETH-1&#39;:   0.2052,
                &#39;ETH-2&#39;:   0.2085,
                &#39;ETH-3&#39;:   0.6132,
                &#39;ETH-4&#39;:   0.4511,
                &#39;IAEA-C1&#39;: 0.3018,
                &#39;IAEA-C2&#39;: 0.6409,
                &#39;MERCK&#39;:   0.5135,
                } # I-CDES (Bernasconi et al., 2021)
        &#39;&#39;&#39;
        Nominal Δ&lt;sub&gt;47&lt;/sub&gt; values assigned to the Δ&lt;sub&gt;47&lt;/sub&gt; anchor samples, used by
        `D47data.standardize_D47()` to normalize unknown samples to an absolute Δ&lt;sub&gt;47&lt;/sub&gt;
        reference frame.

        By default equal to (after [Bernasconi et al. (2021)]):
        ````py
        {
                &#39;ETH-1&#39;   : 0.2052,
                &#39;ETH-2&#39;   : 0.2085,
                &#39;ETH-3&#39;   : 0.6132,
                &#39;ETH-4&#39;   : 0.4511,
                &#39;IAEA-C1&#39; : 0.3018,
                &#39;IAEA-C2&#39; : 0.6409,
                &#39;MERCK&#39;   : 0.5135,
        }
        ````

        [Bernasconi et al. (2021)]: https://doi.org/10.1029/2020GC009588
        &#39;&#39;&#39;


        @property
        def Nominal_D47(self):
                return self.Nominal_D4x
        

        @Nominal_D47.setter
        def Nominal_D47(self, new):
                self.Nominal_D4x = dict(**new)
                self.refresh()


        def __init__(self, l = [], **kwargs):
                D4xdata.__init__(self, l = l, mass = &#39;47&#39;, **kwargs)


        def D47fromTeq(self, fCo2eqD47 = &#39;petersen&#39;, priority = &#39;new&#39;):
                &#39;&#39;&#39;
                Find all samples for which `Teq` is specified, compute equilibrium Δ&lt;sub&gt;47&lt;/sub&gt;
                value for that temperature, and add treat these samples as additional anchors.

                __Parameters__

                + `fCo2eqD47`: Which CO&lt;sub&gt;2&lt;/sub&gt; equilibrium law to use
                (`petersen`: [Petersen et al. (2019)];
                `wang`: [Wang et al. (2019)]).
                + `priority`: if `replace`: forget old anchors and only use the new ones;
                if `new`: keep pre-existing anchors but update them in case of conflict
                between old and new Δ&lt;sub&gt;47&lt;/sub&gt; values;
                if `old`: keep pre-existing anchors but preserve their original Δ&lt;sub&gt;47&lt;/sub&gt;
                values in case of conflict.

                [Petersen et al. (2019)]: https://doi.org/10.1029/2018GC008127
                [Wang et al. (2019)]: https://doi.org/10.1016/j.gca.2004.05.039
                &#39;&#39;&#39;
                f = {
                        &#39;petersen&#39;: fCO2eqD47_Petersen,
                        &#39;wang&#39;: fCO2eqD47_Wang,
                        }[fCo2eqD47]
                foo = {}
                for r in self:
                        if &#39;Teq&#39; in r:
                                if r[&#39;Sample&#39;] in foo:
                                        assert foo[r[&#39;Sample&#39;]] == f(r[&#39;Teq&#39;]), f&#39;Different values of `Teq` provided for sample `{r[&#34;Sample&#34;]}`.&#39;
                                else:
                                        foo[r[&#39;Sample&#39;]] = f(r[&#39;Teq&#39;])
                        else:
                                        assert r[&#39;Sample&#39;] not in foo, f&#39;`Teq` is inconsistently specified for sample `{r[&#34;Sample&#34;]}`.&#39;

                if priority == &#39;replace&#39;:
                        self.Nominal_D47 = {}
                for s in foo:
                        if priority != &#39;old&#39; or s not in self.Nominal_D47:
                                self.Nominal_D47[s] = foo[s]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="D47crunch.D4xdata" href="#D47crunch.D4xdata">D4xdata</a></li>
<li>builtins.list</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="D47crunch.D47data.Nominal_D4x"><code class="name">var <span class="ident">Nominal_D4x</span></code></dt>
<dd>
<div class="desc"><p>Nominal Δ<sub>47</sub> values assigned to the Δ<sub>47</sub> anchor samples, used by
<code>D47data.standardize_D47()</code> to normalize unknown samples to an absolute Δ<sub>47</sub>
reference frame.</p>
<p>By default equal to (after <a href="https://doi.org/10.1029/2020GC009588">Bernasconi et al. (2021)</a>):</p>
<pre><code class="language-py">{
        'ETH-1'   : 0.2052,
        'ETH-2'   : 0.2085,
        'ETH-3'   : 0.6132,
        'ETH-4'   : 0.4511,
        'IAEA-C1' : 0.3018,
        'IAEA-C2' : 0.6409,
        'MERCK'   : 0.5135,
}
</code></pre></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="D47crunch.D47data.Nominal_D47"><code class="name">var <span class="ident">Nominal_D47</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def Nominal_D47(self):
        return self.Nominal_D4x</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="D47crunch.D47data.D47fromTeq"><code class="name flex">
<span>def <span class="ident">D47fromTeq</span></span>(<span>self, fCo2eqD47='petersen', priority='new')</span>
</code></dt>
<dd>
<div class="desc"><p>Find all samples for which <code>Teq</code> is specified, compute equilibrium Δ<sub>47</sub>
value for that temperature, and add treat these samples as additional anchors.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>fCo2eqD47</code>: Which CO<sub>2</sub> equilibrium law to use
(<code>petersen</code>: <a href="https://doi.org/10.1029/2018GC008127">Petersen et al. (2019)</a>;
<code>wang</code>: <a href="https://doi.org/10.1016/j.gca.2004.05.039">Wang et al. (2019)</a>).</li>
<li><code>priority</code>: if <code>replace</code>: forget old anchors and only use the new ones;
if <code>new</code>: keep pre-existing anchors but update them in case of conflict
between old and new Δ<sub>47</sub> values;
if <code>old</code>: keep pre-existing anchors but preserve their original Δ<sub>47</sub>
values in case of conflict.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def D47fromTeq(self, fCo2eqD47 = &#39;petersen&#39;, priority = &#39;new&#39;):
        &#39;&#39;&#39;
        Find all samples for which `Teq` is specified, compute equilibrium Δ&lt;sub&gt;47&lt;/sub&gt;
        value for that temperature, and add treat these samples as additional anchors.

        __Parameters__

        + `fCo2eqD47`: Which CO&lt;sub&gt;2&lt;/sub&gt; equilibrium law to use
        (`petersen`: [Petersen et al. (2019)];
        `wang`: [Wang et al. (2019)]).
        + `priority`: if `replace`: forget old anchors and only use the new ones;
        if `new`: keep pre-existing anchors but update them in case of conflict
        between old and new Δ&lt;sub&gt;47&lt;/sub&gt; values;
        if `old`: keep pre-existing anchors but preserve their original Δ&lt;sub&gt;47&lt;/sub&gt;
        values in case of conflict.

        [Petersen et al. (2019)]: https://doi.org/10.1029/2018GC008127
        [Wang et al. (2019)]: https://doi.org/10.1016/j.gca.2004.05.039
        &#39;&#39;&#39;
        f = {
                &#39;petersen&#39;: fCO2eqD47_Petersen,
                &#39;wang&#39;: fCO2eqD47_Wang,
                }[fCo2eqD47]
        foo = {}
        for r in self:
                if &#39;Teq&#39; in r:
                        if r[&#39;Sample&#39;] in foo:
                                assert foo[r[&#39;Sample&#39;]] == f(r[&#39;Teq&#39;]), f&#39;Different values of `Teq` provided for sample `{r[&#34;Sample&#34;]}`.&#39;
                        else:
                                foo[r[&#39;Sample&#39;]] = f(r[&#39;Teq&#39;])
                else:
                                assert r[&#39;Sample&#39;] not in foo, f&#39;`Teq` is inconsistently specified for sample `{r[&#34;Sample&#34;]}`.&#39;

        if priority == &#39;replace&#39;:
                self.Nominal_D47 = {}
        for s in foo:
                if priority != &#39;old&#39; or s not in self.Nominal_D47:
                        self.Nominal_D47[s] = foo[s]</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="D47crunch.D4xdata" href="#D47crunch.D4xdata">D4xdata</a></b></code>:
<ul class="hlist">
<li><code><a title="D47crunch.D4xdata.ALPHA_18O_ACID_REACTION" href="#D47crunch.D4xdata.ALPHA_18O_ACID_REACTION">ALPHA_18O_ACID_REACTION</a></code></li>
<li><code><a title="D47crunch.D4xdata.LEVENE_REF_SAMPLE" href="#D47crunch.D4xdata.LEVENE_REF_SAMPLE">LEVENE_REF_SAMPLE</a></code></li>
<li><code><a title="D47crunch.D4xdata.Nominal_d13C_VPDB" href="#D47crunch.D4xdata.Nominal_d13C_VPDB">Nominal_d13C_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.Nominal_d18O_VPDB" href="#D47crunch.D4xdata.Nominal_d18O_VPDB">Nominal_d18O_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.R13_VPDB" href="#D47crunch.D4xdata.R13_VPDB">R13_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.R17_VPDB" href="#D47crunch.D4xdata.R17_VPDB">R17_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.R17_VSMOW" href="#D47crunch.D4xdata.R17_VSMOW">R17_VSMOW</a></code></li>
<li><code><a title="D47crunch.D4xdata.R18_VPDB" href="#D47crunch.D4xdata.R18_VPDB">R18_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.R18_VSMOW" href="#D47crunch.D4xdata.R18_VSMOW">R18_VSMOW</a></code></li>
<li><code><a title="D47crunch.D4xdata.assign_timestamps" href="#D47crunch.D4xdata.assign_timestamps">assign_timestamps</a></code></li>
<li><code><a title="D47crunch.D4xdata.combine_samples" href="#D47crunch.D4xdata.combine_samples">combine_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.compute_bulk_and_clumping_deltas" href="#D47crunch.D4xdata.compute_bulk_and_clumping_deltas">compute_bulk_and_clumping_deltas</a></code></li>
<li><code><a title="D47crunch.D4xdata.compute_bulk_delta" href="#D47crunch.D4xdata.compute_bulk_delta">compute_bulk_delta</a></code></li>
<li><code><a title="D47crunch.D4xdata.compute_isobar_ratios" href="#D47crunch.D4xdata.compute_isobar_ratios">compute_isobar_ratios</a></code></li>
<li><code><a title="D47crunch.D4xdata.compute_r" href="#D47crunch.D4xdata.compute_r">compute_r</a></code></li>
<li><code><a title="D47crunch.D4xdata.consolidate" href="#D47crunch.D4xdata.consolidate">consolidate</a></code></li>
<li><code><a title="D47crunch.D4xdata.consolidate_samples" href="#D47crunch.D4xdata.consolidate_samples">consolidate_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.consolidate_sessions" href="#D47crunch.D4xdata.consolidate_sessions">consolidate_sessions</a></code></li>
<li><code><a title="D47crunch.D4xdata.crunch" href="#D47crunch.D4xdata.crunch">crunch</a></code></li>
<li><code><a title="D47crunch.D4xdata.d13C_STANDARDIZATION_METHOD" href="#D47crunch.D4xdata.d13C_STANDARDIZATION_METHOD">d13C_STANDARDIZATION_METHOD</a></code></li>
<li><code><a title="D47crunch.D4xdata.d18O_STANDARDIZATION_METHOD" href="#D47crunch.D4xdata.d18O_STANDARDIZATION_METHOD">d18O_STANDARDIZATION_METHOD</a></code></li>
<li><code><a title="D47crunch.D4xdata.fill_in_missing_info" href="#D47crunch.D4xdata.fill_in_missing_info">fill_in_missing_info</a></code></li>
<li><code><a title="D47crunch.D4xdata.input" href="#D47crunch.D4xdata.input">input</a></code></li>
<li><code><a title="D47crunch.D4xdata.lambda_17" href="#D47crunch.D4xdata.lambda_17">lambda_17</a></code></li>
<li><code><a title="D47crunch.D4xdata.log" href="#D47crunch.D4xdata.log">log</a></code></li>
<li><code><a title="D47crunch.D4xdata.make_verbal" href="#D47crunch.D4xdata.make_verbal">make_verbal</a></code></li>
<li><code><a title="D47crunch.D4xdata.msg" href="#D47crunch.D4xdata.msg">msg</a></code></li>
<li><code><a title="D47crunch.D4xdata.plot_distribution_of_analyses" href="#D47crunch.D4xdata.plot_distribution_of_analyses">plot_distribution_of_analyses</a></code></li>
<li><code><a title="D47crunch.D4xdata.plot_residuals" href="#D47crunch.D4xdata.plot_residuals">plot_residuals</a></code></li>
<li><code><a title="D47crunch.D4xdata.plot_sessions" href="#D47crunch.D4xdata.plot_sessions">plot_sessions</a></code></li>
<li><code><a title="D47crunch.D4xdata.plot_single_session" href="#D47crunch.D4xdata.plot_single_session">plot_single_session</a></code></li>
<li><code><a title="D47crunch.D4xdata.read" href="#D47crunch.D4xdata.read">read</a></code></li>
<li><code><a title="D47crunch.D4xdata.refresh" href="#D47crunch.D4xdata.refresh">refresh</a></code></li>
<li><code><a title="D47crunch.D4xdata.refresh_samples" href="#D47crunch.D4xdata.refresh_samples">refresh_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.refresh_sessions" href="#D47crunch.D4xdata.refresh_sessions">refresh_sessions</a></code></li>
<li><code><a title="D47crunch.D4xdata.repeatabilities" href="#D47crunch.D4xdata.repeatabilities">repeatabilities</a></code></li>
<li><code><a title="D47crunch.D4xdata.report" href="#D47crunch.D4xdata.report">report</a></code></li>
<li><code><a title="D47crunch.D4xdata.rmswd" href="#D47crunch.D4xdata.rmswd">rmswd</a></code></li>
<li><code><a title="D47crunch.D4xdata.sample_D4x_correl" href="#D47crunch.D4xdata.sample_D4x_correl">sample_D4x_correl</a></code></li>
<li><code><a title="D47crunch.D4xdata.sample_D4x_covar" href="#D47crunch.D4xdata.sample_D4x_covar">sample_D4x_covar</a></code></li>
<li><code><a title="D47crunch.D4xdata.sample_average" href="#D47crunch.D4xdata.sample_average">sample_average</a></code></li>
<li><code><a title="D47crunch.D4xdata.simulate" href="#D47crunch.D4xdata.simulate">simulate</a></code></li>
<li><code><a title="D47crunch.D4xdata.split_samples" href="#D47crunch.D4xdata.split_samples">split_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.standardization_error" href="#D47crunch.D4xdata.standardization_error">standardization_error</a></code></li>
<li><code><a title="D47crunch.D4xdata.standardize" href="#D47crunch.D4xdata.standardize">standardize</a></code></li>
<li><code><a title="D47crunch.D4xdata.standardize_d13C" href="#D47crunch.D4xdata.standardize_d13C">standardize_d13C</a></code></li>
<li><code><a title="D47crunch.D4xdata.standardize_d18O" href="#D47crunch.D4xdata.standardize_d18O">standardize_d18O</a></code></li>
<li><code><a title="D47crunch.D4xdata.summary" href="#D47crunch.D4xdata.summary">summary</a></code></li>
<li><code><a title="D47crunch.D4xdata.table_of_analyses" href="#D47crunch.D4xdata.table_of_analyses">table_of_analyses</a></code></li>
<li><code><a title="D47crunch.D4xdata.table_of_samples" href="#D47crunch.D4xdata.table_of_samples">table_of_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.table_of_sessions" href="#D47crunch.D4xdata.table_of_sessions">table_of_sessions</a></code></li>
<li><code><a title="D47crunch.D4xdata.unsplit_samples" href="#D47crunch.D4xdata.unsplit_samples">unsplit_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.vmsg" href="#D47crunch.D4xdata.vmsg">vmsg</a></code></li>
<li><code><a title="D47crunch.D4xdata.wg" href="#D47crunch.D4xdata.wg">wg</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="D47crunch.D48data"><code class="flex name class">
<span>class <span class="ident">D48data</span></span>
<span>(</span><span>l=[], **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Store and process data for a large set of Δ<sub>48</sub> analyses,
usually comprising more than one analytical session.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>l</code>: a list of dictionaries, with each dictionary including at least the keys
<code>Sample</code>, <code>d45</code>, <code>d46</code>, and <code>d47</code> or <code>d48</code>.</li>
<li><code>logfile</code>: if specified, write detailed logs to this file path when calling <code><a title="D47crunch.D4xdata" href="#D47crunch.D4xdata">D4xdata</a></code> methods.</li>
<li><code>session</code>: define session name for analyses without a <code>Session</code> key</li>
<li><code>verbose</code>: if <code>True</code>, print out detailed logs when calling <code><a title="D47crunch.D4xdata" href="#D47crunch.D4xdata">D4xdata</a></code> methods.</li>
</ul>
<p>Returns a <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> object derived from <code>list</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class D48data(D4xdata):
        &#39;&#39;&#39;
        Store and process data for a large set of Δ&lt;sub&gt;48&lt;/sub&gt; analyses,
        usually comprising more than one analytical session.
        &#39;&#39;&#39;

        Nominal_D4x = {
                &#39;ETH-1&#39;:  0.138,
                &#39;ETH-2&#39;:  0.138,
                &#39;ETH-3&#39;:  0.270,
                &#39;ETH-4&#39;:  0.223,
                &#39;GU-1&#39;:  -0.419,
                } # (Fiebig et al., 2019, 2021)
        &#39;&#39;&#39;
        Nominal Δ&lt;sub&gt;48&lt;/sub&gt; values assigned to the Δ&lt;sub&gt;48&lt;/sub&gt; anchor samples, used by
        `D47data.standardize_D48()` to normalize unknown samples to an absolute Δ&lt;sub&gt;48&lt;/sub&gt;
        reference frame.

        By default equal to (after [Fiebig et al. (2019)], Fiebig et al. (in press)):
        ````py
        {
                &#39;ETH-1&#39; :  0.138,
                &#39;ETH-2&#39; :  0.138,
                &#39;ETH-3&#39; :  0.270,
                &#39;ETH-4&#39; :  0.223,
                &#39;GU-1&#39;  : -0.419,
        }
        ````

        [Fiebig et al. (2019)]: https://doi.org/10.1016/j.chemgeo.2019.05.019
        &#39;&#39;&#39;


        @property
        def Nominal_D48(self):
                return self.Nominal_D4x

        
        @Nominal_D48.setter
        def Nominal_D48(self, new):
                self.Nominal_D4x = dict(**new)
                self.refresh()


        def __init__(self, l = [], **kwargs):
                D4xdata.__init__(self, l = l, mass = &#39;48&#39;, **kwargs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="D47crunch.D4xdata" href="#D47crunch.D4xdata">D4xdata</a></li>
<li>builtins.list</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="D47crunch.D48data.Nominal_D4x"><code class="name">var <span class="ident">Nominal_D4x</span></code></dt>
<dd>
<div class="desc"><p>Nominal Δ<sub>48</sub> values assigned to the Δ<sub>48</sub> anchor samples, used by
<code>D47data.standardize_D48()</code> to normalize unknown samples to an absolute Δ<sub>48</sub>
reference frame.</p>
<p>By default equal to (after <a href="https://doi.org/10.1016/j.chemgeo.2019.05.019">Fiebig et al. (2019)</a>, Fiebig et al. (in press)):</p>
<pre><code class="language-py">{
        'ETH-1' :  0.138,
        'ETH-2' :  0.138,
        'ETH-3' :  0.270,
        'ETH-4' :  0.223,
        'GU-1'  : -0.419,
}
</code></pre></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="D47crunch.D48data.Nominal_D48"><code class="name">var <span class="ident">Nominal_D48</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def Nominal_D48(self):
        return self.Nominal_D4x</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="D47crunch.D4xdata" href="#D47crunch.D4xdata">D4xdata</a></b></code>:
<ul class="hlist">
<li><code><a title="D47crunch.D4xdata.ALPHA_18O_ACID_REACTION" href="#D47crunch.D4xdata.ALPHA_18O_ACID_REACTION">ALPHA_18O_ACID_REACTION</a></code></li>
<li><code><a title="D47crunch.D4xdata.LEVENE_REF_SAMPLE" href="#D47crunch.D4xdata.LEVENE_REF_SAMPLE">LEVENE_REF_SAMPLE</a></code></li>
<li><code><a title="D47crunch.D4xdata.Nominal_d13C_VPDB" href="#D47crunch.D4xdata.Nominal_d13C_VPDB">Nominal_d13C_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.Nominal_d18O_VPDB" href="#D47crunch.D4xdata.Nominal_d18O_VPDB">Nominal_d18O_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.R13_VPDB" href="#D47crunch.D4xdata.R13_VPDB">R13_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.R17_VPDB" href="#D47crunch.D4xdata.R17_VPDB">R17_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.R17_VSMOW" href="#D47crunch.D4xdata.R17_VSMOW">R17_VSMOW</a></code></li>
<li><code><a title="D47crunch.D4xdata.R18_VPDB" href="#D47crunch.D4xdata.R18_VPDB">R18_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.R18_VSMOW" href="#D47crunch.D4xdata.R18_VSMOW">R18_VSMOW</a></code></li>
<li><code><a title="D47crunch.D4xdata.assign_timestamps" href="#D47crunch.D4xdata.assign_timestamps">assign_timestamps</a></code></li>
<li><code><a title="D47crunch.D4xdata.combine_samples" href="#D47crunch.D4xdata.combine_samples">combine_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.compute_bulk_and_clumping_deltas" href="#D47crunch.D4xdata.compute_bulk_and_clumping_deltas">compute_bulk_and_clumping_deltas</a></code></li>
<li><code><a title="D47crunch.D4xdata.compute_bulk_delta" href="#D47crunch.D4xdata.compute_bulk_delta">compute_bulk_delta</a></code></li>
<li><code><a title="D47crunch.D4xdata.compute_isobar_ratios" href="#D47crunch.D4xdata.compute_isobar_ratios">compute_isobar_ratios</a></code></li>
<li><code><a title="D47crunch.D4xdata.compute_r" href="#D47crunch.D4xdata.compute_r">compute_r</a></code></li>
<li><code><a title="D47crunch.D4xdata.consolidate" href="#D47crunch.D4xdata.consolidate">consolidate</a></code></li>
<li><code><a title="D47crunch.D4xdata.consolidate_samples" href="#D47crunch.D4xdata.consolidate_samples">consolidate_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.consolidate_sessions" href="#D47crunch.D4xdata.consolidate_sessions">consolidate_sessions</a></code></li>
<li><code><a title="D47crunch.D4xdata.crunch" href="#D47crunch.D4xdata.crunch">crunch</a></code></li>
<li><code><a title="D47crunch.D4xdata.d13C_STANDARDIZATION_METHOD" href="#D47crunch.D4xdata.d13C_STANDARDIZATION_METHOD">d13C_STANDARDIZATION_METHOD</a></code></li>
<li><code><a title="D47crunch.D4xdata.d18O_STANDARDIZATION_METHOD" href="#D47crunch.D4xdata.d18O_STANDARDIZATION_METHOD">d18O_STANDARDIZATION_METHOD</a></code></li>
<li><code><a title="D47crunch.D4xdata.fill_in_missing_info" href="#D47crunch.D4xdata.fill_in_missing_info">fill_in_missing_info</a></code></li>
<li><code><a title="D47crunch.D4xdata.input" href="#D47crunch.D4xdata.input">input</a></code></li>
<li><code><a title="D47crunch.D4xdata.lambda_17" href="#D47crunch.D4xdata.lambda_17">lambda_17</a></code></li>
<li><code><a title="D47crunch.D4xdata.log" href="#D47crunch.D4xdata.log">log</a></code></li>
<li><code><a title="D47crunch.D4xdata.make_verbal" href="#D47crunch.D4xdata.make_verbal">make_verbal</a></code></li>
<li><code><a title="D47crunch.D4xdata.msg" href="#D47crunch.D4xdata.msg">msg</a></code></li>
<li><code><a title="D47crunch.D4xdata.plot_distribution_of_analyses" href="#D47crunch.D4xdata.plot_distribution_of_analyses">plot_distribution_of_analyses</a></code></li>
<li><code><a title="D47crunch.D4xdata.plot_residuals" href="#D47crunch.D4xdata.plot_residuals">plot_residuals</a></code></li>
<li><code><a title="D47crunch.D4xdata.plot_sessions" href="#D47crunch.D4xdata.plot_sessions">plot_sessions</a></code></li>
<li><code><a title="D47crunch.D4xdata.plot_single_session" href="#D47crunch.D4xdata.plot_single_session">plot_single_session</a></code></li>
<li><code><a title="D47crunch.D4xdata.read" href="#D47crunch.D4xdata.read">read</a></code></li>
<li><code><a title="D47crunch.D4xdata.refresh" href="#D47crunch.D4xdata.refresh">refresh</a></code></li>
<li><code><a title="D47crunch.D4xdata.refresh_samples" href="#D47crunch.D4xdata.refresh_samples">refresh_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.refresh_sessions" href="#D47crunch.D4xdata.refresh_sessions">refresh_sessions</a></code></li>
<li><code><a title="D47crunch.D4xdata.repeatabilities" href="#D47crunch.D4xdata.repeatabilities">repeatabilities</a></code></li>
<li><code><a title="D47crunch.D4xdata.report" href="#D47crunch.D4xdata.report">report</a></code></li>
<li><code><a title="D47crunch.D4xdata.rmswd" href="#D47crunch.D4xdata.rmswd">rmswd</a></code></li>
<li><code><a title="D47crunch.D4xdata.sample_D4x_correl" href="#D47crunch.D4xdata.sample_D4x_correl">sample_D4x_correl</a></code></li>
<li><code><a title="D47crunch.D4xdata.sample_D4x_covar" href="#D47crunch.D4xdata.sample_D4x_covar">sample_D4x_covar</a></code></li>
<li><code><a title="D47crunch.D4xdata.sample_average" href="#D47crunch.D4xdata.sample_average">sample_average</a></code></li>
<li><code><a title="D47crunch.D4xdata.simulate" href="#D47crunch.D4xdata.simulate">simulate</a></code></li>
<li><code><a title="D47crunch.D4xdata.split_samples" href="#D47crunch.D4xdata.split_samples">split_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.standardization_error" href="#D47crunch.D4xdata.standardization_error">standardization_error</a></code></li>
<li><code><a title="D47crunch.D4xdata.standardize" href="#D47crunch.D4xdata.standardize">standardize</a></code></li>
<li><code><a title="D47crunch.D4xdata.standardize_d13C" href="#D47crunch.D4xdata.standardize_d13C">standardize_d13C</a></code></li>
<li><code><a title="D47crunch.D4xdata.standardize_d18O" href="#D47crunch.D4xdata.standardize_d18O">standardize_d18O</a></code></li>
<li><code><a title="D47crunch.D4xdata.summary" href="#D47crunch.D4xdata.summary">summary</a></code></li>
<li><code><a title="D47crunch.D4xdata.table_of_analyses" href="#D47crunch.D4xdata.table_of_analyses">table_of_analyses</a></code></li>
<li><code><a title="D47crunch.D4xdata.table_of_samples" href="#D47crunch.D4xdata.table_of_samples">table_of_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.table_of_sessions" href="#D47crunch.D4xdata.table_of_sessions">table_of_sessions</a></code></li>
<li><code><a title="D47crunch.D4xdata.unsplit_samples" href="#D47crunch.D4xdata.unsplit_samples">unsplit_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.vmsg" href="#D47crunch.D4xdata.vmsg">vmsg</a></code></li>
<li><code><a title="D47crunch.D4xdata.wg" href="#D47crunch.D4xdata.wg">wg</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="D47crunch.D4xdata"><code class="flex name class">
<span>class <span class="ident">D4xdata</span></span>
<span>(</span><span>l=[], mass='47', logfile='', session='mySession', verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Store and process data for a large set of Δ<sub>47</sub> and/or Δ<sub>48</sub>
analyses, usually comprising more than one analytical session.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>l</code>: a list of dictionaries, with each dictionary including at least the keys
<code>Sample</code>, <code>d45</code>, <code>d46</code>, and <code>d47</code> or <code>d48</code>.</li>
<li><code>logfile</code>: if specified, write detailed logs to this file path when calling <code><a title="D47crunch.D4xdata" href="#D47crunch.D4xdata">D4xdata</a></code> methods.</li>
<li><code>session</code>: define session name for analyses without a <code>Session</code> key</li>
<li><code>verbose</code>: if <code>True</code>, print out detailed logs when calling <code><a title="D47crunch.D4xdata" href="#D47crunch.D4xdata">D4xdata</a></code> methods.</li>
</ul>
<p>Returns a <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> object derived from <code>list</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class D4xdata(list):
        &#39;&#39;&#39;
        Store and process data for a large set of Δ&lt;sub&gt;47&lt;/sub&gt; and/or Δ&lt;sub&gt;48&lt;/sub&gt;
        analyses, usually comprising more than one analytical session.
        &#39;&#39;&#39;

        ### 17O CORRECTION PARAMETERS
        R13_VPDB = 0.01118  # (Chang &amp; Li, 1990)
        &#39;&#39;&#39;
        Absolute (&lt;sup&gt;13&lt;/sup&gt;C/&lt;sup&gt;12&lt;/sup&gt;C) ratio of VPDB.
        By default equal to 0.01118 ([Chang &amp; Li, 1990])

        [Chang &amp; Li, 1990]: http://www.cnki.com.cn/Article/CJFDTotal-JXTW199004006.htm
        &#39;&#39;&#39;

        R18_VSMOW = 0.0020052  # (Baertschi, 1976)
        &#39;&#39;&#39;
        Absolute (&lt;sup&gt;18&lt;/sup&gt;O/&lt;sup&gt;16&lt;/sup&gt;C) ratio of VSMOW.
        By default equal to 0.0020052 ([Baertschi, 1976])

        [Baertschi, 1976]: https://doi.org/10.1016/0012-821X(76)90115-1
        &#39;&#39;&#39;

        lambda_17 = 0.528  # (Barkan &amp; Luz, 2005)
        &#39;&#39;&#39;
        Mass-dependent exponent for triple oxygen isotopes.
        By default equal to 0.528 ([Barkan &amp; Luz, 2005])

        [Barkan &amp; Luz, 2005]: https://doi.org/10.1002/rcm.2250
        &#39;&#39;&#39;

        R17_VSMOW = 0.00038475  # (Assonov &amp; Brenninkmeijer, 2003, rescaled to R13_VPDB)
        &#39;&#39;&#39;
        Absolute (&lt;sup&gt;17&lt;/sup&gt;O/&lt;sup&gt;16&lt;/sup&gt;C) ratio of VSMOW.
        By default equal to 0.00038475
        ([Assonov &amp; Brenninkmeijer, 2003], rescaled to `R13_VPDB`)

        [Assonov &amp; Brenninkmeijer, 2003]: https://dx.doi.org/10.1002/rcm.1011
        &#39;&#39;&#39;

        R18_VPDB = R18_VSMOW * 1.03092
        &#39;&#39;&#39;
        Absolute (&lt;sup&gt;18&lt;/sup&gt;O/&lt;sup&gt;16&lt;/sup&gt;C) ratio of VPDB.
        By definition equal to `R18_VSMOW * 1.03092`.
        &#39;&#39;&#39;

        R17_VPDB = R17_VSMOW * 1.03092 ** lambda_17
        &#39;&#39;&#39;
        Absolute (&lt;sup&gt;17&lt;/sup&gt;O/&lt;sup&gt;16&lt;/sup&gt;C) ratio of VPDB.
        By definition equal to `R17_VSMOW * 1.03092 ** lambda_17`.
        &#39;&#39;&#39;

        LEVENE_REF_SAMPLE = &#39;ETH-3&#39;
        &#39;&#39;&#39;
        After the Δ&lt;sub&gt;4x&lt;/sub&gt; standardization step, each sample is tested to
        assess whether the Δ&lt;sub&gt;4x&lt;/sub&gt; variance within all analyses for that
        sample differs significantly from that observed for a given reference
        sample (using [Levene&#39;s test], which yields a p-value corresponding to
        the null hypothesis that the underlying variances are equal).

        `LEVENE_REF_SAMPLE` (by default equal to `&#39;ETH-3&#39;`) specifies which
        sample should be used as a reference for this test.

        [Levene&#39;s test]: https://en.wikipedia.org/wiki/Levene%27s_test
        &#39;&#39;&#39;

        ALPHA_18O_ACID_REACTION = round(np.exp(3.59 / (90 + 273.15) - 1.79e-3), 6)  # (Kim et al., 2007, calcite)
        &#39;&#39;&#39;
        Specifies the &lt;sup&gt;18&lt;/sup&gt;O/&lt;sup&gt;16&lt;/sup&gt;O fractionation factor generally applicable
        to acid reactions in the dataset. Currently used by `D4xdata.wg()`,
        `D4xdata.standardize_d13C`, and `D4xdata.standardize_d18O`.

        By default equal to 1.008129 (calcite reacted at 90 °C, [Kim et al., 2007]).

        [Kim et al., 2007]: https://dx.doi.org/10.1016/j.chemgeo.2007.08.005
        &#39;&#39;&#39;

        Nominal_d13C_VPDB = {
                &#39;ETH-1&#39;: 2.02,
                &#39;ETH-2&#39;: -10.17,
                &#39;ETH-3&#39;: 1.71,
                }       # (Bernasconi et al., 2018)
        &#39;&#39;&#39;
        Nominal δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; values assigned to carbonate standards, used by
        `D4xdata.standardize_d13C()`.

        By default equal to `{&#39;ETH-1&#39;: 2.02, &#39;ETH-2&#39;: -10.17, &#39;ETH-3&#39;: 1.71}` after
        [Bernasconi et al. (2018)].

        [Bernasconi et al. (2018)]: https://doi.org/10.1029/2017GC007385
        &#39;&#39;&#39;

        Nominal_d18O_VPDB = {
                &#39;ETH-1&#39;: -2.19,
                &#39;ETH-2&#39;: -18.69,
                &#39;ETH-3&#39;: -1.78,
                }       # (Bernasconi et al., 2018)
        &#39;&#39;&#39;
        Nominal δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VPDB&lt;/sub&gt; values assigned to carbonate standards, used by
        `D4xdata.standardize_d18O()`.

        By default equal to `{&#39;ETH-1&#39;: -2.19, &#39;ETH-2&#39;: -18.69, &#39;ETH-3&#39;: -1.78}` after
        [Bernasconi et al. (2018)].

        [Bernasconi et al. (2018)]: https://doi.org/10.1029/2017GC007385
        &#39;&#39;&#39;

        d13C_STANDARDIZATION_METHOD = &#39;2pt&#39;
        &#39;&#39;&#39;
        Method by which to standardize δ&lt;sup&gt;13&lt;/sup&gt;C values:
        
        + `none`: do not apply any δ&lt;sup&gt;13&lt;/sup&gt;C standardization.
        + `&#39;1pt&#39;`: within each session, offset all initial δ&lt;sup&gt;13&lt;/sup&gt;C values so as to
        minimize the difference between final δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; values and
        `Nominal_d13C_VPDB` (averaged over all analyses for which `Nominal_d13C_VPDB` is defined).
        + `&#39;2pt&#39;`: within each session, apply a affine trasformation to all δ&lt;sup&gt;13&lt;/sup&gt;C
        values so as to minimize the difference between final δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt;
        values and `Nominal_d13C_VPDB` (averaged over all analyses for which `Nominal_d13C_VPDB`
        is defined).
        &#39;&#39;&#39;

        d18O_STANDARDIZATION_METHOD = &#39;2pt&#39;
        &#39;&#39;&#39;
        Method by which to standardize δ&lt;sup&gt;18&lt;/sup&gt;O values:
        
        + `none`: do not apply any δ&lt;sup&gt;18&lt;/sup&gt;O standardization.
        + `&#39;1pt&#39;`: within each session, offset all initial δ&lt;sup&gt;18&lt;/sup&gt;O values so as to
        minimize the difference between final δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VPDB&lt;/sub&gt; values and
        `Nominal_d18O_VPDB` (averaged over all analyses for which `Nominal_d18O_VPDB` is defined).
        + `&#39;2pt&#39;`: within each session, apply a affine trasformation to all δ&lt;sup&gt;18&lt;/sup&gt;O
        values so as to minimize the difference between final δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VPDB&lt;/sub&gt;
        values and `Nominal_d18O_VPDB` (averaged over all analyses for which `Nominal_d18O_VPDB`
        is defined).
        &#39;&#39;&#39;

        def __init__(self, l = [], mass = &#39;47&#39;, logfile = &#39;&#39;, session = &#39;mySession&#39;, verbose = False):
                &#39;&#39;&#39;
                __Parameters__

                + `l`: a list of dictionaries, with each dictionary including at least the keys
                `Sample`, `d45`, `d46`, and `d47` or `d48`.
                + `logfile`: if specified, write detailed logs to this file path when calling `D4xdata` methods.
                + `session`: define session name for analyses without a `Session` key
                + `verbose`: if `True`, print out detailed logs when calling `D4xdata` methods.

                Returns a `D47data` object derived from `list`.
                &#39;&#39;&#39;
                self._4x = mass
                self.verbose = verbose
                self.prefix = &#39;D4xdata&#39;
                self.logfile = logfile
                list.__init__(self, l)
                self.Nf = None
                self.repeatability = {}
                self.refresh(session = session)


        def make_verbal(oldfun):
                &#39;&#39;&#39;
                Decorator: allow temporarily changing `self.prefix` and overriding `self.verbose`.
                &#39;&#39;&#39;
                @wraps(oldfun)
                def newfun(*args, verbose = &#39;&#39;, **kwargs):
                        myself = args[0]
                        oldprefix = myself.prefix
                        myself.prefix = oldfun.__name__
                        if verbose != &#39;&#39;:
                                oldverbose = myself.verbose
                                myself.verbose = verbose
                        out = oldfun(*args, **kwargs)
                        myself.prefix = oldprefix
                        if verbose != &#39;&#39;:
                                myself.verbose = oldverbose
                        return out
                return newfun


        def msg(self, txt):
                &#39;&#39;&#39;
                Log a message to `self.logfile`, and print it out if `verbose = True`
                &#39;&#39;&#39;
                self.log(txt)
                if self.verbose:
                        print(f&#39;{f&#34;[{self.prefix}]&#34;:&lt;16} {txt}&#39;)


        def vmsg(self, txt):
                &#39;&#39;&#39;
                Log a message to `self.logfile` and print it out
                &#39;&#39;&#39;
                self.log(txt)
                print(txt)


        def log(self, *txts):
                &#39;&#39;&#39;
                Log a message to `self.logfile`
                &#39;&#39;&#39;
                if self.logfile:
                        with open(self.logfile, &#39;a&#39;) as fid:
                                for txt in txts:
                                        fid.write(f&#39;\n{dt.now().strftime(&#34;%Y-%m-%d %H:%M:%S&#34;)} {f&#34;[{self.prefix}]&#34;:&lt;16} {txt}&#39;)


        def refresh(self, session = &#39;mySession&#39;):
                &#39;&#39;&#39;
                Update `self.sessions`, `self.samples`, `self.anchors`, and `self.unknowns`.
                &#39;&#39;&#39;
                self.fill_in_missing_info(session = session)
                self.refresh_sessions()
                self.refresh_samples()


        def refresh_sessions(self):
                &#39;&#39;&#39;
                Update `self.sessions` and set `scrambling_drift`, `slope_drift`, and `wg_drift`
                to `False` for all sessions.
                &#39;&#39;&#39;
                self.sessions = {
                        s: {&#39;data&#39;: [r for r in self if r[&#39;Session&#39;] == s]}
                        for s in sorted({r[&#39;Session&#39;] for r in self})
                        }
                for s in self.sessions:
                        self.sessions[s][&#39;scrambling_drift&#39;] = False
                        self.sessions[s][&#39;slope_drift&#39;] = False
                        self.sessions[s][&#39;wg_drift&#39;] = False
                        self.sessions[s][&#39;d13C_standardization_method&#39;] = self.d13C_STANDARDIZATION_METHOD
                        self.sessions[s][&#39;d18O_standardization_method&#39;] = self.d18O_STANDARDIZATION_METHOD


        def refresh_samples(self):
                &#39;&#39;&#39;
                Define `self.samples`, `self.anchors`, and `self.unknowns`.
                &#39;&#39;&#39;
                self.samples = {
                        s: {&#39;data&#39;: [r for r in self if r[&#39;Sample&#39;] == s]}
                        for s in sorted({r[&#39;Sample&#39;] for r in self})
                        }
                self.anchors = {s: self.samples[s] for s in self.samples if s in self.Nominal_D4x}
                self.unknowns = {s: self.samples[s] for s in self.samples if s not in self.Nominal_D4x}


        def read(self, filename, sep = &#39;&#39;, session = &#39;&#39;):
                &#39;&#39;&#39;
                Read file in csv format to load data into a `D47data` object.

                In the csv file, spaces before and after field separators (`&#39;,&#39;` by default)
                are optional. Each line corresponds to a single analysis.

                The required fields are:

                + `UID`: a unique identifier
                + `Session`: an identifier for the analytical session
                + `Sample`: a sample identifier
                + `d45`, `d46`, and at least one of `d47` or `d48`: the working-gas delta values

                Independently known oxygen-17 anomalies may be provided as `D17O` (in ‰ relative to
                VSMOW, λ = `self.lambda_17`), and are otherwise assumed to be zero. Working-gas deltas `d47`, `d48`
                and `d49` are optional, and set to NaN by default.

                __Parameters__

                + `fileneme`: the path of the file to read
                + `sep`: csv separator delimiting the fields
                + `session`: set `Session` field to this string for all analyses
                &#39;&#39;&#39;
                with open(filename) as fid:
                        self.input(fid.read(), sep = sep, session = session)


        def input(self, txt, sep = &#39;&#39;, session = &#39;&#39;):
                &#39;&#39;&#39;
                Read `txt` string in csv format to load analysis data into a `D47data` object.

                In the csv string, spaces before and after field separators (`&#39;,&#39;` by default)
                are optional. Each line corresponds to a single analysis.

                The required fields are:

                + `UID`: a unique identifier
                + `Session`: an identifier for the analytical session
                + `Sample`: a sample identifier
                + `d45`, `d46`, and at least one of `d47` or `d48`: the working-gas delta values

                Independently known oxygen-17 anomalies may be provided as `D17O` (in ‰ relative to
                VSMOW, λ = `self.lambda_17`), and are otherwise assumed to be zero. Working-gas deltas `d47`, `d48`
                and `d49` are optional, and set to NaN by default.

                __Parameters__

                + `txt`: the csv string to read
                + `sep`: csv separator delimiting the fields. By default, use `,`, `;`, or `\t`,
                whichever appers most often in `txt`.
                + `session`: set `Session` field to this string for all analyses
                &#39;&#39;&#39;
                if sep == &#39;&#39;:
                        sep = sorted(&#39;,;\t&#39;, key = lambda x: - txt.count(x))[0]
                txt = [[x.strip() for x in l.split(sep)] for l in txt.splitlines() if l.strip()]
                data = [{k: v if k in [&#39;UID&#39;, &#39;Session&#39;, &#39;Sample&#39;] else smart_type(v) for k,v in zip(txt[0], l) if v != &#39;&#39;} for l in txt[1:]]

                if session != &#39;&#39;:
                        for r in data:
                                r[&#39;Session&#39;] = session

                self += data
                self.refresh()


        @make_verbal
        def wg(self, samples = None, a18_acid = None):
                &#39;&#39;&#39;
                Compute bulk composition of the working gas for each session based on
                the carbonate standards defined in both `self.Nominal_d13C_VPDB` and
                `self.Nominal_d18O_VPDB`.
                &#39;&#39;&#39;

                self.msg(&#39;Computing WG composition:&#39;)

                if a18_acid is None:
                        a18_acid = self.ALPHA_18O_ACID_REACTION
                if samples is None:
                        samples = [s for s in self.Nominal_d13C_VPDB if s in self.Nominal_d18O_VPDB]

                assert a18_acid, f&#39;Acid fractionation factor should not be zero.&#39;

                samples = [s for s in samples if s in self.Nominal_d13C_VPDB and s in self.Nominal_d18O_VPDB]
                R45R46_standards = {}
                for sample in samples:
                        d13C_vpdb = self.Nominal_d13C_VPDB[sample]
                        d18O_vpdb = self.Nominal_d18O_VPDB[sample]
                        R13_s = self.R13_VPDB * (1 + d13C_vpdb / 1000)
                        R17_s = self.R17_VPDB * ((1 + d18O_vpdb / 1000) * a18_acid) ** self.lambda_17
                        R18_s = self.R18_VPDB * (1 + d18O_vpdb / 1000) * a18_acid

                        C12_s = 1 / (1 + R13_s)
                        C13_s = R13_s / (1 + R13_s)
                        C16_s = 1 / (1 + R17_s + R18_s)
                        C17_s = R17_s / (1 + R17_s + R18_s)
                        C18_s = R18_s / (1 + R17_s + R18_s)

                        C626_s = C12_s * C16_s ** 2
                        C627_s = 2 * C12_s * C16_s * C17_s
                        C628_s = 2 * C12_s * C16_s * C18_s
                        C636_s = C13_s * C16_s ** 2
                        C637_s = 2 * C13_s * C16_s * C17_s
                        C727_s = C12_s * C17_s ** 2

                        R45_s = (C627_s + C636_s) / C626_s
                        R46_s = (C628_s + C637_s + C727_s) / C626_s
                        R45R46_standards[sample] = (R45_s, R46_s)
                
                for s in self.sessions:
                        db = [r for r in self.sessions[s][&#39;data&#39;] if r[&#39;Sample&#39;] in samples]
                        assert db, f&#39;No sample from {samples} found in session &#34;{s}&#34;.&#39;
#                       dbsamples = sorted({r[&#39;Sample&#39;] for r in db})

                        X = [r[&#39;d45&#39;] for r in db]
                        Y = [R45R46_standards[r[&#39;Sample&#39;]][0] for r in db]
                        x1, x2 = np.min(X), np.max(X)

                        if x1 &lt; x2:
                                wgcoord = x1/(x1-x2)
                        else:
                                wgcoord = 999

                        if wgcoord &lt; -.5 or wgcoord &gt; 1.5:
                                # unreasonable to extrapolate to d45 = 0
                                R45_wg = np.mean([y/(1+x/1000) for x,y in zip(X,Y)])
                        else :
                                # d45 = 0 is reasonably well bracketed
                                R45_wg = np.polyfit(X, Y, 1)[1]

                        X = [r[&#39;d46&#39;] for r in db]
                        Y = [R45R46_standards[r[&#39;Sample&#39;]][1] for r in db]
                        x1, x2 = np.min(X), np.max(X)

                        if x1 &lt; x2:
                                wgcoord = x1/(x1-x2)
                        else:
                                wgcoord = 999

                        if wgcoord &lt; -.5 or wgcoord &gt; 1.5:
                                # unreasonable to extrapolate to d46 = 0
                                R46_wg = np.mean([y/(1+x/1000) for x,y in zip(X,Y)])
                        else :
                                # d46 = 0 is reasonably well bracketed
                                R46_wg = np.polyfit(X, Y, 1)[1]

                        d13Cwg_VPDB, d18Owg_VSMOW = self.compute_bulk_delta(R45_wg, R46_wg)

                        self.msg(f&#39;Session {s} WG:   δ13C_VPDB = {d13Cwg_VPDB:.3f}   δ18O_VSMOW = {d18Owg_VSMOW:.3f}&#39;)

                        self.sessions[s][&#39;d13Cwg_VPDB&#39;] = d13Cwg_VPDB
                        self.sessions[s][&#39;d18Owg_VSMOW&#39;] = d18Owg_VSMOW
                        for r in self.sessions[s][&#39;data&#39;]:
                                r[&#39;d13Cwg_VPDB&#39;] = d13Cwg_VPDB
                                r[&#39;d18Owg_VSMOW&#39;] = d18Owg_VSMOW


        def compute_bulk_delta(self, R45, R46, D17O = 0):
                &#39;&#39;&#39;
                Compute δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; and δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt;,
                by solving the generalized form of equation (17) from [Brand et al. (2010)],
                assuming that δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt; is not too big (0 ± 50 ‰) and
                solving the corresponding second-order Taylor polynomial.
                (Appendix A of [Daëron et al., 2016])

                [Brand et al. (2010)]: https://doi.org/10.1351/PAC-REP-09-01-05
                [Daëron et al., 2016]: https://doi.org/10.1016/j.chemgeo.2016.08.014
                &#39;&#39;&#39;

                K = np.exp(D17O / 1000) * self.R17_VSMOW * self.R18_VSMOW ** -self.lambda_17

                A = -3 * K ** 2 * self.R18_VSMOW ** (2 * self.lambda_17)
                B = 2 * K * R45 * self.R18_VSMOW ** self.lambda_17
                C = 2 * self.R18_VSMOW
                D = -R46

                aa = A * self.lambda_17 * (2 * self.lambda_17 - 1) + B * self.lambda_17 * (self.lambda_17 - 1) / 2
                bb = 2 * A * self.lambda_17 + B * self.lambda_17 + C
                cc = A + B + C + D

                d18O_VSMOW = 1000 * (-bb + (bb ** 2 - 4 * aa * cc) ** .5) / (2 * aa)

                R18 = (1 + d18O_VSMOW / 1000) * self.R18_VSMOW
                R17 = K * R18 ** self.lambda_17
                R13 = R45 - 2 * R17

                d13C_VPDB = 1000 * (R13 / self.R13_VPDB - 1)

                return d13C_VPDB, d18O_VSMOW


        @make_verbal
        def crunch(self, verbose = &#39;&#39;):
                &#39;&#39;&#39;
                Compute bulk composition and raw clumped isotope anomalies for all analyses.
                &#39;&#39;&#39;
                for r in self:
                        self.compute_bulk_and_clumping_deltas(r)
                self.standardize_d13C()
                self.standardize_d18O()
                self.msg(f&#34;Crunched {len(self)} analyses.&#34;)


        def fill_in_missing_info(self, session = &#39;mySession&#39;):
                &#39;&#39;&#39;
                Fill in optional fields with default values
                &#39;&#39;&#39;
                for i,r in enumerate(self):
                        if &#39;D17O&#39; not in r:
                                r[&#39;D17O&#39;] = 0.
                        if &#39;UID&#39; not in r:
                                r[&#39;UID&#39;] = f&#39;{i+1}&#39;
                        if &#39;Session&#39; not in r:
                                r[&#39;Session&#39;] = session
                        for k in [&#39;d47&#39;, &#39;d48&#39;, &#39;d49&#39;]:
                                if k not in r:
                                        r[k] = np.nan


        def standardize_d13C(self):
                &#39;&#39;&#39;
                Perform δ&lt;sup&gt;13&lt;/sup&gt;C standadization within each session `s` according to
                `self.sessions[s][&#39;d13C_standardization_method&#39;]`, which is defined by default
                by `D47data.refresh_sessions()`as equal to `self.d13C_STANDARDIZATION_METHOD`, but
                may be redefined abitrarily at a later stage.
                &#39;&#39;&#39;
                for s in self.sessions:
                        if self.sessions[s][&#39;d13C_standardization_method&#39;] in [&#39;1pt&#39;, &#39;2pt&#39;]:
                                XY = [(r[&#39;d13C_VPDB&#39;], self.Nominal_d13C_VPDB[r[&#39;Sample&#39;]]) for r in self.sessions[s][&#39;data&#39;] if r[&#39;Sample&#39;] in self.Nominal_d13C_VPDB]
                                X,Y = zip(*XY)
                                if self.sessions[s][&#39;d13C_standardization_method&#39;] == &#39;1pt&#39;:
                                        offset = np.mean(Y) - np.mean(X)
                                        for r in self.sessions[s][&#39;data&#39;]:
                                                r[&#39;d13C_VPDB&#39;] += offset                                
                                elif self.sessions[s][&#39;d13C_standardization_method&#39;] == &#39;2pt&#39;:
                                        a,b = np.polyfit(X,Y,1)
                                        for r in self.sessions[s][&#39;data&#39;]:
                                                r[&#39;d13C_VPDB&#39;] = a * r[&#39;d13C_VPDB&#39;] + b

        def standardize_d18O(self):
                &#39;&#39;&#39;
                Perform δ&lt;sup&gt;18&lt;/sup&gt;O standadization within each session `s` according to
                `self.ALPHA_18O_ACID_REACTION` and `self.sessions[s][&#39;d18O_standardization_method&#39;]`,
                which is defined by default by `D47data.refresh_sessions()`as equal to
                `self.d18O_STANDARDIZATION_METHOD`, but may be redefined abitrarily at a later stage.
                &#39;&#39;&#39;
                for s in self.sessions:
                        if self.sessions[s][&#39;d18O_standardization_method&#39;] in [&#39;1pt&#39;, &#39;2pt&#39;]:
                                XY = [(r[&#39;d18O_VSMOW&#39;], self.Nominal_d18O_VPDB[r[&#39;Sample&#39;]]) for r in self.sessions[s][&#39;data&#39;] if r[&#39;Sample&#39;] in self.Nominal_d18O_VPDB]
                                X,Y = zip(*XY)
                                Y = [(1000+y) * self.R18_VPDB * self.ALPHA_18O_ACID_REACTION / self.R18_VSMOW - 1000 for y in Y]
                                if self.sessions[s][&#39;d18O_standardization_method&#39;] == &#39;1pt&#39;:
                                        offset = np.mean(Y) - np.mean(X)
                                        for r in self.sessions[s][&#39;data&#39;]:
                                                r[&#39;d18O_VSMOW&#39;] += offset                               
                                elif self.sessions[s][&#39;d18O_standardization_method&#39;] == &#39;2pt&#39;:
                                        a,b = np.polyfit(X,Y,1)
                                        for r in self.sessions[s][&#39;data&#39;]:
                                                r[&#39;d18O_VSMOW&#39;] = a * r[&#39;d18O_VSMOW&#39;] + b
        

        def compute_bulk_and_clumping_deltas(self, r):
                &#39;&#39;&#39;
                Compute δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt;, δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt;, and
                raw Δ&lt;sub&gt;47&lt;/sub&gt;, Δ&lt;sub&gt;48&lt;/sub&gt;, Δ&lt;sub&gt;49&lt;/sub&gt; values for an analysis `r`.
                &#39;&#39;&#39;

                # Compute working gas R13, R18, and isobar ratios
                R13_wg = self.R13_VPDB * (1 + r[&#39;d13Cwg_VPDB&#39;] / 1000)
                R18_wg = self.R18_VSMOW * (1 + r[&#39;d18Owg_VSMOW&#39;] / 1000)
                R45_wg, R46_wg, R47_wg, R48_wg, R49_wg = self.compute_isobar_ratios(R13_wg, R18_wg)

                # Compute analyte isobar ratios
                R45 = (1 + r[&#39;d45&#39;] / 1000) * R45_wg
                R46 = (1 + r[&#39;d46&#39;] / 1000) * R46_wg
                R47 = (1 + r[&#39;d47&#39;] / 1000) * R47_wg
                R48 = (1 + r[&#39;d48&#39;] / 1000) * R48_wg
                R49 = (1 + r[&#39;d49&#39;] / 1000) * R49_wg

                r[&#39;d13C_VPDB&#39;], r[&#39;d18O_VSMOW&#39;] = self.compute_bulk_delta(R45, R46, D17O = r[&#39;D17O&#39;])
                R13 = (1 + r[&#39;d13C_VPDB&#39;] / 1000) * self.R13_VPDB
                R18 = (1 + r[&#39;d18O_VSMOW&#39;] / 1000) * self.R18_VSMOW

                # Compute stochastic isobar ratios of the analyte
                R45stoch, R46stoch, R47stoch, R48stoch, R49stoch = self.compute_isobar_ratios(
                        R13, R18, D17O = r[&#39;D17O&#39;]
                )

                # Check that R45/R45stoch and R46/R46stoch are undistinguishable from 1,
                # and raise a warning if the corresponding anomalies exceed 0.02 ppm.
                if (R45 / R45stoch - 1) &gt; 5e-8:
                        self.vmsg(f&#39;This is unexpected: R45/R45stoch - 1 = {1e6 * (R45 / R45stoch - 1):.3f} ppm&#39;)
                if (R46 / R46stoch - 1) &gt; 5e-8:
                        self.vmsg(f&#39;This is unexpected: R46/R46stoch - 1 = {1e6 * (R46 / R46stoch - 1):.3f} ppm&#39;)

                # Compute raw clumped isotope anomalies
                r[&#39;D47raw&#39;] = 1000 * (R47 / R47stoch - 1)
                r[&#39;D48raw&#39;] = 1000 * (R48 / R48stoch - 1)
                r[&#39;D49raw&#39;] = 1000 * (R49 / R49stoch - 1)


        def compute_isobar_ratios(self, R13, R18, D17O=0, D47=0, D48=0, D49=0):
                &#39;&#39;&#39;
                Compute isobar ratios for a sample with isotopic ratios `R13` and `R18`,
                optionally accounting for non-zero values of Δ&lt;sup&gt;17&lt;/sup&gt;O (`D17O`) and clumped isotope
                anomalies (`D47`, `D48`, `D49`), all expressed in permil.
                &#39;&#39;&#39;

                # Compute R17
                R17 = self.R17_VSMOW * np.exp(D17O / 1000) * (R18 / self.R18_VSMOW) ** self.lambda_17

                # Compute isotope concentrations
                C12 = (1 + R13) ** -1
                C13 = C12 * R13
                C16 = (1 + R17 + R18) ** -1
                C17 = C16 * R17
                C18 = C16 * R18

                # Compute stochastic isotopologue concentrations
                C626 = C16 * C12 * C16
                C627 = C16 * C12 * C17 * 2
                C628 = C16 * C12 * C18 * 2
                C636 = C16 * C13 * C16
                C637 = C16 * C13 * C17 * 2
                C638 = C16 * C13 * C18 * 2
                C727 = C17 * C12 * C17
                C728 = C17 * C12 * C18 * 2
                C737 = C17 * C13 * C17
                C738 = C17 * C13 * C18 * 2
                C828 = C18 * C12 * C18
                C838 = C18 * C13 * C18

                # Compute stochastic isobar ratios
                R45 = (C636 + C627) / C626
                R46 = (C628 + C637 + C727) / C626
                R47 = (C638 + C728 + C737) / C626
                R48 = (C738 + C828) / C626
                R49 = C838 / C626

                # Account for stochastic anomalies
                R47 *= 1 + D47 / 1000
                R48 *= 1 + D48 / 1000
                R49 *= 1 + D49 / 1000

                # Return isobar ratios
                return R45, R46, R47, R48, R49


        def split_samples(self, samples_to_split = &#39;all&#39;, grouping = &#39;by_session&#39;):
                &#39;&#39;&#39;
                Split unknown samples by UID (treat all analyses as different samples)
                or by session (treat analyses of a given sample in different sessions as
                different samples).

                __Parameters__

                + `samples_to_split`: a list of samples to split, e.g., `[&#39;IAEA-C1&#39;, &#39;IAEA-C2&#39;]`
                + `grouping`: `by_uid` | `by_session`
                &#39;&#39;&#39;
                if samples_to_split == &#39;all&#39;:
                        samples_to_split = [s for s in self.unknowns]
                gkeys = {&#39;by_uid&#39;:&#39;UID&#39;, &#39;by_session&#39;:&#39;Session&#39;}
                self.grouping = grouping.lower()
                if self.grouping in gkeys:
                        gkey = gkeys[self.grouping]
                for r in self:
                        if r[&#39;Sample&#39;] in samples_to_split:
                                r[&#39;Sample_original&#39;] = r[&#39;Sample&#39;]
                                r[&#39;Sample&#39;] = f&#34;{r[&#39;Sample&#39;]}__{r[gkey]}&#34;
                        elif r[&#39;Sample&#39;] in self.unknowns:
                                r[&#39;Sample_original&#39;] = r[&#39;Sample&#39;]
                self.refresh_samples()


        def unsplit_samples(self, tables = False):
                &#39;&#39;&#39;
                Reverse the effects of `D47data.split_samples()`.
                
                This should only be used after `D4xdata.standardize()` with `method=&#39;pooled&#39;`.
                
                After `D4xdata.standardize()` with `method=&#39;indep_sessions&#39;`, one should
                probably use `D4xdata.combine_samples()` instead to reverse the effects of
                `D47data.split_samples()` with `grouping=&#39;by_uid&#39;`, or `w_avg()` to reverse the
                effects of `D47data.split_samples()` with `grouping=&#39;by_sessions&#39;` (because in
                that case session-averaged Δ&lt;sub&gt;4x&lt;/sub&gt; values are statistically independent).
                &#39;&#39;&#39;
                unknowns_old = sorted({s for s in self.unknowns})
                CM_old = self.standardization.covar[:,:]
                VD_old = self.standardization.params.valuesdict().copy()
                vars_old = self.standardization.var_names

                unknowns_new = sorted({r[&#39;Sample_original&#39;] for r in self if &#39;Sample_original&#39; in r})

                Ns = len(vars_old) - len(unknowns_old)
                vars_new = vars_old[:Ns] + [f&#39;D{self._4x}_{pf(u)}&#39; for u in unknowns_new]
                VD_new = {k: VD_old[k] for k in vars_old[:Ns]}

                W = np.zeros((len(vars_new), len(vars_old)))
                W[:Ns,:Ns] = np.eye(Ns)
                for u in unknowns_new:
                        splits = sorted({r[&#39;Sample&#39;] for r in self if &#39;Sample_original&#39; in r and r[&#39;Sample_original&#39;] == u})
                        if self.grouping == &#39;by_session&#39;:
                                weights = [self.samples[s][f&#39;SE_D{self._4x}&#39;]**-2 for s in splits]
                        elif self.grouping == &#39;by_uid&#39;:
                                weights = [1 for s in splits]
                        sw = sum(weights)
                        weights = [w/sw for w in weights]
                        W[vars_new.index(f&#39;D{self._4x}_{pf(u)}&#39;),[vars_old.index(f&#39;D{self._4x}_{pf(s)}&#39;) for s in splits]] = weights[:]

                CM_new = W @ CM_old @ W.T
                V = W @ np.array([[VD_old[k]] for k in vars_old])
                VD_new = {k:v[0] for k,v in zip(vars_new, V)}

                self.standardization.covar = CM_new
                self.standardization.params.valuesdict = lambda : VD_new
                self.standardization.var_names = vars_new

                for r in self:
                        if r[&#39;Sample&#39;] in self.unknowns:
                                r[&#39;Sample_split&#39;] = r[&#39;Sample&#39;]
                                r[&#39;Sample&#39;] = r[&#39;Sample_original&#39;]

                self.refresh_samples()
                self.consolidate_samples()
                self.repeatabilities()

                if tables:
                        self.table_of_analyses()
                        self.table_of_samples()

        def assign_timestamps(self):
                &#39;&#39;&#39;
                Assign a time field `t` of type `float` to each analysis.

                If `TimeTag` is one of the data fields, `t` is equal within a given session
                to `TimeTag` minus the mean value of `TimeTag` for that session.
                Otherwise, `TimeTag` is by default equal to the index of each analysis
                in the dataset and `t` is defined as above.
                &#39;&#39;&#39;
                for session in self.sessions:
                        sdata = self.sessions[session][&#39;data&#39;]
                        try:
                                t0 = np.mean([r[&#39;TimeTag&#39;] for r in sdata])
                                for r in sdata:
                                        r[&#39;t&#39;] = r[&#39;TimeTag&#39;] - t0
#                               print(&#39;DEBUG - USING TimeTag        &lt;-----------------------------------&#39;)
                        except KeyError:
                                t0 = (len(sdata)-1)/2
                                for t,r in enumerate(sdata):
                                        r[&#39;t&#39;] = t - t0


        def report(self):
                &#39;&#39;&#39;
                Prints a report on the standardization fit.
                &#39;&#39;&#39;
                report_fit(self.standardization)


        def combine_samples(self, sample_groups):
                &#39;&#39;&#39;
                Combine analyses of different samples to compute weighted average Δ&lt;sub&gt;47&lt;/sub&gt;
                and new error (co)variances corresponding to the groups defined by the `sample_groups`
                dictionary.
                
                Caution: samples are weighted by number of replicate analyses, which is a
                reasonable default behavior but is not always optimal (e.g., in the case of strongly
                correlated analytical errors for one or more samples).
                
                Returns a tuplet of:
                
                + the list of group names
                + an array of the corresponding Δ&lt;sub&gt;47&lt;/sub&gt; values
                + the corresponding (co)variance matrix
                
                __Parameters__

                + `sample_groups`: a dictionary of the form:
                ```py
                {&#39;group1&#39;: [&#39;sample_1&#39;, &#39;sample_2&#39;],
                 &#39;group2&#39;: [&#39;sample_3&#39;, &#39;sample_4&#39;, &#39;sample_5&#39;]}
                ```
                &#39;&#39;&#39;
                
                samples = [s for k in sorted(sample_groups.keys()) for s in sorted(sample_groups[k])]
                groups = sorted(sample_groups.keys())
                group_total_weights = {k: sum([self.samples[s][&#39;N&#39;] for s in sample_groups[k]]) for k in groups}
                D4x_old = np.array([[self.samples[x][f&#39;D{self._4x}&#39;]] for x in samples])
                CM_old = np.array([[self.sample_D4x_covar(x,y) for x in samples] for y in samples])
                W = np.array([
                        [self.samples[i][&#39;N&#39;]/group_total_weights[j] if i in sample_groups[j] else 0 for i in samples]
                        for j in groups])
                D4x_new = W @ D4x_old
                CM_new = W @ CM_old @ W.T

                return groups, D4x_new[:,0], CM_new
                

        @make_verbal
        def standardize(self,
                method = &#39;pooled&#39;,
                weighted_sessions = [],
                consolidate = True,
                consolidate_tables = False,
                consolidate_plots = False,
                constraints = {},
                ):
                &#39;&#39;&#39;
                Compute absolute Δ&lt;sub&gt;47&lt;/sub&gt; values for all replicate analyses and for sample averages.
                If `method` argument is set to `&#39;pooled&#39;`, the standardization processes all sessions
                in a single step, assuming that all samples (anchors and unknowns alike) are
                homogeneous (i.e. that their true Δ&lt;sub&gt;47&lt;/sub&gt; value does not change between sessions).
                If `method` argument is set to `&#39;indep_sessions&#39;`, the standardization processes each
                session independently, based only on anchors analyses.
                &#39;&#39;&#39;

                self.standardization_method = method
                self.assign_timestamps()

                if method == &#39;pooled&#39;:
                        if weighted_sessions:
                                for session_group in weighted_sessions:
                                        X = D4xdata([r for r in self if r[&#39;Session&#39;] in session_group], mass = self._4x)
                                        X.Nominal_D4x = self.Nominal_D4x.copy()
                                        X.refresh()
                                        result = X.standardize(method = &#39;pooled&#39;, weighted_sessions = [], consolidate = False)
                                        w = np.sqrt(result.redchi)
                                        self.msg(f&#39;Session group {session_group} MRSWD = {w:.4f}&#39;)
                                        for r in X:
                                                r[f&#39;wD{self._4x}raw&#39;] *= w
                        else:
                                self.msg(f&#39;All D{self._4x}raw weights set to 1 ‰&#39;)
                                for r in self:
                                        r[f&#39;wD{self._4x}raw&#39;] = 1.

                        params = Parameters()
                        for k,session in enumerate(self.sessions):
                                self.msg(f&#34;Session {session}: scrambling_drift is {self.sessions[session][&#39;scrambling_drift&#39;]}.&#34;)
                                self.msg(f&#34;Session {session}: slope_drift is {self.sessions[session][&#39;slope_drift&#39;]}.&#34;)
                                self.msg(f&#34;Session {session}: wg_drift is {self.sessions[session][&#39;wg_drift&#39;]}.&#34;)
                                s = pf(session)
                                params.add(f&#39;a_{s}&#39;, value = 0.9)
                                params.add(f&#39;b_{s}&#39;, value = 0.)
                                params.add(f&#39;c_{s}&#39;, value = -0.9)
                                params.add(f&#39;a2_{s}&#39;, value = 0., vary = self.sessions[session][&#39;scrambling_drift&#39;])
                                params.add(f&#39;b2_{s}&#39;, value = 0., vary = self.sessions[session][&#39;slope_drift&#39;])
                                params.add(f&#39;c2_{s}&#39;, value = 0., vary = self.sessions[session][&#39;wg_drift&#39;])
                        for sample in self.unknowns:
                                params.add(f&#39;D{self._4x}_{pf(sample)}&#39;, value = 0.5)

                        for k in constraints:
                                params[k].expr = constraints[k]

                        def residuals(p):
                                R = []
                                for r in self:
                                        session = pf(r[&#39;Session&#39;])
                                        sample = pf(r[&#39;Sample&#39;])
                                        if r[&#39;Sample&#39;] in self.Nominal_D4x:
                                                R += [ (
                                                        r[f&#39;D{self._4x}raw&#39;] - (
                                                                p[f&#39;a_{session}&#39;] * self.Nominal_D4x[r[&#39;Sample&#39;]]
                                                                + p[f&#39;b_{session}&#39;] * r[f&#39;d{self._4x}&#39;]
                                                                +       p[f&#39;c_{session}&#39;]
                                                                + r[&#39;t&#39;] * (
                                                                        p[f&#39;a2_{session}&#39;] * self.Nominal_D4x[r[&#39;Sample&#39;]]
                                                                        + p[f&#39;b2_{session}&#39;] * r[f&#39;d{self._4x}&#39;]
                                                                        +       p[f&#39;c2_{session}&#39;]
                                                                        )
                                                                )
                                                        ) / r[f&#39;wD{self._4x}raw&#39;] ]
                                        else:
                                                R += [ (
                                                        r[f&#39;D{self._4x}raw&#39;] - (
                                                                p[f&#39;a_{session}&#39;] * p[f&#39;D{self._4x}_{sample}&#39;]
                                                                + p[f&#39;b_{session}&#39;] * r[f&#39;d{self._4x}&#39;]
                                                                +       p[f&#39;c_{session}&#39;]
                                                                + r[&#39;t&#39;] * (
                                                                        p[f&#39;a2_{session}&#39;] * p[f&#39;D{self._4x}_{sample}&#39;]
                                                                        + p[f&#39;b2_{session}&#39;] * r[f&#39;d{self._4x}&#39;]
                                                                        +       p[f&#39;c2_{session}&#39;]
                                                                        )
                                                                )
                                                        ) / r[f&#39;wD{self._4x}raw&#39;] ]
                                return R

                        M = Minimizer(residuals, params)
                        result = M.least_squares()
                        self.Nf = result.nfree
                        self.t95 = tstudent.ppf(1 - 0.05/2, self.Nf)
#                       if self.verbose:
#                               report_fit(result)

                        for r in self:
                                s = pf(r[&#34;Session&#34;])
                                a = result.params.valuesdict()[f&#39;a_{s}&#39;]
                                b = result.params.valuesdict()[f&#39;b_{s}&#39;]
                                c = result.params.valuesdict()[f&#39;c_{s}&#39;]
                                a2 = result.params.valuesdict()[f&#39;a2_{s}&#39;]
                                b2 = result.params.valuesdict()[f&#39;b2_{s}&#39;]
                                c2 = result.params.valuesdict()[f&#39;c2_{s}&#39;]
                                r[f&#39;D{self._4x}&#39;] = (r[f&#39;D{self._4x}raw&#39;] - c - b * r[f&#39;d{self._4x}&#39;] - c2 * r[&#39;t&#39;] - b2 * r[&#39;t&#39;] * r[f&#39;d{self._4x}&#39;]) / (a + a2 * r[&#39;t&#39;])

                        self.standardization = result

                        for session in self.sessions:
                                self.sessions[session][&#39;Np&#39;] = 3
                                for k in [&#39;scrambling&#39;, &#39;slope&#39;, &#39;wg&#39;]:
                                        if self.sessions[session][f&#39;{k}_drift&#39;]:
                                                self.sessions[session][&#39;Np&#39;] += 1

                        if consolidate:
                                self.consolidate(tables = consolidate_tables, plots = consolidate_plots)
                        return result


                elif method == &#39;indep_sessions&#39;:

                        if weighted_sessions:
                                for session_group in weighted_sessions:
                                        X = D4xdata([r for r in self if r[&#39;Session&#39;] in session_group], mass = self._4x)
                                        X.Nominal_D4x = self.Nominal_D4x.copy()
                                        X.refresh()
                                        # This is only done to assign r[&#39;wD47raw&#39;] for r in X:
                                        X.standardize(method = method, weighted_sessions = [], consolidate = False)
                                        self.msg(f&#39;D{self._4x}raw weights set to {1000*X[0][f&#34;wD{self._4x}raw&#34;]:.1f} ppm for sessions in {session_group}&#39;)
                        else:
                                self.msg(&#39;All weights set to 1 ‰&#39;)
                                for r in self:
                                        r[f&#39;wD{self._4x}raw&#39;] = 1

                        for session in self.sessions:
                                s = self.sessions[session]
                                p_names = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;a2&#39;, &#39;b2&#39;, &#39;c2&#39;]
                                p_active = [True, True, True, s[&#39;scrambling_drift&#39;], s[&#39;slope_drift&#39;], s[&#39;wg_drift&#39;]]
                                s[&#39;Np&#39;] = sum(p_active)
                                sdata = s[&#39;data&#39;]

                                A = np.array([
                                        [
                                                self.Nominal_D4x[r[&#39;Sample&#39;]] / r[f&#39;wD{self._4x}raw&#39;],
                                                r[f&#39;d{self._4x}&#39;] / r[f&#39;wD{self._4x}raw&#39;],
                                                1 / r[f&#39;wD{self._4x}raw&#39;],
                                                self.Nominal_D4x[r[&#39;Sample&#39;]] * r[&#39;t&#39;] / r[f&#39;wD{self._4x}raw&#39;],
                                                r[f&#39;d{self._4x}&#39;] * r[&#39;t&#39;] / r[f&#39;wD{self._4x}raw&#39;],
                                                r[&#39;t&#39;] / r[f&#39;wD{self._4x}raw&#39;]
                                                ]
                                        for r in sdata if r[&#39;Sample&#39;] in self.anchors
                                        ])[:,p_active] # only keep columns for the active parameters
                                Y = np.array([[r[f&#39;D{self._4x}raw&#39;] / r[f&#39;wD{self._4x}raw&#39;]] for r in sdata if r[&#39;Sample&#39;] in self.anchors])
                                s[&#39;Na&#39;] = Y.size
                                CM = linalg.inv(A.T @ A)
                                bf = (CM @ A.T @ Y).T[0,:]
                                k = 0
                                for n,a in zip(p_names, p_active):
                                        if a:
                                                s[n] = bf[k]
#                                               self.msg(f&#39;{n} = {bf[k]}&#39;)
                                                k += 1
                                        else:
                                                s[n] = 0.
#                                               self.msg(f&#39;{n} = 0.0&#39;)

                                for r in sdata :
                                        a, b, c, a2, b2, c2 = s[&#39;a&#39;], s[&#39;b&#39;], s[&#39;c&#39;], s[&#39;a2&#39;], s[&#39;b2&#39;], s[&#39;c2&#39;]
                                        r[f&#39;D{self._4x}&#39;] = (r[f&#39;D{self._4x}raw&#39;] - c - b * r[f&#39;d{self._4x}&#39;] - c2 * r[&#39;t&#39;] - b2 * r[&#39;t&#39;] * r[f&#39;d{self._4x}&#39;]) / (a + a2 * r[&#39;t&#39;])
                                        r[f&#39;wD{self._4x}&#39;] /= (a + a2 * r[&#39;t&#39;])

                                s[&#39;CM&#39;] = np.zeros((6,6))
                                i = 0
                                k_active = [j for j,a in enumerate(p_active) if a]
                                for j,a in enumerate(p_active):
                                        if a:
                                                s[&#39;CM&#39;][j,k_active] = CM[i,:]
                                                i += 1

                        if not weighted_sessions:
                                w = self.rmswd()[&#39;rmswd&#39;]
                                for r in self:
                                                r[f&#39;wD{self._4x}&#39;] *= w
                                                r[f&#39;wD{self._4x}raw&#39;] *= w
                                for session in self.sessions:
                                        self.sessions[session][&#39;CM&#39;] *= w**2

                        for session in self.sessions:
                                s = self.sessions[session]
                                s[&#39;SE_a&#39;] = s[&#39;CM&#39;][0,0]**.5
                                s[&#39;SE_b&#39;] = s[&#39;CM&#39;][1,1]**.5
                                s[&#39;SE_c&#39;] = s[&#39;CM&#39;][2,2]**.5
                                s[&#39;SE_a2&#39;] = s[&#39;CM&#39;][3,3]**.5
                                s[&#39;SE_b2&#39;] = s[&#39;CM&#39;][4,4]**.5
                                s[&#39;SE_c2&#39;] = s[&#39;CM&#39;][5,5]**.5

                        if not weighted_sessions:
                                self.Nf = len(self) - len(self.unknowns) - np.sum([self.sessions[s][&#39;Np&#39;] for s in self.sessions])
                        else:
                                self.Nf = 0
                                for sg in weighted_sessions:
                                        self.Nf += self.rmswd(sessions = sg)[&#39;Nf&#39;]

                        self.t95 = tstudent.ppf(1 - 0.05/2, self.Nf)

                        avgD4x = {
                                sample: np.mean([r[f&#39;D{self._4x}&#39;] for r in self if r[&#39;Sample&#39;] == sample])
                                for sample in self.samples
                                }
                        chi2 = np.sum([(r[f&#39;D{self._4x}&#39;] - avgD4x[r[&#39;Sample&#39;]])**2 for r in self])
                        rD4x = (chi2/self.Nf)**.5
                        self.repeatability[f&#39;sigma_{self._4x}&#39;] = rD4x

                        if consolidate:
                                self.consolidate(tables = consolidate_tables, plots = consolidate_plots)


        def standardization_error(self, session, d4x, D4x, t = 0):
                &#39;&#39;&#39;
                Compute standardization error for a given session and
                (δ&lt;sub&gt;47&lt;/sub&gt;, Δ&lt;sub&gt;47&lt;/sub&gt;) composition.
                &#39;&#39;&#39;
                a = self.sessions[session][&#39;a&#39;]
                b = self.sessions[session][&#39;b&#39;]
                c = self.sessions[session][&#39;c&#39;]
                a2 = self.sessions[session][&#39;a2&#39;]
                b2 = self.sessions[session][&#39;b2&#39;]
                c2 = self.sessions[session][&#39;c2&#39;]
                CM = self.sessions[session][&#39;CM&#39;]

                x, y = D4x, d4x
                z = a * x + b * y + c + a2 * x * t + b2 * y * t + c2 * t
#               x = (z - b*y - b2*y*t - c - c2*t) / (a+a2*t)
                dxdy = -(b+b2*t) / (a+a2*t)
                dxdz = 1. / (a+a2*t)
                dxda = -x / (a+a2*t)
                dxdb = -y / (a+a2*t)
                dxdc = -1. / (a+a2*t)
                dxda2 = -x * a2 / (a+a2*t)
                dxdb2 = -y * t / (a+a2*t)
                dxdc2 = -t / (a+a2*t)
                V = np.array([dxda, dxdb, dxdc, dxda2, dxdb2, dxdc2])
                sx = (V @ CM @ V.T) ** .5
                return sx


        @make_verbal
        def summary(self,
                dir = &#39;output&#39;,
                filename = None,
                save_to_file = True,
                print_out = True,
                ):
                &#39;&#39;&#39;
                Print out an/or save to disk a summary of the standardization results.

                __Parameters__

                + `dir`: the directory in which to save the table
                + `filename`: the name to the csv file to write to
                + `save_to_file`: whether to save the table to disk
                + `print_out`: whether to print out the table
                &#39;&#39;&#39;

                out = []
                out += [[&#39;N samples (anchors + unknowns)&#39;, f&#34;{len(self.samples)} ({len(self.anchors)} + {len(self.unknowns)})&#34;]]
                out += [[&#39;N analyses (anchors + unknowns)&#39;, f&#34;{len(self)} ({len([r for r in self if r[&#39;Sample&#39;] in self.anchors])} + {len([r for r in self if r[&#39;Sample&#39;] in self.unknowns])})&#34;]]
                out += [[&#39;Repeatability of δ13C_VPDB&#39;, f&#34;{1000 * self.repeatability[&#39;r_d13C_VPDB&#39;]:.1f} ppm&#34;]]
                out += [[&#39;Repeatability of δ18O_VSMOW&#39;, f&#34;{1000 * self.repeatability[&#39;r_d18O_VSMOW&#39;]:.1f} ppm&#34;]]
                out += [[f&#39;Repeatability of Δ{self._4x} (anchors)&#39;, f&#34;{1000 * self.repeatability[f&#39;r_D{self._4x}a&#39;]:.1f} ppm&#34;]]
                out += [[f&#39;Repeatability of Δ{self._4x} (unknowns)&#39;, f&#34;{1000 * self.repeatability[f&#39;r_D{self._4x}u&#39;]:.1f} ppm&#34;]]
                out += [[f&#39;Repeatability of Δ{self._4x} (all)&#39;, f&#34;{1000 * self.repeatability[f&#39;r_D{self._4x}&#39;]:.1f} ppm&#34;]]
                out += [[&#39;Model degrees of freedom&#39;, f&#34;{self.Nf}&#34;]]
                out += [[&#39;Student\&#39;s 95% t-factor&#39;, f&#34;{self.t95:.2f}&#34;]]
                out += [[&#39;Standardization method&#39;, self.standardization_method]]

                if save_to_file:
                        if not os.path.exists(dir):
                                os.makedirs(dir)
                        if filename is None:
                                filename = f&#39;D{self._4x}_summary.csv&#39;
                        with open(f&#39;{dir}/{filename}&#39;, &#39;w&#39;) as fid:
                                fid.write(make_csv(out))
                if print_out:
                        self.msg(&#39;\n&#39; + pretty_table(out, header = 0))


        @make_verbal
        def table_of_sessions(self,
                dir = &#39;output&#39;,
                filename = None,
                save_to_file = True,
                print_out = True,
                ):
                &#39;&#39;&#39;
                Print out an/or save to disk a table of sessions.

                __Parameters__

                + `dir`: the directory in which to save the table
                + `filename`: the name to the csv file to write to
                + `save_to_file`: whether to save the table to disk
                + `print_out`: whether to print out the table
                &#39;&#39;&#39;
                include_a2 = any([self.sessions[session][&#39;scrambling_drift&#39;] for session in self.sessions])
                include_b2 = any([self.sessions[session][&#39;slope_drift&#39;] for session in self.sessions])
                include_c2 = any([self.sessions[session][&#39;wg_drift&#39;] for session in self.sessions])

                out = [[&#39;Session&#39;,&#39;Na&#39;,&#39;Nu&#39;,&#39;d13Cwg_VPDB&#39;,&#39;d18Owg_VSMOW&#39;,&#39;r_d13C&#39;,&#39;r_d18O&#39;,f&#39;r_D{self._4x}&#39;,&#39;a ± SE&#39;,&#39;1e3 x b ± SE&#39;,&#39;c ± SE&#39;]]
                if include_a2:
                        out[-1] += [&#39;a2 ± SE&#39;]
                if include_b2:
                        out[-1] += [&#39;b2 ± SE&#39;]
                if include_c2:
                        out[-1] += [&#39;c2 ± SE&#39;]
                for session in self.sessions:
                        out += [[
                                session,
                                f&#34;{self.sessions[session][&#39;Na&#39;]}&#34;,
                                f&#34;{self.sessions[session][&#39;Nu&#39;]}&#34;,
                                f&#34;{self.sessions[session][&#39;d13Cwg_VPDB&#39;]:.3f}&#34;,
                                f&#34;{self.sessions[session][&#39;d18Owg_VSMOW&#39;]:.3f}&#34;,
                                f&#34;{self.sessions[session][&#39;r_d13C_VPDB&#39;]:.4f}&#34;,
                                f&#34;{self.sessions[session][&#39;r_d18O_VSMOW&#39;]:.4f}&#34;,
                                f&#34;{self.sessions[session][f&#39;r_D{self._4x}&#39;]:.4f}&#34;,
                                f&#34;{self.sessions[session][&#39;a&#39;]:.3f} ± {self.sessions[session][&#39;SE_a&#39;]:.3f}&#34;,
                                f&#34;{1e3*self.sessions[session][&#39;b&#39;]:.3f} ± {1e3*self.sessions[session][&#39;SE_b&#39;]:.3f}&#34;,
                                f&#34;{self.sessions[session][&#39;c&#39;]:.3f} ± {self.sessions[session][&#39;SE_c&#39;]:.3f}&#34;,
                                ]]
                        if include_a2:
                                if self.sessions[session][&#39;scrambling_drift&#39;]:
                                        out[-1] += [f&#34;{self.sessions[session][&#39;a2&#39;]:.1e} ± {self.sessions[session][&#39;SE_a2&#39;]:.1e}&#34;]
                                else:
                                        out[-1] += [&#39;&#39;]
                        if include_b2:
                                if self.sessions[session][&#39;slope_drift&#39;]:
                                        out[-1] += [f&#34;{self.sessions[session][&#39;b2&#39;]:.1e} ± {self.sessions[session][&#39;SE_b2&#39;]:.1e}&#34;]
                                else:
                                        out[-1] += [&#39;&#39;]
                        if include_c2:
                                if self.sessions[session][&#39;wg_drift&#39;]:
                                        out[-1] += [f&#34;{self.sessions[session][&#39;c2&#39;]:.1e} ± {self.sessions[session][&#39;SE_c2&#39;]:.1e}&#34;]
                                else:
                                        out[-1] += [&#39;&#39;]

                if save_to_file:
                        if not os.path.exists(dir):
                                os.makedirs(dir)
                        if filename is None:
                                filename = f&#39;D{self._4x}_sessions.csv&#39;
                        with open(f&#39;{dir}/{filename}&#39;, &#39;w&#39;) as fid:
                                fid.write(make_csv(out))
                if print_out:
                        self.msg(&#39;\n&#39; + pretty_table(out))
                return out


        @make_verbal
        def table_of_analyses(
                self,
                dir = &#39;output&#39;,
                filename = None,
                save_to_file = True,
                print_out = True,
                ):
                &#39;&#39;&#39;
                Print out an/or save to disk a table of analyses.

                __Parameters__

                + `dir`: the directory in which to save the table
                + `filename`: the name to the csv file to write to
                + `save_to_file`: whether to save the table to disk
                + `print_out`: whether to print out the table
                &#39;&#39;&#39;

                out = [[&#39;UID&#39;,&#39;Session&#39;,&#39;Sample&#39;]]
                extra_fields = [f for f in [(&#39;SampleMass&#39;,&#39;.2f&#39;),(&#39;ColdFingerPressure&#39;,&#39;.1f&#39;),(&#39;AcidReactionYield&#39;,&#39;.3f&#39;)] if f[0] in {k for r in self for k in r}]
                for f in extra_fields:
                        out[-1] += [f[0]]
                out[-1] += [&#39;d13Cwg_VPDB&#39;,&#39;d18Owg_VSMOW&#39;,&#39;d45&#39;,&#39;d46&#39;,&#39;d47&#39;,&#39;d48&#39;,&#39;d49&#39;,&#39;d13C_VPDB&#39;,&#39;d18O_VSMOW&#39;,&#39;D47raw&#39;,&#39;D48raw&#39;,&#39;D49raw&#39;,f&#39;D{self._4x}&#39;]
                for r in self:
                        out += [[f&#34;{r[&#39;UID&#39;]}&#34;,f&#34;{r[&#39;Session&#39;]}&#34;,f&#34;{r[&#39;Sample&#39;]}&#34;]]
                        for f in extra_fields:
                                out[-1] += [f&#34;{r[f[0]]:{f[1]}}&#34;]
                        out[-1] += [
                                f&#34;{r[&#39;d13Cwg_VPDB&#39;]:.3f}&#34;,
                                f&#34;{r[&#39;d18Owg_VSMOW&#39;]:.3f}&#34;,
                                f&#34;{r[&#39;d45&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;d46&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;d47&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;d48&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;d49&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;d13C_VPDB&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;d18O_VSMOW&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;D47raw&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;D48raw&#39;]:.6f}&#34;,
                                f&#34;{r[&#39;D49raw&#39;]:.6f}&#34;,
                                f&#34;{r[f&#39;D{self._4x}&#39;]:.6f}&#34;
                                ]
                if save_to_file:
                        if not os.path.exists(dir):
                                os.makedirs(dir)
                        if filename is None:
                                filename = f&#39;D{self._4x}_analyses.csv&#39;
                        with open(f&#39;{dir}/{filename}&#39;, &#39;w&#39;) as fid:
                                fid.write(make_csv(out))
                if print_out:
                        self.msg(&#39;\n&#39; + pretty_table(out))
                return out


        @make_verbal
        def table_of_samples(
                self,
                dir = &#39;output&#39;,
                filename = None,
                save_to_file = True,
                print_out = True,
                ):
                &#39;&#39;&#39;
                Print out an/or save to disk a table of samples.

                __Parameters__

                + `dir`: the directory in which to save the table
                + `filename`: the name to the csv file to write to
                + `save_to_file`: whether to save the table to disk
                + `print_out`: whether to print out the table
                &#39;&#39;&#39;

                out = [[&#39;Sample&#39;,&#39;N&#39;,&#39;d13C_VPDB&#39;,&#39;d18O_VSMOW&#39;,f&#39;D{self._4x}&#39;,&#39;SE&#39;,&#39;95% CL&#39;,&#39;SD&#39;,&#39;p_Levene&#39;]]
                for sample in self.anchors:
                        out += [[
                                f&#34;{sample}&#34;,
                                f&#34;{self.samples[sample][&#39;N&#39;]}&#34;,
                                f&#34;{self.samples[sample][&#39;d13C_VPDB&#39;]:.2f}&#34;,
                                f&#34;{self.samples[sample][&#39;d18O_VSMOW&#39;]:.2f}&#34;,
                                f&#34;{self.samples[sample][f&#39;D{self._4x}&#39;]:.4f}&#34;,&#39;&#39;,&#39;&#39;,
                                f&#34;{self.samples[sample][f&#39;SD_D{self._4x}&#39;]:.4f}&#34; if self.samples[sample][&#39;N&#39;] &gt; 1 else &#39;&#39;, &#39;&#39;
                                ]]
                for sample in self.unknowns:
                        out += [[
                                f&#34;{sample}&#34;,
                                f&#34;{self.samples[sample][&#39;N&#39;]}&#34;,
                                f&#34;{self.samples[sample][&#39;d13C_VPDB&#39;]:.2f}&#34;,
                                f&#34;{self.samples[sample][&#39;d18O_VSMOW&#39;]:.2f}&#34;,
                                f&#34;{self.samples[sample][f&#39;D{self._4x}&#39;]:.4f}&#34;,
                                f&#34;{self.samples[sample][f&#39;SE_D{self._4x}&#39;]:.4f}&#34;,
                                f&#34;± {self.samples[sample][f&#39;SE_D{self._4x}&#39;] * self.t95:.4f}&#34;,
                                f&#34;{self.samples[sample][f&#39;SD_D{self._4x}&#39;]:.4f}&#34; if self.samples[sample][&#39;N&#39;] &gt; 1 else &#39;&#39;,
                                f&#34;{self.samples[sample][&#39;p_Levene&#39;]:.3f}&#34; if self.samples[sample][&#39;N&#39;] &gt; 2 else &#39;&#39;
                                ]]
                if save_to_file:
                        if not os.path.exists(dir):
                                os.makedirs(dir)
                        if filename is None:
                                filename = f&#39;D{self._4x}_samples.csv&#39;
                        with open(f&#39;{dir}/{filename}&#39;, &#39;w&#39;) as fid:
                                fid.write(make_csv(out))
                if print_out:
                        self.msg(&#39;\n&#39;+pretty_table(out))


        def plot_sessions(self, dir = &#39;output&#39;, figsize = (8,8)):
                &#39;&#39;&#39;
                Generate session plots and save them to disk.

                __Parameters__

                + `dir`: the directory in which to save the plots
                + `figsize`: the width and height (in inches) of each plot
                &#39;&#39;&#39;
                if not os.path.exists(dir):
                        os.makedirs(dir)

                for session in self.sessions:
                        sp = self.plot_single_session(session, xylimits = &#39;constant&#39;)
                        ppl.savefig(f&#39;{dir}/D{self._4x}_plot_{session}.pdf&#39;)
                        ppl.close(sp.fig)


        @make_verbal
        def consolidate_samples(self):
                &#39;&#39;&#39;
                Compile various statistics for each sample.

                For each anchor sample:

                + `D47`: the nominal Δ&lt;sub&gt;47&lt;/sub&gt; value for this anchor, specified by `self.Nominal_D47`
                + `SE_D47`: set to zero by definition

                For each unknown sample:

                + `D47`: the standardized Δ&lt;sub&gt;47&lt;/sub&gt; value for this unknown
                + `SE_D47`: the standard error of Δ&lt;sub&gt;47&lt;/sub&gt; for this unknown

                For each anchor and unknown:

                + `N`: the total number of analyses of this sample
                + `SD_D47`: the “sample” (in the statistical sense) standard deviation for this sample
                + `d13C_VPDB`: the average δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; value for this sample
                + `d18O_VSMOW`: the average δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt; value for this sample (as CO&lt;sub&gt;2&lt;/sub&gt;)
                + `p_Levene`: the p-value from a [Levene test] of equal variance, indicating whether
                the Δ&lt;sub&gt;47&lt;/sub&gt; repeatability this sample differs significantly from that observed
                for the reference sample specified by `self.LEVENE_REF_SAMPLE`.

                [Levene test]: https://en.wikipedia.org/wiki/Levene%27s_test
                &#39;&#39;&#39;
                D4x_ref_pop = [r[f&#39;D{self._4x}&#39;] for r in self.samples[self.LEVENE_REF_SAMPLE][&#39;data&#39;]]
                for sample in self.samples:
                        self.samples[sample][&#39;N&#39;] = len(self.samples[sample][&#39;data&#39;])
                        if self.samples[sample][&#39;N&#39;] &gt; 1:
                                self.samples[sample][f&#39;SD_D{self._4x}&#39;] = stdev([r[f&#39;D{self._4x}&#39;] for r in self.samples[sample][&#39;data&#39;]])

                        self.samples[sample][&#39;d13C_VPDB&#39;] = np.mean([r[&#39;d13C_VPDB&#39;] for r in self.samples[sample][&#39;data&#39;]])
                        self.samples[sample][&#39;d18O_VSMOW&#39;] = np.mean([r[&#39;d18O_VSMOW&#39;] for r in self.samples[sample][&#39;data&#39;]])

                        D4x_pop = [r[f&#39;D{self._4x}&#39;] for r in self.samples[sample][&#39;data&#39;]]
                        if len(D4x_pop) &gt; 2:
                                self.samples[sample][&#39;p_Levene&#39;] = levene(D4x_ref_pop, D4x_pop, center = &#39;median&#39;)[1]

                if self.standardization_method == &#39;pooled&#39;:
                        for sample in self.anchors:
                                self.samples[sample][f&#39;D{self._4x}&#39;] = self.Nominal_D4x[sample]
                                self.samples[sample][f&#39;SE_D{self._4x}&#39;] = 0.
                        for sample in self.unknowns:
                                self.samples[sample][f&#39;D{self._4x}&#39;] = self.standardization.params.valuesdict()[f&#39;D{self._4x}_{pf(sample)}&#39;]
                                try:
                                        self.samples[sample][f&#39;SE_D{self._4x}&#39;] = self.sample_D4x_covar(sample)**.5
                                except ValueError:
                                        # when `sample` is constrained by self.standardize(constraints = {...}),
                                        # it is no longer listed in self.standardization.var_names.
                                        # Temporary fix: define SE as zero for now
                                        self.samples[sample][f&#39;SE_D4{self._4x}&#39;] = 0.

                elif self.standardization_method == &#39;indep_sessions&#39;:
                        for sample in self.anchors:
                                self.samples[sample][f&#39;D{self._4x}&#39;] = self.Nominal_D4x[sample]
                                self.samples[sample][f&#39;SE_D{self._4x}&#39;] = 0.
                        for sample in self.unknowns:
                                self.msg(f&#39;Consolidating sample {sample}&#39;)
                                self.unknowns[sample][f&#39;session_D{self._4x}&#39;] = {}
                                session_avg = []
                                for session in self.sessions:
                                        sdata = [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]
                                        if sdata:
                                                self.msg(f&#39;{sample} found in session {session}&#39;)
                                                avg_D4x = np.mean([r[f&#39;D{self._4x}&#39;] for r in sdata])
                                                avg_d4x = np.mean([r[f&#39;d{self._4x}&#39;] for r in sdata])
                                                # !! TODO: sigma_s below does not account for temporal changes in standardization error
                                                sigma_s = self.standardization_error(session, avg_d4x, avg_D4x)
                                                sigma_u = sdata[0][f&#39;wD{self._4x}raw&#39;] / self.sessions[session][&#39;a&#39;] / len(sdata)**.5
                                                session_avg.append([avg_D4x, (sigma_u**2 + sigma_s**2)**.5])
                                                self.unknowns[sample][f&#39;session_D{self._4x}&#39;][session] = session_avg[-1]
                                self.samples[sample][f&#39;D{self._4x}&#39;], self.samples[sample][f&#39;SE_D{self._4x}&#39;] = w_avg(*zip(*session_avg))
                                weights = {s: self.unknowns[sample][f&#39;session_D{self._4x}&#39;][s][1]**-2 for s in self.unknowns[sample][f&#39;session_D{self._4x}&#39;]}
                                wsum = sum([weights[s] for s in weights])
                                for s in weights:
                                        self.unknowns[sample][f&#39;session_D{self._4x}&#39;][s] += [self.unknowns[sample][f&#39;session_D{self._4x}&#39;][s][1]**-2 / wsum]


        def consolidate_sessions(self):
                &#39;&#39;&#39;
                Compile various statistics for each session.

                + `Na`: Number of anchor analyses in the session
                + `Nu`: Number of unknown analyses in the session
                + `r_d13C_VPDB`: δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; repeatability of analyses within the session
                + `r_d18O_VSMOW`: δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt; repeatability of analyses within the session
                + `r_D47`: Δ&lt;sub&gt;47&lt;/sub&gt; repeatability of analyses within the session
                + `a`: scrambling factor
                + `b`: compositional slope
                + `c`: WG offset
                + `SE_a`: Model stadard erorr of `a`
                + `SE_b`: Model stadard erorr of `b`
                + `SE_c`: Model stadard erorr of `c`
                + `scrambling_drift` (boolean): whether to allow a temporal drift in the scrambling factor (`a`)
                + `slope_drift` (boolean): whether to allow a temporal drift in the compositional slope (`b`)
                + `wg_drift` (boolean): whether to allow a temporal drift in the WG offset (`c`)
                + `a2`: scrambling factor drift
                + `b2`: compositional slope drift
                + `c2`: WG offset drift
                + `Np`: Number of standardization parameters to fit
                + `CM`: model covariance matrix for (`a`, `b`, `c`, `a2`, `b2`, `c2`)
                + `d13Cwg_VPDB`: δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; of WG
                + `d18Owg_VSMOW`: δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt; of WG
                &#39;&#39;&#39;
                for session in self.sessions:
                        if &#39;d13Cwg_VPDB&#39; not in self.sessions[session]:
                                self.sessions[session][&#39;d13Cwg_VPDB&#39;] = self.sessions[session][&#39;data&#39;][0][&#39;d13Cwg_VPDB&#39;]
                        if &#39;d18Owg_VSMOW&#39; not in self.sessions[session]:
                                self.sessions[session][&#39;d18Owg_VSMOW&#39;] = self.sessions[session][&#39;data&#39;][0][&#39;d18Owg_VSMOW&#39;]
                        self.sessions[session][&#39;Na&#39;] = len([r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.anchors])
                        self.sessions[session][&#39;Nu&#39;] = len([r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.unknowns])

                        self.msg(f&#39;Computing repeatabilities for session {session}&#39;)
                        self.sessions[session][&#39;r_d13C_VPDB&#39;] = self.compute_r(&#39;d13C_VPDB&#39;, samples = &#39;anchors&#39;, sessions = [session])
                        self.sessions[session][&#39;r_d18O_VSMOW&#39;] = self.compute_r(&#39;d18O_VSMOW&#39;, samples = &#39;anchors&#39;, sessions = [session])
                        self.sessions[session][f&#39;r_D{self._4x}&#39;] = self.compute_r(f&#39;D{self._4x}&#39;, sessions = [session])

                if self.standardization_method == &#39;pooled&#39;:
                        for session in self.sessions:

                                self.sessions[session][&#39;a&#39;] = self.standardization.params.valuesdict()[f&#39;a_{pf(session)}&#39;]
                                i = self.standardization.var_names.index(f&#39;a_{pf(session)}&#39;)
                                self.sessions[session][&#39;SE_a&#39;] = self.standardization.covar[i,i]**.5

                                self.sessions[session][&#39;b&#39;] = self.standardization.params.valuesdict()[f&#39;b_{pf(session)}&#39;]
                                i = self.standardization.var_names.index(f&#39;b_{pf(session)}&#39;)
                                self.sessions[session][&#39;SE_b&#39;] = self.standardization.covar[i,i]**.5

                                self.sessions[session][&#39;c&#39;] = self.standardization.params.valuesdict()[f&#39;c_{pf(session)}&#39;]
                                i = self.standardization.var_names.index(f&#39;c_{pf(session)}&#39;)
                                self.sessions[session][&#39;SE_c&#39;] = self.standardization.covar[i,i]**.5

                                self.sessions[session][&#39;a2&#39;] = self.standardization.params.valuesdict()[f&#39;a2_{pf(session)}&#39;]
                                if self.sessions[session][&#39;scrambling_drift&#39;]:
                                        i = self.standardization.var_names.index(f&#39;a2_{pf(session)}&#39;)
                                        self.sessions[session][&#39;SE_a2&#39;] = self.standardization.covar[i,i]**.5
                                else:
                                        self.sessions[session][&#39;SE_a2&#39;] = 0.

                                self.sessions[session][&#39;b2&#39;] = self.standardization.params.valuesdict()[f&#39;b2_{pf(session)}&#39;]
                                if self.sessions[session][&#39;slope_drift&#39;]:
                                        i = self.standardization.var_names.index(f&#39;b2_{pf(session)}&#39;)
                                        self.sessions[session][&#39;SE_b2&#39;] = self.standardization.covar[i,i]**.5
                                else:
                                        self.sessions[session][&#39;SE_b2&#39;] = 0.

                                self.sessions[session][&#39;c2&#39;] = self.standardization.params.valuesdict()[f&#39;c2_{pf(session)}&#39;]
                                if self.sessions[session][&#39;wg_drift&#39;]:
                                        i = self.standardization.var_names.index(f&#39;c2_{pf(session)}&#39;)
                                        self.sessions[session][&#39;SE_c2&#39;] = self.standardization.covar[i,i]**.5
                                else:
                                        self.sessions[session][&#39;SE_c2&#39;] = 0.

                                i = self.standardization.var_names.index(f&#39;a_{pf(session)}&#39;)
                                j = self.standardization.var_names.index(f&#39;b_{pf(session)}&#39;)
                                k = self.standardization.var_names.index(f&#39;c_{pf(session)}&#39;)
                                CM = np.zeros((6,6))
                                CM[:3,:3] = self.standardization.covar[[i,j,k],:][:,[i,j,k]]
                                try:
                                        i2 = self.standardization.var_names.index(f&#39;a2_{pf(session)}&#39;)
                                        CM[3,[0,1,2,3]] = self.standardization.covar[i2,[i,j,k,i2]]
                                        CM[[0,1,2,3],3] = self.standardization.covar[[i,j,k,i2],i2]
                                        try:
                                                j2 = self.standardization.var_names.index(f&#39;b2_{pf(session)}&#39;)
                                                CM[3,4] = self.standardization.covar[i2,j2]
                                                CM[4,3] = self.standardization.covar[j2,i2]
                                        except ValueError:
                                                pass
                                        try:
                                                k2 = self.standardization.var_names.index(f&#39;c2_{pf(session)}&#39;)
                                                CM[3,5] = self.standardization.covar[i2,k2]
                                                CM[5,3] = self.standardization.covar[k2,i2]
                                        except ValueError:
                                                pass
                                except ValueError:
                                        pass
                                try:
                                        j2 = self.standardization.var_names.index(f&#39;b2_{pf(session)}&#39;)
                                        CM[4,[0,1,2,4]] = self.standardization.covar[j2,[i,j,k,j2]]
                                        CM[[0,1,2,4],4] = self.standardization.covar[[i,j,k,j2],j2]
                                        try:
                                                k2 = self.standardization.var_names.index(f&#39;c2_{pf(session)}&#39;)
                                                CM[4,5] = self.standardization.covar[j2,k2]
                                                CM[5,4] = self.standardization.covar[k2,j2]
                                        except ValueError:
                                                pass
                                except ValueError:
                                        pass
                                try:
                                        k2 = self.standardization.var_names.index(f&#39;c2_{pf(session)}&#39;)
                                        CM[5,[0,1,2,5]] = self.standardization.covar[k2,[i,j,k,k2]]
                                        CM[[0,1,2,5],5] = self.standardization.covar[[i,j,k,k2],k2]
                                except ValueError:
                                        pass

                                self.sessions[session][&#39;CM&#39;] = CM

                elif self.standardization_method == &#39;indep_sessions&#39;:
                        pass # Not implemented yet


        @make_verbal
        def repeatabilities(self):
                &#39;&#39;&#39;
                Compute analytical repeatabilities for δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt;,
                δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt;, Δ&lt;sub&gt;47&lt;/sub&gt; (for all samples, for anchors,
                and for unknowns).
                &#39;&#39;&#39;
                self.msg(&#39;Computing reproducibilities for all sessions&#39;)

                self.repeatability[&#39;r_d13C_VPDB&#39;] = self.compute_r(&#39;d13C_VPDB&#39;, samples = &#39;anchors&#39;)
                self.repeatability[&#39;r_d18O_VSMOW&#39;] = self.compute_r(&#39;d18O_VSMOW&#39;, samples = &#39;anchors&#39;)
                self.repeatability[f&#39;r_D{self._4x}a&#39;] = self.compute_r(f&#39;D{self._4x}&#39;, samples = &#39;anchors&#39;)
                self.repeatability[f&#39;r_D{self._4x}u&#39;] = self.compute_r(f&#39;D{self._4x}&#39;, samples = &#39;unknowns&#39;)
                self.repeatability[f&#39;r_D{self._4x}&#39;] = self.compute_r(f&#39;D{self._4x}&#39;, samples = &#39;all samples&#39;)


        @make_verbal
        def consolidate(self, tables = True, plots = True):
                &#39;&#39;&#39;
                Collect information about samples, sessions and repeatabilities.
                &#39;&#39;&#39;
                self.consolidate_samples()
                self.consolidate_sessions()
                self.repeatabilities()

                if tables:
                        self.summary()
                        self.table_of_sessions()
                        self.table_of_analyses()
                        self.table_of_samples()

                if plots:
                        self.plot_sessions()


        @make_verbal
        def rmswd(self,
                samples = &#39;all samples&#39;,
                sessions = &#39;all sessions&#39;,
                ):
                &#39;&#39;&#39;
                Compute the root mean squared weighted deviation, χ&lt;sup&gt;2&lt;/sup&gt; and
                corresponding degrees of freedom of `[r[&#39;D47&#39;] for r in self]`.
                
                Currently used only in `D4xdata.standardize(method = &#39;indep_sessions&#39;)`
                &#39;&#39;&#39;
                if samples == &#39;all samples&#39;:
                        mysamples = [k for k in self.samples]
                elif samples == &#39;anchors&#39;:
                        mysamples = [k for k in self.anchors]
                elif samples == &#39;unknowns&#39;:
                        mysamples = [k for k in self.unknowns]
                else:
                        mysamples = samples

                if sessions == &#39;all sessions&#39;:
                        sessions = [k for k in self.sessions]

                chisq, Nf = 0, 0
                for sample in mysamples :
                        G = [ r for r in self if r[&#39;Sample&#39;] == sample and r[&#39;Session&#39;] in sessions ]
                        if len(G) &gt; 1 :
                                X, sX = w_avg([r[f&#39;D{self._4x}&#39;] for r in G], [r[f&#39;wD{self._4x}&#39;] for r in G])
                                Nf += (len(G) - 1)
                                chisq += np.sum([ ((r[f&#39;D{self._4x}&#39;]-X)/r[f&#39;wD{self._4x}&#39;])**2 for r in G])
                r = (chisq / Nf)**.5 if Nf &gt; 0 else 0
                self.msg(f&#39;RMSWD of r[&#34;D{self._4x}&#34;] is {r:.6f} for {samples}.&#39;)
                return {&#39;rmswd&#39;: r, &#39;chisq&#39;: chisq, &#39;Nf&#39;: Nf}

        
        @make_verbal
        def compute_r(self, key, samples = &#39;all samples&#39;, sessions = &#39;all sessions&#39;):
                &#39;&#39;&#39;
                Compute the repeatability of `[r[key] for r in self]`
                &#39;&#39;&#39;
                # NB: it&#39;s debatable whether rD47 should be computed
                # with Nf = len(self)-len(self.samples) instead of
                # Nf = len(self) - len(self.unknwons) - 3*len(self.sessions)

                if samples == &#39;all samples&#39;:
                        mysamples = [k for k in self.samples]
                elif samples == &#39;anchors&#39;:
                        mysamples = [k for k in self.anchors]
                elif samples == &#39;unknowns&#39;:
                        mysamples = [k for k in self.unknowns]
                else:
                        mysamples = samples

                if sessions == &#39;all sessions&#39;:
                        sessions = [k for k in self.sessions]

                if key in [&#39;D47&#39;, &#39;D48&#39;]:
                        chisq, Nf = 0, 0
                        for sample in mysamples :
                                X = [ r[key] for r in self if r[&#39;Sample&#39;] == sample and r[&#39;Session&#39;] in sessions ]
                                if len(X) &gt; 1 :
                                        chisq += np.sum([ (x-self.samples[sample][key])**2 for x in X ])
                                        if sample in self.unknowns:
                                                Nf += len(X) - 1
                                        else:
                                                Nf += len(X)
                        if samples in [&#39;anchors&#39;, &#39;all samples&#39;]:
                                Nf -= sum([self.sessions[s][&#39;Np&#39;] for s in sessions])
                        r = (chisq / Nf)**.5 if Nf &gt; 0 else 0

                else: # if key not in [&#39;D47&#39;, &#39;D48&#39;]
                        chisq, Nf = 0, 0
                        for sample in mysamples :
                                X = [ r[key] for r in self if r[&#39;Sample&#39;] == sample and r[&#39;Session&#39;] in sessions ]
                                if len(X) &gt; 1 :
                                        Nf += len(X) - 1
                                        chisq += np.sum([ (x-np.mean(X))**2 for x in X ])
                        r = (chisq / Nf)**.5 if Nf &gt; 0 else 0

                self.msg(f&#39;Repeatability of r[&#34;{key}&#34;] is {1000*r:.1f} ppm for {samples}.&#39;)
                return r

        def sample_average(self, samples, weights = &#39;equal&#39;, normalize = True):
                &#39;&#39;&#39;
                Weighted average Δ&lt;sub&gt;47&lt;/sub&gt; value of a group of samples, accounting for covariance.

                Returns the weighed average Δ47 value and associated SE
                of a group of samples. Weights are equal by default. If `normalize` is
                true, `weights` will be rescaled so that their sum equals 1.

                __Examples__

                ```python
                self.sample_average([&#39;X&#39;,&#39;Y&#39;], [1, 2])
                ```

                returns the value and SE of [Δ&lt;sub&gt;47&lt;/sub&gt;(X) + 2 Δ&lt;sub&gt;47&lt;/sub&gt;(Y)]/3,
                where Δ&lt;sub&gt;47&lt;/sub&gt;(X) and Δ&lt;sub&gt;47&lt;/sub&gt;(Y) are the average Δ&lt;sub&gt;47&lt;/sub&gt;
                values of samples X and Y, respectively.

                ```python
                self.sample_average([&#39;X&#39;,&#39;Y&#39;], [1, -1], normalize = False)
                ```

                returns the value and SE of the difference Δ&lt;sub&gt;47&lt;/sub&gt;(X) - Δ&lt;sub&gt;47&lt;/sub&gt;(Y).
                &#39;&#39;&#39;
                if weights == &#39;equal&#39;:
                        weights = [1/len(samples)] * len(samples)

                if normalize:
                        s = sum(weights)
                        if s:
                                weights = [w/s for w in weights]

                try:
#                       indices = [self.standardization.var_names.index(f&#39;D47_{pf(sample)}&#39;) for sample in samples]
#                       C = self.standardization.covar[indices,:][:,indices]
                        C = np.array([[self.sample_D4x_covar(x, y) for x in samples] for y in samples])
                        X = [self.samples[sample][f&#39;D{self._4x}&#39;] for sample in samples]
                        return correlated_sum(X, C, weights)
                except ValueError:
                        return (0., 0.)


        def sample_D4x_covar(self, sample1, sample2 = None):
                &#39;&#39;&#39;
                Covariance between Δ&lt;sub&gt;47&lt;/sub&gt; values of samples

                Returns the error covariance between the average Δ&lt;sub&gt;47&lt;/sub&gt; values of two
                samples. If if only `sample_1` is specified, or if `sample_1 == sample_2`),
                returns the Δ&lt;sub&gt;47&lt;/sub&gt; variance for that sample.
                &#39;&#39;&#39;
                if sample2 is None:
                        sample2 = sample1
                if self.standardization_method == &#39;pooled&#39;:
                        i = self.standardization.var_names.index(f&#39;D{self._4x}_{pf(sample1)}&#39;)
                        j = self.standardization.var_names.index(f&#39;D{self._4x}_{pf(sample2)}&#39;)
                        return self.standardization.covar[i, j]
                elif self.standardization_method == &#39;indep_sessions&#39;:
                        if sample1 == sample2:
                                return self.samples[sample1][f&#39;SE_D{self._4x}&#39;]**2
                        else:
                                c = 0
                                for session in self.sessions:
                                        sdata1 = [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample1]
                                        sdata2 = [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample2]
                                        if sdata1 and sdata2:
                                                a = self.sessions[session][&#39;a&#39;]
                                                # !! TODO: CM below does not account for temporal changes in standardization parameters
                                                CM = self.sessions[session][&#39;CM&#39;][:3,:3]
                                                avg_D4x_1 = np.mean([r[f&#39;D{self._4x}&#39;] for r in sdata1])
                                                avg_d4x_1 = np.mean([r[f&#39;d{self._4x}&#39;] for r in sdata1])
                                                avg_D4x_2 = np.mean([r[f&#39;D{self._4x}&#39;] for r in sdata2])
                                                avg_d4x_2 = np.mean([r[f&#39;d{self._4x}&#39;] for r in sdata2])
                                                c += (
                                                        self.unknowns[sample1][f&#39;session_D{self._4x}&#39;][session][2]
                                                        * self.unknowns[sample2][f&#39;session_D{self._4x}&#39;][session][2]
                                                        * np.array([[avg_D4x_1, avg_d4x_1, 1]])
                                                        @ CM
                                                        @ np.array([[avg_D4x_2, avg_d4x_2, 1]]).T
                                                        ) / a**2
                                return float(c)

        def sample_D4x_correl(self, sample1, sample2 = None):
                &#39;&#39;&#39;
                Correlation between Δ&lt;sub&gt;47&lt;/sub&gt; errors of samples

                Returns the error correlation between the average Δ47 values of two samples.
                &#39;&#39;&#39;
                if sample2 is None or sample2 == sample1:
                        return 1.
                return (
                        self.sample_D4x_covar(sample1, sample2)
                        / self.unknowns[sample1][f&#39;SE_D{self._4x}&#39;]
                        / self.unknowns[sample2][f&#39;SE_D{self._4x}&#39;]
                        )

        def plot_single_session(self,
                session,
                kw_plot_anchors = dict(ls=&#39;None&#39;, marker=&#39;x&#39;, mec=(.75, 0, 0), mew = .75, ms = 4),
                kw_plot_unknowns = dict(ls=&#39;None&#39;, marker=&#39;x&#39;, mec=(0, 0, .75), mew = .75, ms = 4),
                kw_plot_anchor_avg = dict(ls=&#39;-&#39;, marker=&#39;None&#39;, color=(.75, 0, 0), lw = .75),
                kw_plot_unknown_avg = dict(ls=&#39;-&#39;, marker=&#39;None&#39;, color=(0, 0, .75), lw = .75),
                kw_contour_error = dict(colors = [[0, 0, 0]], alpha = .5, linewidths = 0.75),
                xylimits = &#39;free&#39;, # | &#39;constant&#39;
                x_label = None,
                y_label = None,
                error_contour_interval = &#39;auto&#39;,
                fig = &#39;new&#39;,
                ):
                &#39;&#39;&#39;
                Generate plot for a single session
                &#39;&#39;&#39;
                if x_label is None:
                        x_label = f&#39;δ$_{{{self._4x}}}$ (‰)&#39;,
                if y_label is None:
                        y_label = f&#39;Δ$_{{{self._4x}}}$ (‰)&#39;,

                out = SessionPlot()
                anchors = [a for a in self.anchors if [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == a]]
                unknowns = [u for u in self.unknowns if [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == u]]
                
                if fig == &#39;new&#39;:
                        out.fig = ppl.figure(figsize = (6,6))
                        ppl.subplots_adjust(.1,.1,.9,.9)

                out.anchor_analyses, = ppl.plot(
                        [r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.anchors],
                        [r[f&#39;D{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.anchors],
                        **kw_plot_anchors)
                out.unknown_analyses, = ppl.plot(
                        [r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.unknowns],
                        [r[f&#39;D{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.unknowns],
                        **kw_plot_unknowns)
                out.anchor_avg = ppl.plot(
                        np.array([ np.array([
                                np.min([r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]) - 1,
                                np.max([r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]) + 1
                                ]) for sample in anchors]).T,
                        np.array([ np.array([0, 0]) + self.Nominal_D4x[sample] for sample in anchors]).T,
                        &#39;-&#39;, **kw_plot_anchor_avg)
                out.unknown_avg = ppl.plot(
                        np.array([ np.array([
                                np.min([r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]) - 1,
                                np.max([r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]) + 1
                                ]) for sample in unknowns]).T,
                        np.array([ np.array([0, 0]) + self.unknowns[sample][f&#39;D{self._4x}&#39;] for sample in unknowns]).T,
                        &#39;-&#39;, **kw_plot_unknown_avg)
                if xylimits == &#39;constant&#39;:
                        x = [r[f&#39;d{self._4x}&#39;] for r in self]
                        y = [r[f&#39;D{self._4x}&#39;] for r in self]
                        x1, x2, y1, y2 = np.min(x), np.max(x), np.min(y), np.max(y)
                        w, h = x2-x1, y2-y1
                        x1 -= w/20
                        x2 += w/20
                        y1 -= h/20
                        y2 += h/20
                        ppl.axis([x1, x2, y1, y2])
                elif xylimits == &#39;free&#39;:
                        x1, x2, y1, y2 = ppl.axis()
                else:
                        x1, x2, y1, y2 = ppl.axis(xylimits)
                                
                if error_contour_interval != &#39;none&#39;:
                        xi, yi = np.linspace(x1, x2), np.linspace(y1, y2)
                        XI,YI = np.meshgrid(xi, yi)
                        SI = np.array([[self.standardization_error(session, x, y) for x in xi] for y in yi])
                        if error_contour_interval == &#39;auto&#39;:
                                rng = np.max(SI) - np.min(SI)
                                if rng &lt;= 0.01:
                                        cinterval = 0.001
                                elif rng &lt;= 0.03:
                                        cinterval = 0.004
                                elif rng &lt;= 0.1:
                                        cinterval = 0.01
                                elif rng &lt;= 0.3:
                                        cinterval = 0.03
                                elif rng &lt;= 1.:
                                        cinterval = 0.1
                                else:
                                        cinterval = 0.5
                        else:
                                cinterval = error_contour_interval

                        cval = np.arange(np.ceil(SI.min() / .001) * .001, np.ceil(SI.max() / .001 + 1) * .001, cinterval)
                        out.contour = ppl.contour(XI, YI, SI, cval, **kw_contour_error)
                        out.clabel = ppl.clabel(out.contour)

                ppl.xlabel(x_label)
                ppl.ylabel(y_label)
                ppl.title(session, weight = &#39;bold&#39;)
                ppl.grid(alpha = .2)
                out.ax = ppl.gca()              

                return out

        def plot_residuals(self, dir = &#39;output&#39;, filename = None, highlight = [], colors = None):
                &#39;&#39;&#39;
                Plot residuals of each analysis as a function of time (actually, as a function of
                the order of analyses in the D47data() object)

                + `dir`: the directory in which to save the plot
                + `highlight`: a list of samples to highlight
                + `colors`: a dict of {&lt;sample&gt;: &lt;color&gt;} for all samples
                &#39;&#39;&#39;
                fig = ppl.figure(figsize = (8,4))
                ppl.subplots_adjust(.1,.05,.78,.8)
                N = len(self.anchors)
                if colors is None:
                        if len(highlight) &gt; 0:
                                Nh = len(highlight)
                                if Nh == 1:
                                        colors = {highlight[0]: (0,0,0)}
                                elif Nh == 3:
                                        colors = {a: c for a,c in zip(highlight, [(0,0,1), (1,0,0), (0,2/3,0)])}
                                elif Nh == 4:
                                        colors = {a: c for a,c in zip(highlight, [(0,0,1), (1,0,0), (0,2/3,0), (.75,0,.75)])}
                                else:
                                        colors = {a: hls_to_rgb(k/Nh, .4, 1) for k,a in enumerate(highlight)}
                        else:
                                if N == 3:
                                        colors = {a: c for a,c in zip(self.anchors, [(0,0,1), (1,0,0), (0,2/3,0)])}
                                elif N == 4:
                                        colors = {a: c for a,c in zip(self.anchors, [(0,0,1), (1,0,0), (0,2/3,0), (.75,0,.75)])}
                                else:
                                        colors = {a: hls_to_rgb(k/N, .4, 1) for k,a in enumerate(self.anchors)}
                session = self[0][&#39;Session&#39;]
                x1 = 0
#               ymax = np.max([1e3 * (r[&#39;D47&#39;] - self.samples[r[&#39;Sample&#39;]][&#39;D47&#39;]) for r in self])
                x_sessions = {}
                one_or_more_singlets = False
                one_or_more_multiplets = False
                for k,r in enumerate(self):
                        if r[&#39;Session&#39;] != session:
                                x2 = k-1
                                x_sessions[session] = (x1+x2)/2
                                ppl.axvline(k - 0.5, color = &#39;k&#39;, lw = .5)
                                session = r[&#39;Session&#39;]
                                x1 = k
                        singlet = len(self.samples[r[&#39;Sample&#39;]][&#39;data&#39;]) == 1
                        if r[&#39;Sample&#39;] in self.unknowns:
                                if singlet:
                                        one_or_more_singlets = True
                                else:
                                        one_or_more_multiplets = True
                        kw = dict(
                                marker = &#39;x&#39; if singlet else &#39;+&#39;,
                                ms = 4 if singlet else 5,
                                ls = &#39;None&#39;,
                                mec = colors[r[&#39;Sample&#39;]] if r[&#39;Sample&#39;] in colors else (0,0,0),
                                mew = 1,
                                alpha = 0.2 if singlet else 1,
                                )
                        if highlight and r[&#39;Sample&#39;] not in highlight:
                                kw[&#39;alpha&#39;] = 0.2
                        ppl.plot(k, 1e3 * (r[&#39;D47&#39;] - self.samples[r[&#39;Sample&#39;]][&#39;D47&#39;]), **kw)
                x2 = k
                x_sessions[session] = (x1+x2)/2

                ppl.axhspan(-self.repeatability[&#39;r_D47&#39;]*1000, self.repeatability[&#39;r_D47&#39;]*1000, color = &#39;k&#39;, alpha = .05, lw = 1)
                ppl.text(len(self), self.repeatability[&#39;r_D47&#39;]*1000, f&#34;   SD = {self.repeatability[&#39;r_D47&#39;]*1000:.1f} ppm&#34;, size = 9, alpha = .75, va = &#39;center&#39;)
                ppl.axhspan(-self.repeatability[&#39;r_D47&#39;]*1000*self.t95, self.repeatability[&#39;r_D47&#39;]*1000*self.t95, color = &#39;k&#39;, alpha = .05, lw = 1)
                ppl.text(len(self), self.repeatability[&#39;r_D47&#39;]*1000*self.t95, f&#34;   95% CL: ± {self.repeatability[&#39;r_D47&#39;]*1000*self.t95:.1f} ppm&#34;, size = 9, alpha = .75, va = &#39;center&#39;)

                ymax = ppl.axis()[3]
                for s in x_sessions:
                        ppl.text(
                                x_sessions[s],
                                ymax +1,
                                s,
                                va = &#39;bottom&#39;,
                                **(
                                        dict(ha = &#39;center&#39;)
                                        if len(self.sessions[s][&#39;data&#39;]) &gt; (0.15 * len(self))
                                        else dict(ha = &#39;left&#39;, rotation = 45)
                                        )
                                )

                for s in colors:
                        kw[&#39;marker&#39;] = &#39;+&#39;
                        kw[&#39;ms&#39;] = 5
                        kw[&#39;mec&#39;] = colors[s]
                        kw[&#39;label&#39;] = s
                        kw[&#39;alpha&#39;] = 1
                        ppl.plot([], [], **kw)

                kw[&#39;mec&#39;] = (0,0,0)

                if one_or_more_singlets:
                        kw[&#39;marker&#39;] = &#39;x&#39;
                        kw[&#39;ms&#39;] = 4
                        kw[&#39;alpha&#39;] = .2
                        kw[&#39;label&#39;] = &#39;other (N$\\,$=$\\,$1)&#39; if one_or_more_multiplets else &#39;other&#39;
                        ppl.plot([], [], **kw)

                if one_or_more_multiplets:
                        kw[&#39;marker&#39;] = &#39;+&#39;
                        kw[&#39;ms&#39;] = 4
                        kw[&#39;alpha&#39;] = 1
                        kw[&#39;label&#39;] = &#39;other (N$\\,$&gt;$\\,$1)&#39; if one_or_more_singlets else &#39;other&#39;
                        ppl.plot([], [], **kw)

                ppl.legend(loc = &#39;lower left&#39;, bbox_to_anchor = (1.03, 0), borderaxespad = 0)
                ppl.xticks([])
                ppl.ylabel(&#39;Δ$_{47}$ residuals (ppm)&#39;)
                ppl.axis([-1, len(self), None, None])

                if not os.path.exists(dir):
                        os.makedirs(dir)
                if filename is None:
                        return fig
                elif filename == &#39;&#39;:
                        filename = f&#39;D{self._4x}_residuals.pdf&#39;
                ppl.savefig(f&#39;{dir}/{filename}&#39;)
                ppl.close(fig)

        def simulate(self,
                samples = None,
                a = 1.,
                b = 0.,
                c = -0.9,
                rD4x = 0.015,
                seed = 0,
                ):
                &#39;&#39;&#39;
                Populate `D47data` instance with simulated analyses from a single session.
                
                __Parameters__

                + `samples`: a list of entries; each entry is a dictionary with the following fields:
                    * `Sample`: the name of the sample
                    * either `d47` (the δ&lt;sub&gt;47&lt;/sub&gt; value of this sample), or `d13C_VPDB` and `d18O_VPDB` (its δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; and δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VPDB&lt;/sub&gt; values)
                    * `D47`: the absolute Δ&lt;sub&gt;47&lt;/sub&gt; value of this sample
                    * `N`: how many analyses of this sample should be generated
                + `a`: scrambling factor)
                + `b`: compositional nonlinearity
                + `c`: working gas offset
                + `rD47`: Δ&lt;sub&gt;47&lt;/sub&gt; repeatability
                + `seed`: explicitly set to a non-zero value to achieve random but repeatable simulations
                
                Beware that `d47` values computed from `d13C_VPDB` and `d18O_VPDB` are calculated assuming
                a working gas with δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;0 and δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;0.
                In the unusual case where simulating a different working gas composition is necessary, `d47` must be specified explicitly.
                
                Samples already defined in `D47data.Nominal_d13C_VPDB`, `D47data.Nominal_d18O_VPDB`, and `D47data.Nominal_D47`
                do not require explicit `d47`, `D47`, `d13C_VPDB` nor `d18O_VPDB` (the nominal values will be used by default).
                
                Here is an example of using this method to simulate a given combination of anchors and unknowns:

                ````py
                import D47crunch
                D = D47crunch.D47data()
                D.simulate([
                    dict(Sample = &#39;ETH-1&#39;, N = 6),
                    dict(Sample = &#39;ETH-2&#39;, N = 6),
                    dict(Sample = &#39;ETH-3&#39;, N = 12),
                    dict(Sample = &#39;FOO&#39;, d13C_VPDB = -5., d18O_VPDB = -10., D47 = 0.3, N = 4),
                    ], rD47 = 0.010)
                D.standardize()
                D.plot_sessions()
                D.verbose = True
                D.table_of_samples()
                ````
                
                This should output something like:
                
                ````
                [table_of_samples] 
                ––––––  ––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
                Sample   N  d13C_VPDB  d18O_VSMOW     D47      SE    95% CL      SD  p_Levene
                ––––––  ––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
                ETH-1    6        nan         nan  0.2052                    0.0076          
                ETH-2    6        nan         nan  0.2085                    0.0089          
                ETH-3   12        nan         nan  0.6132                    0.0118          
                FOO      4        nan         nan  0.3031  0.0057  ± 0.0118  0.0104     0.572
                ––––––  ––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
                ````
                &#39;&#39;&#39;
                from numpy import random as nprandom
                if seed:
                        rng = nprandom.default_rng(seed)
                else:
                        rng = nprandom.default_rng()
                
                if samples is None:
                        samples = [dict(Sample = s, N = 4) for s in self.Nominal_D4x]
                        samples += [{
                                &#39;Sample&#39;: &#39;FOO&#39;,
                                f&#39;d{self._4x}&#39;: 0.,
                                f&#39;D{self._4x}&#39;: 0.5 if self._4x == &#39;47&#39; else 0.2,
                                &#39;N&#39;: 4,
                                }]

                N = sum([s[&#39;N&#39;] for s in samples])
                errors = rng.normal(loc = 0, scale = 1, size = N) # generate random measurement errors
                errors *= rD4x / stdev(errors) # scale errors to rD47
                
                k = 0
                for s in samples:
                
                        if f&#39;D{self._4x}&#39; not in s:
                                if s[&#39;Sample&#39;] not in self.Nominal_D4x:
                                        raise KeyError(f&#34;Sample {s[&#39;Sample&#39;]} is missing a D{self._4x} value and it is not defined in Nominal_D{self._4x}&#34;)
                                else:
                                        s[f&#39;D{self._4x}&#39;] = self.Nominal_D4x[s[&#39;Sample&#39;]]                                       

                        if f&#39;d{self._4x}&#39; not in s:

                                if &#39;d13C_VPDB&#39; not in s:
                                        if s[&#39;Sample&#39;] not in self.Nominal_d13C_VPDB:
                                                raise KeyError(f&#34;Sample {s[&#39;Sample&#39;]} is missing d{self._4x} and d13C_VPDB values, and it is not defined in Nominal_d13C_VPDB.&#34;)
                                        else:
                                                s[&#39;d13C_VPDB&#39;] = self.Nominal_d13C_VPDB[s[&#39;Sample&#39;]]

                                if &#39;d18O_VPDB&#39; not in s:
                                        if s[&#39;Sample&#39;] not in self.Nominal_d18O_VPDB:
                                                raise KeyError(f&#34;Sample {s[&#39;Sample&#39;]} is missing d{self._4x} and d18O_VPDB values, and it is not defined in Nominal_d18O_VPDB.&#34;)
                                        else:
                                                s[&#39;d18O_VPDB&#39;] = self.Nominal_d18O_VPDB[s[&#39;Sample&#39;]]

                                i = 2 if self._4x == &#39;47&#39; else 3
                                R4xwg = self.compute_isobar_ratios(self.R13_VPDB, self.R18_VPDB * self.ALPHA_18O_ACID_REACTION)[i]
                                R4xs = self.compute_isobar_ratios(
                                        self.R13_VPDB * (1 + s[&#39;d13C_VPDB&#39;]/1000),
                                        self.R18_VPDB * (1 + s[&#39;d18O_VPDB&#39;]/1000) * self.ALPHA_18O_ACID_REACTION,
                                        )[i]*(1+s[f&#39;D{self._4x}&#39;]/1000)
                                s[f&#39;d{self._4x}&#39;] = (R4xs/R4xwg-1)*1000
                                        
                        while s[&#39;N&#39;]:
                                self.append({
                                        &#39;Sample&#39;: s[&#39;Sample&#39;],
                                        &#39;d13Cwg_VPDB&#39;: 0.,
                                        &#39;d18Owg_VSMOW&#39;: (self.R18_VSMOW * self.ALPHA_18O_ACID_REACTION - 1) * 1000,
                                        &#39;d13C_VPDB&#39;: self[&#39;d13C_VPDB&#39;],
                                        &#39;d18O_VSMOW&#39;: self[&#39;d18O_VPDB&#39;],
                                        f&#39;d{self._4x}&#39;: s[f&#39;d{self._4x}&#39;],
                                        f&#39;D{self._4x}raw&#39;: a * (s[f&#39;D{self._4x}&#39;] + errors[k]) + b * s[f&#39;d{self._4x}&#39;] + c,
                                        })
                                s[&#39;N&#39;] -= 1
                                k += 1

                self.refresh()

        def plot_distribution_of_analyses(self, dir = &#39;output&#39;, filename = None, vs_time = False, output = None):
                &#39;&#39;&#39;
                Plot temporal distribution of all analyses.
                
                __Parameters__

                + `vs_time`: if `True`, plot as a function of `TimeTag` rather than sequentially.
                &#39;&#39;&#39;

                asamples = [s for s in self.anchors]
                usamples = [s for s in self.unknowns]
                if output is None or output == &#39;fig&#39;:
                        fig = ppl.figure(figsize = (6,4))
                        ppl.subplots_adjust(0.02, 0.03, 0.9, 0.8)
                Xmax = max([r[&#39;TimeTag&#39;] if vs_time else j for j,r in enumerate(self)])
                for k, s in enumerate(asamples + usamples):
                        if vs_time:
                                X = [r[&#39;TimeTag&#39;] for r in self if r[&#39;Sample&#39;] == s]
                        else:
                                X = [x for x,r in enumerate(self) if r[&#39;Sample&#39;] == s]
                        Y = [k for x in X]
                        ppl.plot(X, Y, &#39;o&#39;, mec = None, mew = 0, mfc = &#39;b&#39; if s in usamples else &#39;r&#39;, ms = 3, alpha = .5)
                        ppl.axhline(k, color = &#39;b&#39; if s in usamples else &#39;r&#39;, lw = .5, alpha = .25)
                        ppl.text(Xmax, k, f&#39;  {s}&#39;, va = &#39;center&#39;, ha = &#39;left&#39;, size = 7)
                if vs_time:
                        t = [r[&#39;TimeTag&#39;] for r in self]
                        t1, t2 = min(t), max(t)
                        tspan = t2 - t1
                        t1 -= tspan / len(self)
                        t2 += tspan / len(self)
                        ppl.axis([t1, t2, -1, k+1])
                else:
                        ppl.axis([-1, len(self), -1, k+1])
                        

                x2 = 0
                for session in self.sessions:
                        x1 = min([r[&#39;TimeTag&#39;] if vs_time else j for j,r in enumerate(self) if r[&#39;Session&#39;] == session])
                        if vs_time:
                                ppl.axvline(x1, color = &#39;k&#39;, lw = .75)
                        if k:
                                if vs_time:
                                        ppl.axvspan(x1,x2,color = &#39;k&#39;, zorder = -100, alpha = .2)
                                else:
                                        ppl.axvline((x1+x2)/2, color = &#39;k&#39;, lw = .75)
                        x2 = max([r[&#39;TimeTag&#39;] if vs_time else j for j,r in enumerate(self) if r[&#39;Session&#39;] == session])
#                       from xlrd import xldate_as_datetime
#                       print(session, xldate_as_datetime(x1, 0), xldate_as_datetime(x2, 0))
                        if vs_time:
                                ppl.axvline(x2, color = &#39;k&#39;, lw = .75)
                        ppl.text((2*x1+x2)/3, k+1, session, ha = &#39;left&#39;, va = &#39;bottom&#39;, rotation = 45, size = 8)

                ppl.xticks([])
                ppl.yticks([])

                if output is None:
                        if not os.path.exists(dir):
                                os.makedirs(dir)
                        if filename == None:
                                filename = f&#39;D{self._4x}_distribution_of_analyses.pdf&#39;
                        ppl.savefig(f&#39;{dir}/{filename}&#39;)
                        ppl.close(fig)
                elif output == &#39;ax&#39;:
                        return ppl.gca()
                elif output == &#39;fig&#39;:
                        return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.list</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></li>
<li><a title="D47crunch.D48data" href="#D47crunch.D48data">D48data</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="D47crunch.D4xdata.ALPHA_18O_ACID_REACTION"><code class="name">var <span class="ident">ALPHA_18O_ACID_REACTION</span></code></dt>
<dd>
<div class="desc"><p>Specifies the <sup>18</sup>O/<sup>16</sup>O fractionation factor generally applicable
to acid reactions in the dataset. Currently used by <code><a title="D47crunch.D4xdata.wg" href="#D47crunch.D4xdata.wg">D4xdata.wg()</a></code>,
<code><a title="D47crunch.D4xdata.standardize_d13C" href="#D47crunch.D4xdata.standardize_d13C">D4xdata.standardize_d13C()</a></code>, and <code><a title="D47crunch.D4xdata.standardize_d18O" href="#D47crunch.D4xdata.standardize_d18O">D4xdata.standardize_d18O()</a></code>.</p>
<p>By default equal to 1.008129 (calcite reacted at 90 °C, <a href="https://dx.doi.org/10.1016/j.chemgeo.2007.08.005">Kim et al., 2007</a>).</p></div>
</dd>
<dt id="D47crunch.D4xdata.LEVENE_REF_SAMPLE"><code class="name">var <span class="ident">LEVENE_REF_SAMPLE</span></code></dt>
<dd>
<div class="desc"><p>After the Δ<sub>4x</sub> standardization step, each sample is tested to
assess whether the Δ<sub>4x</sub> variance within all analyses for that
sample differs significantly from that observed for a given reference
sample (using <a href="https://en.wikipedia.org/wiki/Levene%27s_test">Levene's test</a>, which yields a p-value corresponding to
the null hypothesis that the underlying variances are equal).</p>
<p><code>LEVENE_REF_SAMPLE</code> (by default equal to <code>'ETH-3'</code>) specifies which
sample should be used as a reference for this test.</p></div>
</dd>
<dt id="D47crunch.D4xdata.Nominal_d13C_VPDB"><code class="name">var <span class="ident">Nominal_d13C_VPDB</span></code></dt>
<dd>
<div class="desc"><p>Nominal δ<sup>13</sup>C<sub>VPDB</sub> values assigned to carbonate standards, used by
<code><a title="D47crunch.D4xdata.standardize_d13C" href="#D47crunch.D4xdata.standardize_d13C">D4xdata.standardize_d13C()</a></code>.</p>
<p>By default equal to <code>{'ETH-1': 2.02, 'ETH-2': -10.17, 'ETH-3': 1.71}</code> after
<a href="https://doi.org/10.1029/2017GC007385">Bernasconi et al. (2018)</a>.</p></div>
</dd>
<dt id="D47crunch.D4xdata.Nominal_d18O_VPDB"><code class="name">var <span class="ident">Nominal_d18O_VPDB</span></code></dt>
<dd>
<div class="desc"><p>Nominal δ<sup>18</sup>O<sub>VPDB</sub> values assigned to carbonate standards, used by
<code><a title="D47crunch.D4xdata.standardize_d18O" href="#D47crunch.D4xdata.standardize_d18O">D4xdata.standardize_d18O()</a></code>.</p>
<p>By default equal to <code>{'ETH-1': -2.19, 'ETH-2': -18.69, 'ETH-3': -1.78}</code> after
<a href="https://doi.org/10.1029/2017GC007385">Bernasconi et al. (2018)</a>.</p></div>
</dd>
<dt id="D47crunch.D4xdata.R13_VPDB"><code class="name">var <span class="ident">R13_VPDB</span></code></dt>
<dd>
<div class="desc"><p>Absolute (<sup>13</sup>C/<sup>12</sup>C) ratio of VPDB.
By default equal to 0.01118 (<a href="http://www.cnki.com.cn/Article/CJFDTotal-JXTW199004006.htm">Chang &amp; Li, 1990</a>)</p></div>
</dd>
<dt id="D47crunch.D4xdata.R17_VPDB"><code class="name">var <span class="ident">R17_VPDB</span></code></dt>
<dd>
<div class="desc"><p>Absolute (<sup>17</sup>O/<sup>16</sup>C) ratio of VPDB.
By definition equal to <code>R17_VSMOW * 1.03092 ** lambda_17</code>.</p></div>
</dd>
<dt id="D47crunch.D4xdata.R17_VSMOW"><code class="name">var <span class="ident">R17_VSMOW</span></code></dt>
<dd>
<div class="desc"><p>Absolute (<sup>17</sup>O/<sup>16</sup>C) ratio of VSMOW.
By default equal to 0.00038475
(<a href="https://dx.doi.org/10.1002/rcm.1011">Assonov &amp; Brenninkmeijer, 2003</a>, rescaled to <code>R13_VPDB</code>)</p></div>
</dd>
<dt id="D47crunch.D4xdata.R18_VPDB"><code class="name">var <span class="ident">R18_VPDB</span></code></dt>
<dd>
<div class="desc"><p>Absolute (<sup>18</sup>O/<sup>16</sup>C) ratio of VPDB.
By definition equal to <code>R18_VSMOW * 1.03092</code>.</p></div>
</dd>
<dt id="D47crunch.D4xdata.R18_VSMOW"><code class="name">var <span class="ident">R18_VSMOW</span></code></dt>
<dd>
<div class="desc"><p>Absolute (<sup>18</sup>O/<sup>16</sup>C) ratio of VSMOW.
By default equal to 0.0020052 (<a href="https://doi.org/10.1016/0012-821X(76)90115-1">Baertschi, 1976</a>)</p></div>
</dd>
<dt id="D47crunch.D4xdata.d13C_STANDARDIZATION_METHOD"><code class="name">var <span class="ident">d13C_STANDARDIZATION_METHOD</span></code></dt>
<dd>
<div class="desc"><p>Method by which to standardize δ<sup>13</sup>C values:</p>
<ul>
<li><code>none</code>: do not apply any δ<sup>13</sup>C standardization.</li>
<li><code>'1pt'</code>: within each session, offset all initial δ<sup>13</sup>C values so as to
minimize the difference between final δ<sup>13</sup>C<sub>VPDB</sub> values and
<code>Nominal_d13C_VPDB</code> (averaged over all analyses for which <code>Nominal_d13C_VPDB</code> is defined).</li>
<li><code>'2pt'</code>: within each session, apply a affine trasformation to all δ<sup>13</sup>C
values so as to minimize the difference between final δ<sup>13</sup>C<sub>VPDB</sub>
values and <code>Nominal_d13C_VPDB</code> (averaged over all analyses for which <code>Nominal_d13C_VPDB</code>
is defined).</li>
</ul></div>
</dd>
<dt id="D47crunch.D4xdata.d18O_STANDARDIZATION_METHOD"><code class="name">var <span class="ident">d18O_STANDARDIZATION_METHOD</span></code></dt>
<dd>
<div class="desc"><p>Method by which to standardize δ<sup>18</sup>O values:</p>
<ul>
<li><code>none</code>: do not apply any δ<sup>18</sup>O standardization.</li>
<li><code>'1pt'</code>: within each session, offset all initial δ<sup>18</sup>O values so as to
minimize the difference between final δ<sup>18</sup>O<sub>VPDB</sub> values and
<code>Nominal_d18O_VPDB</code> (averaged over all analyses for which <code>Nominal_d18O_VPDB</code> is defined).</li>
<li><code>'2pt'</code>: within each session, apply a affine trasformation to all δ<sup>18</sup>O
values so as to minimize the difference between final δ<sup>18</sup>O<sub>VPDB</sub>
values and <code>Nominal_d18O_VPDB</code> (averaged over all analyses for which <code>Nominal_d18O_VPDB</code>
is defined).</li>
</ul></div>
</dd>
<dt id="D47crunch.D4xdata.lambda_17"><code class="name">var <span class="ident">lambda_17</span></code></dt>
<dd>
<div class="desc"><p>Mass-dependent exponent for triple oxygen isotopes.
By default equal to 0.528 (<a href="https://doi.org/10.1002/rcm.2250">Barkan &amp; Luz, 2005</a>)</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="D47crunch.D4xdata.assign_timestamps"><code class="name flex">
<span>def <span class="ident">assign_timestamps</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Assign a time field <code>t</code> of type <code>float</code> to each analysis.</p>
<p>If <code>TimeTag</code> is one of the data fields, <code>t</code> is equal within a given session
to <code>TimeTag</code> minus the mean value of <code>TimeTag</code> for that session.
Otherwise, <code>TimeTag</code> is by default equal to the index of each analysis
in the dataset and <code>t</code> is defined as above.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">        def assign_timestamps(self):
                &#39;&#39;&#39;
                Assign a time field `t` of type `float` to each analysis.

                If `TimeTag` is one of the data fields, `t` is equal within a given session
                to `TimeTag` minus the mean value of `TimeTag` for that session.
                Otherwise, `TimeTag` is by default equal to the index of each analysis
                in the dataset and `t` is defined as above.
                &#39;&#39;&#39;
                for session in self.sessions:
                        sdata = self.sessions[session][&#39;data&#39;]
                        try:
                                t0 = np.mean([r[&#39;TimeTag&#39;] for r in sdata])
                                for r in sdata:
                                        r[&#39;t&#39;] = r[&#39;TimeTag&#39;] - t0
#                               print(&#39;DEBUG - USING TimeTag        &lt;-----------------------------------&#39;)
                        except KeyError:
                                t0 = (len(sdata)-1)/2
                                for t,r in enumerate(sdata):
                                        r[&#39;t&#39;] = t - t0</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.combine_samples"><code class="name flex">
<span>def <span class="ident">combine_samples</span></span>(<span>self, sample_groups)</span>
</code></dt>
<dd>
<div class="desc"><p>Combine analyses of different samples to compute weighted average Δ<sub>47</sub>
and new error (co)variances corresponding to the groups defined by the <code>sample_groups</code>
dictionary.</p>
<p>Caution: samples are weighted by number of replicate analyses, which is a
reasonable default behavior but is not always optimal (e.g., in the case of strongly
correlated analytical errors for one or more samples).</p>
<p>Returns a tuplet of:</p>
<ul>
<li>the list of group names</li>
<li>an array of the corresponding Δ<sub>47</sub> values</li>
<li>the corresponding (co)variance matrix</li>
</ul>
<p><strong>Parameters</strong></p>
<ul>
<li><code>sample_groups</code>: a dictionary of the form:</li>
</ul>
<pre><code class="language-py">{'group1': ['sample_1', 'sample_2'],
 'group2': ['sample_3', 'sample_4', 'sample_5']}
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_samples(self, sample_groups):
        &#39;&#39;&#39;
        Combine analyses of different samples to compute weighted average Δ&lt;sub&gt;47&lt;/sub&gt;
        and new error (co)variances corresponding to the groups defined by the `sample_groups`
        dictionary.
        
        Caution: samples are weighted by number of replicate analyses, which is a
        reasonable default behavior but is not always optimal (e.g., in the case of strongly
        correlated analytical errors for one or more samples).
        
        Returns a tuplet of:
        
        + the list of group names
        + an array of the corresponding Δ&lt;sub&gt;47&lt;/sub&gt; values
        + the corresponding (co)variance matrix
        
        __Parameters__

        + `sample_groups`: a dictionary of the form:
        ```py
        {&#39;group1&#39;: [&#39;sample_1&#39;, &#39;sample_2&#39;],
         &#39;group2&#39;: [&#39;sample_3&#39;, &#39;sample_4&#39;, &#39;sample_5&#39;]}
        ```
        &#39;&#39;&#39;
        
        samples = [s for k in sorted(sample_groups.keys()) for s in sorted(sample_groups[k])]
        groups = sorted(sample_groups.keys())
        group_total_weights = {k: sum([self.samples[s][&#39;N&#39;] for s in sample_groups[k]]) for k in groups}
        D4x_old = np.array([[self.samples[x][f&#39;D{self._4x}&#39;]] for x in samples])
        CM_old = np.array([[self.sample_D4x_covar(x,y) for x in samples] for y in samples])
        W = np.array([
                [self.samples[i][&#39;N&#39;]/group_total_weights[j] if i in sample_groups[j] else 0 for i in samples]
                for j in groups])
        D4x_new = W @ D4x_old
        CM_new = W @ CM_old @ W.T

        return groups, D4x_new[:,0], CM_new</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.compute_bulk_and_clumping_deltas"><code class="name flex">
<span>def <span class="ident">compute_bulk_and_clumping_deltas</span></span>(<span>self, r)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute δ<sup>13</sup>C<sub>VPDB</sub>, δ<sup>18</sup>O<sub>VSMOW</sub>, and
raw Δ<sub>47</sub>, Δ<sub>48</sub>, Δ<sub>49</sub> values for an analysis <code>r</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_bulk_and_clumping_deltas(self, r):
        &#39;&#39;&#39;
        Compute δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt;, δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt;, and
        raw Δ&lt;sub&gt;47&lt;/sub&gt;, Δ&lt;sub&gt;48&lt;/sub&gt;, Δ&lt;sub&gt;49&lt;/sub&gt; values for an analysis `r`.
        &#39;&#39;&#39;

        # Compute working gas R13, R18, and isobar ratios
        R13_wg = self.R13_VPDB * (1 + r[&#39;d13Cwg_VPDB&#39;] / 1000)
        R18_wg = self.R18_VSMOW * (1 + r[&#39;d18Owg_VSMOW&#39;] / 1000)
        R45_wg, R46_wg, R47_wg, R48_wg, R49_wg = self.compute_isobar_ratios(R13_wg, R18_wg)

        # Compute analyte isobar ratios
        R45 = (1 + r[&#39;d45&#39;] / 1000) * R45_wg
        R46 = (1 + r[&#39;d46&#39;] / 1000) * R46_wg
        R47 = (1 + r[&#39;d47&#39;] / 1000) * R47_wg
        R48 = (1 + r[&#39;d48&#39;] / 1000) * R48_wg
        R49 = (1 + r[&#39;d49&#39;] / 1000) * R49_wg

        r[&#39;d13C_VPDB&#39;], r[&#39;d18O_VSMOW&#39;] = self.compute_bulk_delta(R45, R46, D17O = r[&#39;D17O&#39;])
        R13 = (1 + r[&#39;d13C_VPDB&#39;] / 1000) * self.R13_VPDB
        R18 = (1 + r[&#39;d18O_VSMOW&#39;] / 1000) * self.R18_VSMOW

        # Compute stochastic isobar ratios of the analyte
        R45stoch, R46stoch, R47stoch, R48stoch, R49stoch = self.compute_isobar_ratios(
                R13, R18, D17O = r[&#39;D17O&#39;]
        )

        # Check that R45/R45stoch and R46/R46stoch are undistinguishable from 1,
        # and raise a warning if the corresponding anomalies exceed 0.02 ppm.
        if (R45 / R45stoch - 1) &gt; 5e-8:
                self.vmsg(f&#39;This is unexpected: R45/R45stoch - 1 = {1e6 * (R45 / R45stoch - 1):.3f} ppm&#39;)
        if (R46 / R46stoch - 1) &gt; 5e-8:
                self.vmsg(f&#39;This is unexpected: R46/R46stoch - 1 = {1e6 * (R46 / R46stoch - 1):.3f} ppm&#39;)

        # Compute raw clumped isotope anomalies
        r[&#39;D47raw&#39;] = 1000 * (R47 / R47stoch - 1)
        r[&#39;D48raw&#39;] = 1000 * (R48 / R48stoch - 1)
        r[&#39;D49raw&#39;] = 1000 * (R49 / R49stoch - 1)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.compute_bulk_delta"><code class="name flex">
<span>def <span class="ident">compute_bulk_delta</span></span>(<span>self, R45, R46, D17O=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute δ<sup>13</sup>C<sub>VPDB</sub> and δ<sup>18</sup>O<sub>VSMOW</sub>,
by solving the generalized form of equation (17) from <a href="https://doi.org/10.1351/PAC-REP-09-01-05">Brand et al. (2010)</a>,
assuming that δ<sup>18</sup>O<sub>VSMOW</sub> is not too big (0 ± 50 ‰) and
solving the corresponding second-order Taylor polynomial.
(Appendix A of <a href="https://doi.org/10.1016/j.chemgeo.2016.08.014">Daëron et al., 2016</a>)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_bulk_delta(self, R45, R46, D17O = 0):
        &#39;&#39;&#39;
        Compute δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; and δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt;,
        by solving the generalized form of equation (17) from [Brand et al. (2010)],
        assuming that δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt; is not too big (0 ± 50 ‰) and
        solving the corresponding second-order Taylor polynomial.
        (Appendix A of [Daëron et al., 2016])

        [Brand et al. (2010)]: https://doi.org/10.1351/PAC-REP-09-01-05
        [Daëron et al., 2016]: https://doi.org/10.1016/j.chemgeo.2016.08.014
        &#39;&#39;&#39;

        K = np.exp(D17O / 1000) * self.R17_VSMOW * self.R18_VSMOW ** -self.lambda_17

        A = -3 * K ** 2 * self.R18_VSMOW ** (2 * self.lambda_17)
        B = 2 * K * R45 * self.R18_VSMOW ** self.lambda_17
        C = 2 * self.R18_VSMOW
        D = -R46

        aa = A * self.lambda_17 * (2 * self.lambda_17 - 1) + B * self.lambda_17 * (self.lambda_17 - 1) / 2
        bb = 2 * A * self.lambda_17 + B * self.lambda_17 + C
        cc = A + B + C + D

        d18O_VSMOW = 1000 * (-bb + (bb ** 2 - 4 * aa * cc) ** .5) / (2 * aa)

        R18 = (1 + d18O_VSMOW / 1000) * self.R18_VSMOW
        R17 = K * R18 ** self.lambda_17
        R13 = R45 - 2 * R17

        d13C_VPDB = 1000 * (R13 / self.R13_VPDB - 1)

        return d13C_VPDB, d18O_VSMOW</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.compute_isobar_ratios"><code class="name flex">
<span>def <span class="ident">compute_isobar_ratios</span></span>(<span>self, R13, R18, D17O=0, D47=0, D48=0, D49=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute isobar ratios for a sample with isotopic ratios <code>R13</code> and <code>R18</code>,
optionally accounting for non-zero values of Δ<sup>17</sup>O (<code>D17O</code>) and clumped isotope
anomalies (<code>D47</code>, <code>D48</code>, <code>D49</code>), all expressed in permil.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_isobar_ratios(self, R13, R18, D17O=0, D47=0, D48=0, D49=0):
        &#39;&#39;&#39;
        Compute isobar ratios for a sample with isotopic ratios `R13` and `R18`,
        optionally accounting for non-zero values of Δ&lt;sup&gt;17&lt;/sup&gt;O (`D17O`) and clumped isotope
        anomalies (`D47`, `D48`, `D49`), all expressed in permil.
        &#39;&#39;&#39;

        # Compute R17
        R17 = self.R17_VSMOW * np.exp(D17O / 1000) * (R18 / self.R18_VSMOW) ** self.lambda_17

        # Compute isotope concentrations
        C12 = (1 + R13) ** -1
        C13 = C12 * R13
        C16 = (1 + R17 + R18) ** -1
        C17 = C16 * R17
        C18 = C16 * R18

        # Compute stochastic isotopologue concentrations
        C626 = C16 * C12 * C16
        C627 = C16 * C12 * C17 * 2
        C628 = C16 * C12 * C18 * 2
        C636 = C16 * C13 * C16
        C637 = C16 * C13 * C17 * 2
        C638 = C16 * C13 * C18 * 2
        C727 = C17 * C12 * C17
        C728 = C17 * C12 * C18 * 2
        C737 = C17 * C13 * C17
        C738 = C17 * C13 * C18 * 2
        C828 = C18 * C12 * C18
        C838 = C18 * C13 * C18

        # Compute stochastic isobar ratios
        R45 = (C636 + C627) / C626
        R46 = (C628 + C637 + C727) / C626
        R47 = (C638 + C728 + C737) / C626
        R48 = (C738 + C828) / C626
        R49 = C838 / C626

        # Account for stochastic anomalies
        R47 *= 1 + D47 / 1000
        R48 *= 1 + D48 / 1000
        R49 *= 1 + D49 / 1000

        # Return isobar ratios
        return R45, R46, R47, R48, R49</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.compute_r"><code class="name flex">
<span>def <span class="ident">compute_r</span></span>(<span>self, key, samples='all samples', sessions='all sessions')</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the repeatability of <code>[r[key] for r in self]</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@make_verbal
def compute_r(self, key, samples = &#39;all samples&#39;, sessions = &#39;all sessions&#39;):
        &#39;&#39;&#39;
        Compute the repeatability of `[r[key] for r in self]`
        &#39;&#39;&#39;
        # NB: it&#39;s debatable whether rD47 should be computed
        # with Nf = len(self)-len(self.samples) instead of
        # Nf = len(self) - len(self.unknwons) - 3*len(self.sessions)

        if samples == &#39;all samples&#39;:
                mysamples = [k for k in self.samples]
        elif samples == &#39;anchors&#39;:
                mysamples = [k for k in self.anchors]
        elif samples == &#39;unknowns&#39;:
                mysamples = [k for k in self.unknowns]
        else:
                mysamples = samples

        if sessions == &#39;all sessions&#39;:
                sessions = [k for k in self.sessions]

        if key in [&#39;D47&#39;, &#39;D48&#39;]:
                chisq, Nf = 0, 0
                for sample in mysamples :
                        X = [ r[key] for r in self if r[&#39;Sample&#39;] == sample and r[&#39;Session&#39;] in sessions ]
                        if len(X) &gt; 1 :
                                chisq += np.sum([ (x-self.samples[sample][key])**2 for x in X ])
                                if sample in self.unknowns:
                                        Nf += len(X) - 1
                                else:
                                        Nf += len(X)
                if samples in [&#39;anchors&#39;, &#39;all samples&#39;]:
                        Nf -= sum([self.sessions[s][&#39;Np&#39;] for s in sessions])
                r = (chisq / Nf)**.5 if Nf &gt; 0 else 0

        else: # if key not in [&#39;D47&#39;, &#39;D48&#39;]
                chisq, Nf = 0, 0
                for sample in mysamples :
                        X = [ r[key] for r in self if r[&#39;Sample&#39;] == sample and r[&#39;Session&#39;] in sessions ]
                        if len(X) &gt; 1 :
                                Nf += len(X) - 1
                                chisq += np.sum([ (x-np.mean(X))**2 for x in X ])
                r = (chisq / Nf)**.5 if Nf &gt; 0 else 0

        self.msg(f&#39;Repeatability of r[&#34;{key}&#34;] is {1000*r:.1f} ppm for {samples}.&#39;)
        return r</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.consolidate"><code class="name flex">
<span>def <span class="ident">consolidate</span></span>(<span>self, tables=True, plots=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Collect information about samples, sessions and repeatabilities.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@make_verbal
def consolidate(self, tables = True, plots = True):
        &#39;&#39;&#39;
        Collect information about samples, sessions and repeatabilities.
        &#39;&#39;&#39;
        self.consolidate_samples()
        self.consolidate_sessions()
        self.repeatabilities()

        if tables:
                self.summary()
                self.table_of_sessions()
                self.table_of_analyses()
                self.table_of_samples()

        if plots:
                self.plot_sessions()</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.consolidate_samples"><code class="name flex">
<span>def <span class="ident">consolidate_samples</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Compile various statistics for each sample.</p>
<p>For each anchor sample:</p>
<ul>
<li><code>D47</code>: the nominal Δ<sub>47</sub> value for this anchor, specified by <code>self.Nominal_D47</code></li>
<li><code>SE_D47</code>: set to zero by definition</li>
</ul>
<p>For each unknown sample:</p>
<ul>
<li><code>D47</code>: the standardized Δ<sub>47</sub> value for this unknown</li>
<li><code>SE_D47</code>: the standard error of Δ<sub>47</sub> for this unknown</li>
</ul>
<p>For each anchor and unknown:</p>
<ul>
<li><code>N</code>: the total number of analyses of this sample</li>
<li><code>SD_D47</code>: the “sample” (in the statistical sense) standard deviation for this sample</li>
<li><code>d13C_VPDB</code>: the average δ<sup>13</sup>C<sub>VPDB</sub> value for this sample</li>
<li><code>d18O_VSMOW</code>: the average δ<sup>18</sup>O<sub>VSMOW</sub> value for this sample (as CO<sub>2</sub>)</li>
<li><code>p_Levene</code>: the p-value from a <a href="https://en.wikipedia.org/wiki/Levene%27s_test">Levene test</a> of equal variance, indicating whether
the Δ<sub>47</sub> repeatability this sample differs significantly from that observed
for the reference sample specified by <code>self.LEVENE_REF_SAMPLE</code>.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@make_verbal
def consolidate_samples(self):
        &#39;&#39;&#39;
        Compile various statistics for each sample.

        For each anchor sample:

        + `D47`: the nominal Δ&lt;sub&gt;47&lt;/sub&gt; value for this anchor, specified by `self.Nominal_D47`
        + `SE_D47`: set to zero by definition

        For each unknown sample:

        + `D47`: the standardized Δ&lt;sub&gt;47&lt;/sub&gt; value for this unknown
        + `SE_D47`: the standard error of Δ&lt;sub&gt;47&lt;/sub&gt; for this unknown

        For each anchor and unknown:

        + `N`: the total number of analyses of this sample
        + `SD_D47`: the “sample” (in the statistical sense) standard deviation for this sample
        + `d13C_VPDB`: the average δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; value for this sample
        + `d18O_VSMOW`: the average δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt; value for this sample (as CO&lt;sub&gt;2&lt;/sub&gt;)
        + `p_Levene`: the p-value from a [Levene test] of equal variance, indicating whether
        the Δ&lt;sub&gt;47&lt;/sub&gt; repeatability this sample differs significantly from that observed
        for the reference sample specified by `self.LEVENE_REF_SAMPLE`.

        [Levene test]: https://en.wikipedia.org/wiki/Levene%27s_test
        &#39;&#39;&#39;
        D4x_ref_pop = [r[f&#39;D{self._4x}&#39;] for r in self.samples[self.LEVENE_REF_SAMPLE][&#39;data&#39;]]
        for sample in self.samples:
                self.samples[sample][&#39;N&#39;] = len(self.samples[sample][&#39;data&#39;])
                if self.samples[sample][&#39;N&#39;] &gt; 1:
                        self.samples[sample][f&#39;SD_D{self._4x}&#39;] = stdev([r[f&#39;D{self._4x}&#39;] for r in self.samples[sample][&#39;data&#39;]])

                self.samples[sample][&#39;d13C_VPDB&#39;] = np.mean([r[&#39;d13C_VPDB&#39;] for r in self.samples[sample][&#39;data&#39;]])
                self.samples[sample][&#39;d18O_VSMOW&#39;] = np.mean([r[&#39;d18O_VSMOW&#39;] for r in self.samples[sample][&#39;data&#39;]])

                D4x_pop = [r[f&#39;D{self._4x}&#39;] for r in self.samples[sample][&#39;data&#39;]]
                if len(D4x_pop) &gt; 2:
                        self.samples[sample][&#39;p_Levene&#39;] = levene(D4x_ref_pop, D4x_pop, center = &#39;median&#39;)[1]

        if self.standardization_method == &#39;pooled&#39;:
                for sample in self.anchors:
                        self.samples[sample][f&#39;D{self._4x}&#39;] = self.Nominal_D4x[sample]
                        self.samples[sample][f&#39;SE_D{self._4x}&#39;] = 0.
                for sample in self.unknowns:
                        self.samples[sample][f&#39;D{self._4x}&#39;] = self.standardization.params.valuesdict()[f&#39;D{self._4x}_{pf(sample)}&#39;]
                        try:
                                self.samples[sample][f&#39;SE_D{self._4x}&#39;] = self.sample_D4x_covar(sample)**.5
                        except ValueError:
                                # when `sample` is constrained by self.standardize(constraints = {...}),
                                # it is no longer listed in self.standardization.var_names.
                                # Temporary fix: define SE as zero for now
                                self.samples[sample][f&#39;SE_D4{self._4x}&#39;] = 0.

        elif self.standardization_method == &#39;indep_sessions&#39;:
                for sample in self.anchors:
                        self.samples[sample][f&#39;D{self._4x}&#39;] = self.Nominal_D4x[sample]
                        self.samples[sample][f&#39;SE_D{self._4x}&#39;] = 0.
                for sample in self.unknowns:
                        self.msg(f&#39;Consolidating sample {sample}&#39;)
                        self.unknowns[sample][f&#39;session_D{self._4x}&#39;] = {}
                        session_avg = []
                        for session in self.sessions:
                                sdata = [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]
                                if sdata:
                                        self.msg(f&#39;{sample} found in session {session}&#39;)
                                        avg_D4x = np.mean([r[f&#39;D{self._4x}&#39;] for r in sdata])
                                        avg_d4x = np.mean([r[f&#39;d{self._4x}&#39;] for r in sdata])
                                        # !! TODO: sigma_s below does not account for temporal changes in standardization error
                                        sigma_s = self.standardization_error(session, avg_d4x, avg_D4x)
                                        sigma_u = sdata[0][f&#39;wD{self._4x}raw&#39;] / self.sessions[session][&#39;a&#39;] / len(sdata)**.5
                                        session_avg.append([avg_D4x, (sigma_u**2 + sigma_s**2)**.5])
                                        self.unknowns[sample][f&#39;session_D{self._4x}&#39;][session] = session_avg[-1]
                        self.samples[sample][f&#39;D{self._4x}&#39;], self.samples[sample][f&#39;SE_D{self._4x}&#39;] = w_avg(*zip(*session_avg))
                        weights = {s: self.unknowns[sample][f&#39;session_D{self._4x}&#39;][s][1]**-2 for s in self.unknowns[sample][f&#39;session_D{self._4x}&#39;]}
                        wsum = sum([weights[s] for s in weights])
                        for s in weights:
                                self.unknowns[sample][f&#39;session_D{self._4x}&#39;][s] += [self.unknowns[sample][f&#39;session_D{self._4x}&#39;][s][1]**-2 / wsum]</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.consolidate_sessions"><code class="name flex">
<span>def <span class="ident">consolidate_sessions</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Compile various statistics for each session.</p>
<ul>
<li><code>Na</code>: Number of anchor analyses in the session</li>
<li><code>Nu</code>: Number of unknown analyses in the session</li>
<li><code>r_d13C_VPDB</code>: δ<sup>13</sup>C<sub>VPDB</sub> repeatability of analyses within the session</li>
<li><code>r_d18O_VSMOW</code>: δ<sup>18</sup>O<sub>VSMOW</sub> repeatability of analyses within the session</li>
<li><code>r_D47</code>: Δ<sub>47</sub> repeatability of analyses within the session</li>
<li><code>a</code>: scrambling factor</li>
<li><code>b</code>: compositional slope</li>
<li><code>c</code>: WG offset</li>
<li><code>SE_a</code>: Model stadard erorr of <code>a</code></li>
<li><code>SE_b</code>: Model stadard erorr of <code>b</code></li>
<li><code>SE_c</code>: Model stadard erorr of <code>c</code></li>
<li><code>scrambling_drift</code> (boolean): whether to allow a temporal drift in the scrambling factor (<code>a</code>)</li>
<li><code>slope_drift</code> (boolean): whether to allow a temporal drift in the compositional slope (<code>b</code>)</li>
<li><code>wg_drift</code> (boolean): whether to allow a temporal drift in the WG offset (<code>c</code>)</li>
<li><code>a2</code>: scrambling factor drift</li>
<li><code>b2</code>: compositional slope drift</li>
<li><code>c2</code>: WG offset drift</li>
<li><code>Np</code>: Number of standardization parameters to fit</li>
<li><code>CM</code>: model covariance matrix for (<code>a</code>, <code>b</code>, <code>c</code>, <code>a2</code>, <code>b2</code>, <code>c2</code>)</li>
<li><code>d13Cwg_VPDB</code>: δ<sup>13</sup>C<sub>VPDB</sub> of WG</li>
<li><code>d18Owg_VSMOW</code>: δ<sup>18</sup>O<sub>VSMOW</sub> of WG</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def consolidate_sessions(self):
        &#39;&#39;&#39;
        Compile various statistics for each session.

        + `Na`: Number of anchor analyses in the session
        + `Nu`: Number of unknown analyses in the session
        + `r_d13C_VPDB`: δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; repeatability of analyses within the session
        + `r_d18O_VSMOW`: δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt; repeatability of analyses within the session
        + `r_D47`: Δ&lt;sub&gt;47&lt;/sub&gt; repeatability of analyses within the session
        + `a`: scrambling factor
        + `b`: compositional slope
        + `c`: WG offset
        + `SE_a`: Model stadard erorr of `a`
        + `SE_b`: Model stadard erorr of `b`
        + `SE_c`: Model stadard erorr of `c`
        + `scrambling_drift` (boolean): whether to allow a temporal drift in the scrambling factor (`a`)
        + `slope_drift` (boolean): whether to allow a temporal drift in the compositional slope (`b`)
        + `wg_drift` (boolean): whether to allow a temporal drift in the WG offset (`c`)
        + `a2`: scrambling factor drift
        + `b2`: compositional slope drift
        + `c2`: WG offset drift
        + `Np`: Number of standardization parameters to fit
        + `CM`: model covariance matrix for (`a`, `b`, `c`, `a2`, `b2`, `c2`)
        + `d13Cwg_VPDB`: δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; of WG
        + `d18Owg_VSMOW`: δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt; of WG
        &#39;&#39;&#39;
        for session in self.sessions:
                if &#39;d13Cwg_VPDB&#39; not in self.sessions[session]:
                        self.sessions[session][&#39;d13Cwg_VPDB&#39;] = self.sessions[session][&#39;data&#39;][0][&#39;d13Cwg_VPDB&#39;]
                if &#39;d18Owg_VSMOW&#39; not in self.sessions[session]:
                        self.sessions[session][&#39;d18Owg_VSMOW&#39;] = self.sessions[session][&#39;data&#39;][0][&#39;d18Owg_VSMOW&#39;]
                self.sessions[session][&#39;Na&#39;] = len([r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.anchors])
                self.sessions[session][&#39;Nu&#39;] = len([r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.unknowns])

                self.msg(f&#39;Computing repeatabilities for session {session}&#39;)
                self.sessions[session][&#39;r_d13C_VPDB&#39;] = self.compute_r(&#39;d13C_VPDB&#39;, samples = &#39;anchors&#39;, sessions = [session])
                self.sessions[session][&#39;r_d18O_VSMOW&#39;] = self.compute_r(&#39;d18O_VSMOW&#39;, samples = &#39;anchors&#39;, sessions = [session])
                self.sessions[session][f&#39;r_D{self._4x}&#39;] = self.compute_r(f&#39;D{self._4x}&#39;, sessions = [session])

        if self.standardization_method == &#39;pooled&#39;:
                for session in self.sessions:

                        self.sessions[session][&#39;a&#39;] = self.standardization.params.valuesdict()[f&#39;a_{pf(session)}&#39;]
                        i = self.standardization.var_names.index(f&#39;a_{pf(session)}&#39;)
                        self.sessions[session][&#39;SE_a&#39;] = self.standardization.covar[i,i]**.5

                        self.sessions[session][&#39;b&#39;] = self.standardization.params.valuesdict()[f&#39;b_{pf(session)}&#39;]
                        i = self.standardization.var_names.index(f&#39;b_{pf(session)}&#39;)
                        self.sessions[session][&#39;SE_b&#39;] = self.standardization.covar[i,i]**.5

                        self.sessions[session][&#39;c&#39;] = self.standardization.params.valuesdict()[f&#39;c_{pf(session)}&#39;]
                        i = self.standardization.var_names.index(f&#39;c_{pf(session)}&#39;)
                        self.sessions[session][&#39;SE_c&#39;] = self.standardization.covar[i,i]**.5

                        self.sessions[session][&#39;a2&#39;] = self.standardization.params.valuesdict()[f&#39;a2_{pf(session)}&#39;]
                        if self.sessions[session][&#39;scrambling_drift&#39;]:
                                i = self.standardization.var_names.index(f&#39;a2_{pf(session)}&#39;)
                                self.sessions[session][&#39;SE_a2&#39;] = self.standardization.covar[i,i]**.5
                        else:
                                self.sessions[session][&#39;SE_a2&#39;] = 0.

                        self.sessions[session][&#39;b2&#39;] = self.standardization.params.valuesdict()[f&#39;b2_{pf(session)}&#39;]
                        if self.sessions[session][&#39;slope_drift&#39;]:
                                i = self.standardization.var_names.index(f&#39;b2_{pf(session)}&#39;)
                                self.sessions[session][&#39;SE_b2&#39;] = self.standardization.covar[i,i]**.5
                        else:
                                self.sessions[session][&#39;SE_b2&#39;] = 0.

                        self.sessions[session][&#39;c2&#39;] = self.standardization.params.valuesdict()[f&#39;c2_{pf(session)}&#39;]
                        if self.sessions[session][&#39;wg_drift&#39;]:
                                i = self.standardization.var_names.index(f&#39;c2_{pf(session)}&#39;)
                                self.sessions[session][&#39;SE_c2&#39;] = self.standardization.covar[i,i]**.5
                        else:
                                self.sessions[session][&#39;SE_c2&#39;] = 0.

                        i = self.standardization.var_names.index(f&#39;a_{pf(session)}&#39;)
                        j = self.standardization.var_names.index(f&#39;b_{pf(session)}&#39;)
                        k = self.standardization.var_names.index(f&#39;c_{pf(session)}&#39;)
                        CM = np.zeros((6,6))
                        CM[:3,:3] = self.standardization.covar[[i,j,k],:][:,[i,j,k]]
                        try:
                                i2 = self.standardization.var_names.index(f&#39;a2_{pf(session)}&#39;)
                                CM[3,[0,1,2,3]] = self.standardization.covar[i2,[i,j,k,i2]]
                                CM[[0,1,2,3],3] = self.standardization.covar[[i,j,k,i2],i2]
                                try:
                                        j2 = self.standardization.var_names.index(f&#39;b2_{pf(session)}&#39;)
                                        CM[3,4] = self.standardization.covar[i2,j2]
                                        CM[4,3] = self.standardization.covar[j2,i2]
                                except ValueError:
                                        pass
                                try:
                                        k2 = self.standardization.var_names.index(f&#39;c2_{pf(session)}&#39;)
                                        CM[3,5] = self.standardization.covar[i2,k2]
                                        CM[5,3] = self.standardization.covar[k2,i2]
                                except ValueError:
                                        pass
                        except ValueError:
                                pass
                        try:
                                j2 = self.standardization.var_names.index(f&#39;b2_{pf(session)}&#39;)
                                CM[4,[0,1,2,4]] = self.standardization.covar[j2,[i,j,k,j2]]
                                CM[[0,1,2,4],4] = self.standardization.covar[[i,j,k,j2],j2]
                                try:
                                        k2 = self.standardization.var_names.index(f&#39;c2_{pf(session)}&#39;)
                                        CM[4,5] = self.standardization.covar[j2,k2]
                                        CM[5,4] = self.standardization.covar[k2,j2]
                                except ValueError:
                                        pass
                        except ValueError:
                                pass
                        try:
                                k2 = self.standardization.var_names.index(f&#39;c2_{pf(session)}&#39;)
                                CM[5,[0,1,2,5]] = self.standardization.covar[k2,[i,j,k,k2]]
                                CM[[0,1,2,5],5] = self.standardization.covar[[i,j,k,k2],k2]
                        except ValueError:
                                pass

                        self.sessions[session][&#39;CM&#39;] = CM

        elif self.standardization_method == &#39;indep_sessions&#39;:
                pass # Not implemented yet</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.crunch"><code class="name flex">
<span>def <span class="ident">crunch</span></span>(<span>self, verbose='')</span>
</code></dt>
<dd>
<div class="desc"><p>Compute bulk composition and raw clumped isotope anomalies for all analyses.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@make_verbal
def crunch(self, verbose = &#39;&#39;):
        &#39;&#39;&#39;
        Compute bulk composition and raw clumped isotope anomalies for all analyses.
        &#39;&#39;&#39;
        for r in self:
                self.compute_bulk_and_clumping_deltas(r)
        self.standardize_d13C()
        self.standardize_d18O()
        self.msg(f&#34;Crunched {len(self)} analyses.&#34;)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.fill_in_missing_info"><code class="name flex">
<span>def <span class="ident">fill_in_missing_info</span></span>(<span>self, session='mySession')</span>
</code></dt>
<dd>
<div class="desc"><p>Fill in optional fields with default values</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fill_in_missing_info(self, session = &#39;mySession&#39;):
        &#39;&#39;&#39;
        Fill in optional fields with default values
        &#39;&#39;&#39;
        for i,r in enumerate(self):
                if &#39;D17O&#39; not in r:
                        r[&#39;D17O&#39;] = 0.
                if &#39;UID&#39; not in r:
                        r[&#39;UID&#39;] = f&#39;{i+1}&#39;
                if &#39;Session&#39; not in r:
                        r[&#39;Session&#39;] = session
                for k in [&#39;d47&#39;, &#39;d48&#39;, &#39;d49&#39;]:
                        if k not in r:
                                r[k] = np.nan</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.input"><code class="name flex">
<span>def <span class="ident">input</span></span>(<span>self, txt, sep='', session='')</span>
</code></dt>
<dd>
<div class="desc"><p>Read <code>txt</code> string in csv format to load analysis data into a <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> object.</p>
<p>In the csv string, spaces before and after field separators (<code>','</code> by default)
are optional. Each line corresponds to a single analysis.</p>
<p>The required fields are:</p>
<ul>
<li><code>UID</code>: a unique identifier</li>
<li><code>Session</code>: an identifier for the analytical session</li>
<li><code>Sample</code>: a sample identifier</li>
<li><code>d45</code>, <code>d46</code>, and at least one of <code>d47</code> or <code>d48</code>: the working-gas delta values</li>
</ul>
<p>Independently known oxygen-17 anomalies may be provided as <code>D17O</code> (in ‰ relative to
VSMOW, λ = <code>self.lambda_17</code>), and are otherwise assumed to be zero. Working-gas deltas <code>d47</code>, <code>d48</code>
and <code>d49</code> are optional, and set to NaN by default.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>txt</code>: the csv string to read</li>
<li><code>sep</code>: csv separator delimiting the fields. By default, use <code>,</code>, <code>;</code>, or <code>
</code>,
whichever appers most often in <code>txt</code>.</li>
<li><code>session</code>: set <code>Session</code> field to this string for all analyses</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def input(self, txt, sep = &#39;&#39;, session = &#39;&#39;):
        &#39;&#39;&#39;
        Read `txt` string in csv format to load analysis data into a `D47data` object.

        In the csv string, spaces before and after field separators (`&#39;,&#39;` by default)
        are optional. Each line corresponds to a single analysis.

        The required fields are:

        + `UID`: a unique identifier
        + `Session`: an identifier for the analytical session
        + `Sample`: a sample identifier
        + `d45`, `d46`, and at least one of `d47` or `d48`: the working-gas delta values

        Independently known oxygen-17 anomalies may be provided as `D17O` (in ‰ relative to
        VSMOW, λ = `self.lambda_17`), and are otherwise assumed to be zero. Working-gas deltas `d47`, `d48`
        and `d49` are optional, and set to NaN by default.

        __Parameters__

        + `txt`: the csv string to read
        + `sep`: csv separator delimiting the fields. By default, use `,`, `;`, or `\t`,
        whichever appers most often in `txt`.
        + `session`: set `Session` field to this string for all analyses
        &#39;&#39;&#39;
        if sep == &#39;&#39;:
                sep = sorted(&#39;,;\t&#39;, key = lambda x: - txt.count(x))[0]
        txt = [[x.strip() for x in l.split(sep)] for l in txt.splitlines() if l.strip()]
        data = [{k: v if k in [&#39;UID&#39;, &#39;Session&#39;, &#39;Sample&#39;] else smart_type(v) for k,v in zip(txt[0], l) if v != &#39;&#39;} for l in txt[1:]]

        if session != &#39;&#39;:
                for r in data:
                        r[&#39;Session&#39;] = session

        self += data
        self.refresh()</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>self, *txts)</span>
</code></dt>
<dd>
<div class="desc"><p>Log a message to <code>self.logfile</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log(self, *txts):
        &#39;&#39;&#39;
        Log a message to `self.logfile`
        &#39;&#39;&#39;
        if self.logfile:
                with open(self.logfile, &#39;a&#39;) as fid:
                        for txt in txts:
                                fid.write(f&#39;\n{dt.now().strftime(&#34;%Y-%m-%d %H:%M:%S&#34;)} {f&#34;[{self.prefix}]&#34;:&lt;16} {txt}&#39;)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.make_verbal"><code class="name flex">
<span>def <span class="ident">make_verbal</span></span>(<span>oldfun)</span>
</code></dt>
<dd>
<div class="desc"><p>Decorator: allow temporarily changing <code>self.prefix</code> and overriding <code>self.verbose</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_verbal(oldfun):
        &#39;&#39;&#39;
        Decorator: allow temporarily changing `self.prefix` and overriding `self.verbose`.
        &#39;&#39;&#39;
        @wraps(oldfun)
        def newfun(*args, verbose = &#39;&#39;, **kwargs):
                myself = args[0]
                oldprefix = myself.prefix
                myself.prefix = oldfun.__name__
                if verbose != &#39;&#39;:
                        oldverbose = myself.verbose
                        myself.verbose = verbose
                out = oldfun(*args, **kwargs)
                myself.prefix = oldprefix
                if verbose != &#39;&#39;:
                        myself.verbose = oldverbose
                return out
        return newfun</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.msg"><code class="name flex">
<span>def <span class="ident">msg</span></span>(<span>self, txt)</span>
</code></dt>
<dd>
<div class="desc"><p>Log a message to <code>self.logfile</code>, and print it out if <code>verbose = True</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def msg(self, txt):
        &#39;&#39;&#39;
        Log a message to `self.logfile`, and print it out if `verbose = True`
        &#39;&#39;&#39;
        self.log(txt)
        if self.verbose:
                print(f&#39;{f&#34;[{self.prefix}]&#34;:&lt;16} {txt}&#39;)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.plot_distribution_of_analyses"><code class="name flex">
<span>def <span class="ident">plot_distribution_of_analyses</span></span>(<span>self, dir='output', filename=None, vs_time=False, output=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot temporal distribution of all analyses.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>vs_time</code>: if <code>True</code>, plot as a function of <code>TimeTag</code> rather than sequentially.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">        def plot_distribution_of_analyses(self, dir = &#39;output&#39;, filename = None, vs_time = False, output = None):
                &#39;&#39;&#39;
                Plot temporal distribution of all analyses.
                
                __Parameters__

                + `vs_time`: if `True`, plot as a function of `TimeTag` rather than sequentially.
                &#39;&#39;&#39;

                asamples = [s for s in self.anchors]
                usamples = [s for s in self.unknowns]
                if output is None or output == &#39;fig&#39;:
                        fig = ppl.figure(figsize = (6,4))
                        ppl.subplots_adjust(0.02, 0.03, 0.9, 0.8)
                Xmax = max([r[&#39;TimeTag&#39;] if vs_time else j for j,r in enumerate(self)])
                for k, s in enumerate(asamples + usamples):
                        if vs_time:
                                X = [r[&#39;TimeTag&#39;] for r in self if r[&#39;Sample&#39;] == s]
                        else:
                                X = [x for x,r in enumerate(self) if r[&#39;Sample&#39;] == s]
                        Y = [k for x in X]
                        ppl.plot(X, Y, &#39;o&#39;, mec = None, mew = 0, mfc = &#39;b&#39; if s in usamples else &#39;r&#39;, ms = 3, alpha = .5)
                        ppl.axhline(k, color = &#39;b&#39; if s in usamples else &#39;r&#39;, lw = .5, alpha = .25)
                        ppl.text(Xmax, k, f&#39;  {s}&#39;, va = &#39;center&#39;, ha = &#39;left&#39;, size = 7)
                if vs_time:
                        t = [r[&#39;TimeTag&#39;] for r in self]
                        t1, t2 = min(t), max(t)
                        tspan = t2 - t1
                        t1 -= tspan / len(self)
                        t2 += tspan / len(self)
                        ppl.axis([t1, t2, -1, k+1])
                else:
                        ppl.axis([-1, len(self), -1, k+1])
                        

                x2 = 0
                for session in self.sessions:
                        x1 = min([r[&#39;TimeTag&#39;] if vs_time else j for j,r in enumerate(self) if r[&#39;Session&#39;] == session])
                        if vs_time:
                                ppl.axvline(x1, color = &#39;k&#39;, lw = .75)
                        if k:
                                if vs_time:
                                        ppl.axvspan(x1,x2,color = &#39;k&#39;, zorder = -100, alpha = .2)
                                else:
                                        ppl.axvline((x1+x2)/2, color = &#39;k&#39;, lw = .75)
                        x2 = max([r[&#39;TimeTag&#39;] if vs_time else j for j,r in enumerate(self) if r[&#39;Session&#39;] == session])
#                       from xlrd import xldate_as_datetime
#                       print(session, xldate_as_datetime(x1, 0), xldate_as_datetime(x2, 0))
                        if vs_time:
                                ppl.axvline(x2, color = &#39;k&#39;, lw = .75)
                        ppl.text((2*x1+x2)/3, k+1, session, ha = &#39;left&#39;, va = &#39;bottom&#39;, rotation = 45, size = 8)

                ppl.xticks([])
                ppl.yticks([])

                if output is None:
                        if not os.path.exists(dir):
                                os.makedirs(dir)
                        if filename == None:
                                filename = f&#39;D{self._4x}_distribution_of_analyses.pdf&#39;
                        ppl.savefig(f&#39;{dir}/{filename}&#39;)
                        ppl.close(fig)
                elif output == &#39;ax&#39;:
                        return ppl.gca()
                elif output == &#39;fig&#39;:
                        return fig</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.plot_residuals"><code class="name flex">
<span>def <span class="ident">plot_residuals</span></span>(<span>self, dir='output', filename=None, highlight=[], colors=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot residuals of each analysis as a function of time (actually, as a function of
the order of analyses in the D47data() object)</p>
<ul>
<li><code>dir</code>: the directory in which to save the plot</li>
<li><code>highlight</code>: a list of samples to highlight</li>
<li><code>colors</code>: a dict of {<sample>: <color>} for all samples</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">        def plot_residuals(self, dir = &#39;output&#39;, filename = None, highlight = [], colors = None):
                &#39;&#39;&#39;
                Plot residuals of each analysis as a function of time (actually, as a function of
                the order of analyses in the D47data() object)

                + `dir`: the directory in which to save the plot
                + `highlight`: a list of samples to highlight
                + `colors`: a dict of {&lt;sample&gt;: &lt;color&gt;} for all samples
                &#39;&#39;&#39;
                fig = ppl.figure(figsize = (8,4))
                ppl.subplots_adjust(.1,.05,.78,.8)
                N = len(self.anchors)
                if colors is None:
                        if len(highlight) &gt; 0:
                                Nh = len(highlight)
                                if Nh == 1:
                                        colors = {highlight[0]: (0,0,0)}
                                elif Nh == 3:
                                        colors = {a: c for a,c in zip(highlight, [(0,0,1), (1,0,0), (0,2/3,0)])}
                                elif Nh == 4:
                                        colors = {a: c for a,c in zip(highlight, [(0,0,1), (1,0,0), (0,2/3,0), (.75,0,.75)])}
                                else:
                                        colors = {a: hls_to_rgb(k/Nh, .4, 1) for k,a in enumerate(highlight)}
                        else:
                                if N == 3:
                                        colors = {a: c for a,c in zip(self.anchors, [(0,0,1), (1,0,0), (0,2/3,0)])}
                                elif N == 4:
                                        colors = {a: c for a,c in zip(self.anchors, [(0,0,1), (1,0,0), (0,2/3,0), (.75,0,.75)])}
                                else:
                                        colors = {a: hls_to_rgb(k/N, .4, 1) for k,a in enumerate(self.anchors)}
                session = self[0][&#39;Session&#39;]
                x1 = 0
#               ymax = np.max([1e3 * (r[&#39;D47&#39;] - self.samples[r[&#39;Sample&#39;]][&#39;D47&#39;]) for r in self])
                x_sessions = {}
                one_or_more_singlets = False
                one_or_more_multiplets = False
                for k,r in enumerate(self):
                        if r[&#39;Session&#39;] != session:
                                x2 = k-1
                                x_sessions[session] = (x1+x2)/2
                                ppl.axvline(k - 0.5, color = &#39;k&#39;, lw = .5)
                                session = r[&#39;Session&#39;]
                                x1 = k
                        singlet = len(self.samples[r[&#39;Sample&#39;]][&#39;data&#39;]) == 1
                        if r[&#39;Sample&#39;] in self.unknowns:
                                if singlet:
                                        one_or_more_singlets = True
                                else:
                                        one_or_more_multiplets = True
                        kw = dict(
                                marker = &#39;x&#39; if singlet else &#39;+&#39;,
                                ms = 4 if singlet else 5,
                                ls = &#39;None&#39;,
                                mec = colors[r[&#39;Sample&#39;]] if r[&#39;Sample&#39;] in colors else (0,0,0),
                                mew = 1,
                                alpha = 0.2 if singlet else 1,
                                )
                        if highlight and r[&#39;Sample&#39;] not in highlight:
                                kw[&#39;alpha&#39;] = 0.2
                        ppl.plot(k, 1e3 * (r[&#39;D47&#39;] - self.samples[r[&#39;Sample&#39;]][&#39;D47&#39;]), **kw)
                x2 = k
                x_sessions[session] = (x1+x2)/2

                ppl.axhspan(-self.repeatability[&#39;r_D47&#39;]*1000, self.repeatability[&#39;r_D47&#39;]*1000, color = &#39;k&#39;, alpha = .05, lw = 1)
                ppl.text(len(self), self.repeatability[&#39;r_D47&#39;]*1000, f&#34;   SD = {self.repeatability[&#39;r_D47&#39;]*1000:.1f} ppm&#34;, size = 9, alpha = .75, va = &#39;center&#39;)
                ppl.axhspan(-self.repeatability[&#39;r_D47&#39;]*1000*self.t95, self.repeatability[&#39;r_D47&#39;]*1000*self.t95, color = &#39;k&#39;, alpha = .05, lw = 1)
                ppl.text(len(self), self.repeatability[&#39;r_D47&#39;]*1000*self.t95, f&#34;   95% CL: ± {self.repeatability[&#39;r_D47&#39;]*1000*self.t95:.1f} ppm&#34;, size = 9, alpha = .75, va = &#39;center&#39;)

                ymax = ppl.axis()[3]
                for s in x_sessions:
                        ppl.text(
                                x_sessions[s],
                                ymax +1,
                                s,
                                va = &#39;bottom&#39;,
                                **(
                                        dict(ha = &#39;center&#39;)
                                        if len(self.sessions[s][&#39;data&#39;]) &gt; (0.15 * len(self))
                                        else dict(ha = &#39;left&#39;, rotation = 45)
                                        )
                                )

                for s in colors:
                        kw[&#39;marker&#39;] = &#39;+&#39;
                        kw[&#39;ms&#39;] = 5
                        kw[&#39;mec&#39;] = colors[s]
                        kw[&#39;label&#39;] = s
                        kw[&#39;alpha&#39;] = 1
                        ppl.plot([], [], **kw)

                kw[&#39;mec&#39;] = (0,0,0)

                if one_or_more_singlets:
                        kw[&#39;marker&#39;] = &#39;x&#39;
                        kw[&#39;ms&#39;] = 4
                        kw[&#39;alpha&#39;] = .2
                        kw[&#39;label&#39;] = &#39;other (N$\\,$=$\\,$1)&#39; if one_or_more_multiplets else &#39;other&#39;
                        ppl.plot([], [], **kw)

                if one_or_more_multiplets:
                        kw[&#39;marker&#39;] = &#39;+&#39;
                        kw[&#39;ms&#39;] = 4
                        kw[&#39;alpha&#39;] = 1
                        kw[&#39;label&#39;] = &#39;other (N$\\,$&gt;$\\,$1)&#39; if one_or_more_singlets else &#39;other&#39;
                        ppl.plot([], [], **kw)

                ppl.legend(loc = &#39;lower left&#39;, bbox_to_anchor = (1.03, 0), borderaxespad = 0)
                ppl.xticks([])
                ppl.ylabel(&#39;Δ$_{47}$ residuals (ppm)&#39;)
                ppl.axis([-1, len(self), None, None])

                if not os.path.exists(dir):
                        os.makedirs(dir)
                if filename is None:
                        return fig
                elif filename == &#39;&#39;:
                        filename = f&#39;D{self._4x}_residuals.pdf&#39;
                ppl.savefig(f&#39;{dir}/{filename}&#39;)
                ppl.close(fig)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.plot_sessions"><code class="name flex">
<span>def <span class="ident">plot_sessions</span></span>(<span>self, dir='output', figsize=(8, 8))</span>
</code></dt>
<dd>
<div class="desc"><p>Generate session plots and save them to disk.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>dir</code>: the directory in which to save the plots</li>
<li><code>figsize</code>: the width and height (in inches) of each plot</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_sessions(self, dir = &#39;output&#39;, figsize = (8,8)):
        &#39;&#39;&#39;
        Generate session plots and save them to disk.

        __Parameters__

        + `dir`: the directory in which to save the plots
        + `figsize`: the width and height (in inches) of each plot
        &#39;&#39;&#39;
        if not os.path.exists(dir):
                os.makedirs(dir)

        for session in self.sessions:
                sp = self.plot_single_session(session, xylimits = &#39;constant&#39;)
                ppl.savefig(f&#39;{dir}/D{self._4x}_plot_{session}.pdf&#39;)
                ppl.close(sp.fig)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.plot_single_session"><code class="name flex">
<span>def <span class="ident">plot_single_session</span></span>(<span>self, session, kw_plot_anchors={'ls': 'None', 'marker': 'x', 'mec': (0.75, 0, 0), 'mew': 0.75, 'ms': 4}, kw_plot_unknowns={'ls': 'None', 'marker': 'x', 'mec': (0, 0, 0.75), 'mew': 0.75, 'ms': 4}, kw_plot_anchor_avg={'ls': '-', 'marker': 'None', 'color': (0.75, 0, 0), 'lw': 0.75}, kw_plot_unknown_avg={'ls': '-', 'marker': 'None', 'color': (0, 0, 0.75), 'lw': 0.75}, kw_contour_error={'colors': [[0, 0, 0]], 'alpha': 0.5, 'linewidths': 0.75}, xylimits='free', x_label=None, y_label=None, error_contour_interval='auto', fig='new')</span>
</code></dt>
<dd>
<div class="desc"><p>Generate plot for a single session</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_single_session(self,
        session,
        kw_plot_anchors = dict(ls=&#39;None&#39;, marker=&#39;x&#39;, mec=(.75, 0, 0), mew = .75, ms = 4),
        kw_plot_unknowns = dict(ls=&#39;None&#39;, marker=&#39;x&#39;, mec=(0, 0, .75), mew = .75, ms = 4),
        kw_plot_anchor_avg = dict(ls=&#39;-&#39;, marker=&#39;None&#39;, color=(.75, 0, 0), lw = .75),
        kw_plot_unknown_avg = dict(ls=&#39;-&#39;, marker=&#39;None&#39;, color=(0, 0, .75), lw = .75),
        kw_contour_error = dict(colors = [[0, 0, 0]], alpha = .5, linewidths = 0.75),
        xylimits = &#39;free&#39;, # | &#39;constant&#39;
        x_label = None,
        y_label = None,
        error_contour_interval = &#39;auto&#39;,
        fig = &#39;new&#39;,
        ):
        &#39;&#39;&#39;
        Generate plot for a single session
        &#39;&#39;&#39;
        if x_label is None:
                x_label = f&#39;δ$_{{{self._4x}}}$ (‰)&#39;,
        if y_label is None:
                y_label = f&#39;Δ$_{{{self._4x}}}$ (‰)&#39;,

        out = SessionPlot()
        anchors = [a for a in self.anchors if [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == a]]
        unknowns = [u for u in self.unknowns if [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == u]]
        
        if fig == &#39;new&#39;:
                out.fig = ppl.figure(figsize = (6,6))
                ppl.subplots_adjust(.1,.1,.9,.9)

        out.anchor_analyses, = ppl.plot(
                [r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.anchors],
                [r[f&#39;D{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.anchors],
                **kw_plot_anchors)
        out.unknown_analyses, = ppl.plot(
                [r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.unknowns],
                [r[f&#39;D{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] in self.unknowns],
                **kw_plot_unknowns)
        out.anchor_avg = ppl.plot(
                np.array([ np.array([
                        np.min([r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]) - 1,
                        np.max([r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]) + 1
                        ]) for sample in anchors]).T,
                np.array([ np.array([0, 0]) + self.Nominal_D4x[sample] for sample in anchors]).T,
                &#39;-&#39;, **kw_plot_anchor_avg)
        out.unknown_avg = ppl.plot(
                np.array([ np.array([
                        np.min([r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]) - 1,
                        np.max([r[f&#39;d{self._4x}&#39;] for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample]) + 1
                        ]) for sample in unknowns]).T,
                np.array([ np.array([0, 0]) + self.unknowns[sample][f&#39;D{self._4x}&#39;] for sample in unknowns]).T,
                &#39;-&#39;, **kw_plot_unknown_avg)
        if xylimits == &#39;constant&#39;:
                x = [r[f&#39;d{self._4x}&#39;] for r in self]
                y = [r[f&#39;D{self._4x}&#39;] for r in self]
                x1, x2, y1, y2 = np.min(x), np.max(x), np.min(y), np.max(y)
                w, h = x2-x1, y2-y1
                x1 -= w/20
                x2 += w/20
                y1 -= h/20
                y2 += h/20
                ppl.axis([x1, x2, y1, y2])
        elif xylimits == &#39;free&#39;:
                x1, x2, y1, y2 = ppl.axis()
        else:
                x1, x2, y1, y2 = ppl.axis(xylimits)
                        
        if error_contour_interval != &#39;none&#39;:
                xi, yi = np.linspace(x1, x2), np.linspace(y1, y2)
                XI,YI = np.meshgrid(xi, yi)
                SI = np.array([[self.standardization_error(session, x, y) for x in xi] for y in yi])
                if error_contour_interval == &#39;auto&#39;:
                        rng = np.max(SI) - np.min(SI)
                        if rng &lt;= 0.01:
                                cinterval = 0.001
                        elif rng &lt;= 0.03:
                                cinterval = 0.004
                        elif rng &lt;= 0.1:
                                cinterval = 0.01
                        elif rng &lt;= 0.3:
                                cinterval = 0.03
                        elif rng &lt;= 1.:
                                cinterval = 0.1
                        else:
                                cinterval = 0.5
                else:
                        cinterval = error_contour_interval

                cval = np.arange(np.ceil(SI.min() / .001) * .001, np.ceil(SI.max() / .001 + 1) * .001, cinterval)
                out.contour = ppl.contour(XI, YI, SI, cval, **kw_contour_error)
                out.clabel = ppl.clabel(out.contour)

        ppl.xlabel(x_label)
        ppl.ylabel(y_label)
        ppl.title(session, weight = &#39;bold&#39;)
        ppl.grid(alpha = .2)
        out.ax = ppl.gca()              

        return out</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>self, filename, sep='', session='')</span>
</code></dt>
<dd>
<div class="desc"><p>Read file in csv format to load data into a <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> object.</p>
<p>In the csv file, spaces before and after field separators (<code>','</code> by default)
are optional. Each line corresponds to a single analysis.</p>
<p>The required fields are:</p>
<ul>
<li><code>UID</code>: a unique identifier</li>
<li><code>Session</code>: an identifier for the analytical session</li>
<li><code>Sample</code>: a sample identifier</li>
<li><code>d45</code>, <code>d46</code>, and at least one of <code>d47</code> or <code>d48</code>: the working-gas delta values</li>
</ul>
<p>Independently known oxygen-17 anomalies may be provided as <code>D17O</code> (in ‰ relative to
VSMOW, λ = <code>self.lambda_17</code>), and are otherwise assumed to be zero. Working-gas deltas <code>d47</code>, <code>d48</code>
and <code>d49</code> are optional, and set to NaN by default.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>fileneme</code>: the path of the file to read</li>
<li><code>sep</code>: csv separator delimiting the fields</li>
<li><code>session</code>: set <code>Session</code> field to this string for all analyses</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read(self, filename, sep = &#39;&#39;, session = &#39;&#39;):
        &#39;&#39;&#39;
        Read file in csv format to load data into a `D47data` object.

        In the csv file, spaces before and after field separators (`&#39;,&#39;` by default)
        are optional. Each line corresponds to a single analysis.

        The required fields are:

        + `UID`: a unique identifier
        + `Session`: an identifier for the analytical session
        + `Sample`: a sample identifier
        + `d45`, `d46`, and at least one of `d47` or `d48`: the working-gas delta values

        Independently known oxygen-17 anomalies may be provided as `D17O` (in ‰ relative to
        VSMOW, λ = `self.lambda_17`), and are otherwise assumed to be zero. Working-gas deltas `d47`, `d48`
        and `d49` are optional, and set to NaN by default.

        __Parameters__

        + `fileneme`: the path of the file to read
        + `sep`: csv separator delimiting the fields
        + `session`: set `Session` field to this string for all analyses
        &#39;&#39;&#39;
        with open(filename) as fid:
                self.input(fid.read(), sep = sep, session = session)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.refresh"><code class="name flex">
<span>def <span class="ident">refresh</span></span>(<span>self, session='mySession')</span>
</code></dt>
<dd>
<div class="desc"><p>Update <code>self.sessions</code>, <code>self.samples</code>, <code>self.anchors</code>, and <code>self.unknowns</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refresh(self, session = &#39;mySession&#39;):
        &#39;&#39;&#39;
        Update `self.sessions`, `self.samples`, `self.anchors`, and `self.unknowns`.
        &#39;&#39;&#39;
        self.fill_in_missing_info(session = session)
        self.refresh_sessions()
        self.refresh_samples()</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.refresh_samples"><code class="name flex">
<span>def <span class="ident">refresh_samples</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Define <code>self.samples</code>, <code>self.anchors</code>, and <code>self.unknowns</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refresh_samples(self):
        &#39;&#39;&#39;
        Define `self.samples`, `self.anchors`, and `self.unknowns`.
        &#39;&#39;&#39;
        self.samples = {
                s: {&#39;data&#39;: [r for r in self if r[&#39;Sample&#39;] == s]}
                for s in sorted({r[&#39;Sample&#39;] for r in self})
                }
        self.anchors = {s: self.samples[s] for s in self.samples if s in self.Nominal_D4x}
        self.unknowns = {s: self.samples[s] for s in self.samples if s not in self.Nominal_D4x}</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.refresh_sessions"><code class="name flex">
<span>def <span class="ident">refresh_sessions</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update <code>self.sessions</code> and set <code>scrambling_drift</code>, <code>slope_drift</code>, and <code>wg_drift</code>
to <code>False</code> for all sessions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refresh_sessions(self):
        &#39;&#39;&#39;
        Update `self.sessions` and set `scrambling_drift`, `slope_drift`, and `wg_drift`
        to `False` for all sessions.
        &#39;&#39;&#39;
        self.sessions = {
                s: {&#39;data&#39;: [r for r in self if r[&#39;Session&#39;] == s]}
                for s in sorted({r[&#39;Session&#39;] for r in self})
                }
        for s in self.sessions:
                self.sessions[s][&#39;scrambling_drift&#39;] = False
                self.sessions[s][&#39;slope_drift&#39;] = False
                self.sessions[s][&#39;wg_drift&#39;] = False
                self.sessions[s][&#39;d13C_standardization_method&#39;] = self.d13C_STANDARDIZATION_METHOD
                self.sessions[s][&#39;d18O_standardization_method&#39;] = self.d18O_STANDARDIZATION_METHOD</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.repeatabilities"><code class="name flex">
<span>def <span class="ident">repeatabilities</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute analytical repeatabilities for δ<sup>13</sup>C<sub>VPDB</sub>,
δ<sup>18</sup>O<sub>VSMOW</sub>, Δ<sub>47</sub> (for all samples, for anchors,
and for unknowns).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@make_verbal
def repeatabilities(self):
        &#39;&#39;&#39;
        Compute analytical repeatabilities for δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt;,
        δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt;, Δ&lt;sub&gt;47&lt;/sub&gt; (for all samples, for anchors,
        and for unknowns).
        &#39;&#39;&#39;
        self.msg(&#39;Computing reproducibilities for all sessions&#39;)

        self.repeatability[&#39;r_d13C_VPDB&#39;] = self.compute_r(&#39;d13C_VPDB&#39;, samples = &#39;anchors&#39;)
        self.repeatability[&#39;r_d18O_VSMOW&#39;] = self.compute_r(&#39;d18O_VSMOW&#39;, samples = &#39;anchors&#39;)
        self.repeatability[f&#39;r_D{self._4x}a&#39;] = self.compute_r(f&#39;D{self._4x}&#39;, samples = &#39;anchors&#39;)
        self.repeatability[f&#39;r_D{self._4x}u&#39;] = self.compute_r(f&#39;D{self._4x}&#39;, samples = &#39;unknowns&#39;)
        self.repeatability[f&#39;r_D{self._4x}&#39;] = self.compute_r(f&#39;D{self._4x}&#39;, samples = &#39;all samples&#39;)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.report"><code class="name flex">
<span>def <span class="ident">report</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Prints a report on the standardization fit.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def report(self):
        &#39;&#39;&#39;
        Prints a report on the standardization fit.
        &#39;&#39;&#39;
        report_fit(self.standardization)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.rmswd"><code class="name flex">
<span>def <span class="ident">rmswd</span></span>(<span>self, samples='all samples', sessions='all sessions')</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the root mean squared weighted deviation, χ<sup>2</sup> and
corresponding degrees of freedom of <code>[r['D47'] for r in self]</code>.</p>
<p>Currently used only in <code>D4xdata.standardize(method = 'indep_sessions')</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@make_verbal
def rmswd(self,
        samples = &#39;all samples&#39;,
        sessions = &#39;all sessions&#39;,
        ):
        &#39;&#39;&#39;
        Compute the root mean squared weighted deviation, χ&lt;sup&gt;2&lt;/sup&gt; and
        corresponding degrees of freedom of `[r[&#39;D47&#39;] for r in self]`.
        
        Currently used only in `D4xdata.standardize(method = &#39;indep_sessions&#39;)`
        &#39;&#39;&#39;
        if samples == &#39;all samples&#39;:
                mysamples = [k for k in self.samples]
        elif samples == &#39;anchors&#39;:
                mysamples = [k for k in self.anchors]
        elif samples == &#39;unknowns&#39;:
                mysamples = [k for k in self.unknowns]
        else:
                mysamples = samples

        if sessions == &#39;all sessions&#39;:
                sessions = [k for k in self.sessions]

        chisq, Nf = 0, 0
        for sample in mysamples :
                G = [ r for r in self if r[&#39;Sample&#39;] == sample and r[&#39;Session&#39;] in sessions ]
                if len(G) &gt; 1 :
                        X, sX = w_avg([r[f&#39;D{self._4x}&#39;] for r in G], [r[f&#39;wD{self._4x}&#39;] for r in G])
                        Nf += (len(G) - 1)
                        chisq += np.sum([ ((r[f&#39;D{self._4x}&#39;]-X)/r[f&#39;wD{self._4x}&#39;])**2 for r in G])
        r = (chisq / Nf)**.5 if Nf &gt; 0 else 0
        self.msg(f&#39;RMSWD of r[&#34;D{self._4x}&#34;] is {r:.6f} for {samples}.&#39;)
        return {&#39;rmswd&#39;: r, &#39;chisq&#39;: chisq, &#39;Nf&#39;: Nf}</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.sample_D4x_correl"><code class="name flex">
<span>def <span class="ident">sample_D4x_correl</span></span>(<span>self, sample1, sample2=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Correlation between Δ<sub>47</sub> errors of samples</p>
<p>Returns the error correlation between the average Δ47 values of two samples.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_D4x_correl(self, sample1, sample2 = None):
        &#39;&#39;&#39;
        Correlation between Δ&lt;sub&gt;47&lt;/sub&gt; errors of samples

        Returns the error correlation between the average Δ47 values of two samples.
        &#39;&#39;&#39;
        if sample2 is None or sample2 == sample1:
                return 1.
        return (
                self.sample_D4x_covar(sample1, sample2)
                / self.unknowns[sample1][f&#39;SE_D{self._4x}&#39;]
                / self.unknowns[sample2][f&#39;SE_D{self._4x}&#39;]
                )</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.sample_D4x_covar"><code class="name flex">
<span>def <span class="ident">sample_D4x_covar</span></span>(<span>self, sample1, sample2=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Covariance between Δ<sub>47</sub> values of samples</p>
<p>Returns the error covariance between the average Δ<sub>47</sub> values of two
samples. If if only <code>sample_1</code> is specified, or if <code>sample_1 == sample_2</code>),
returns the Δ<sub>47</sub> variance for that sample.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_D4x_covar(self, sample1, sample2 = None):
        &#39;&#39;&#39;
        Covariance between Δ&lt;sub&gt;47&lt;/sub&gt; values of samples

        Returns the error covariance between the average Δ&lt;sub&gt;47&lt;/sub&gt; values of two
        samples. If if only `sample_1` is specified, or if `sample_1 == sample_2`),
        returns the Δ&lt;sub&gt;47&lt;/sub&gt; variance for that sample.
        &#39;&#39;&#39;
        if sample2 is None:
                sample2 = sample1
        if self.standardization_method == &#39;pooled&#39;:
                i = self.standardization.var_names.index(f&#39;D{self._4x}_{pf(sample1)}&#39;)
                j = self.standardization.var_names.index(f&#39;D{self._4x}_{pf(sample2)}&#39;)
                return self.standardization.covar[i, j]
        elif self.standardization_method == &#39;indep_sessions&#39;:
                if sample1 == sample2:
                        return self.samples[sample1][f&#39;SE_D{self._4x}&#39;]**2
                else:
                        c = 0
                        for session in self.sessions:
                                sdata1 = [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample1]
                                sdata2 = [r for r in self.sessions[session][&#39;data&#39;] if r[&#39;Sample&#39;] == sample2]
                                if sdata1 and sdata2:
                                        a = self.sessions[session][&#39;a&#39;]
                                        # !! TODO: CM below does not account for temporal changes in standardization parameters
                                        CM = self.sessions[session][&#39;CM&#39;][:3,:3]
                                        avg_D4x_1 = np.mean([r[f&#39;D{self._4x}&#39;] for r in sdata1])
                                        avg_d4x_1 = np.mean([r[f&#39;d{self._4x}&#39;] for r in sdata1])
                                        avg_D4x_2 = np.mean([r[f&#39;D{self._4x}&#39;] for r in sdata2])
                                        avg_d4x_2 = np.mean([r[f&#39;d{self._4x}&#39;] for r in sdata2])
                                        c += (
                                                self.unknowns[sample1][f&#39;session_D{self._4x}&#39;][session][2]
                                                * self.unknowns[sample2][f&#39;session_D{self._4x}&#39;][session][2]
                                                * np.array([[avg_D4x_1, avg_d4x_1, 1]])
                                                @ CM
                                                @ np.array([[avg_D4x_2, avg_d4x_2, 1]]).T
                                                ) / a**2
                        return float(c)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.sample_average"><code class="name flex">
<span>def <span class="ident">sample_average</span></span>(<span>self, samples, weights='equal', normalize=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Weighted average Δ<sub>47</sub> value of a group of samples, accounting for covariance.</p>
<p>Returns the weighed average Δ47 value and associated SE
of a group of samples. Weights are equal by default. If <code>normalize</code> is
true, <code>weights</code> will be rescaled so that their sum equals 1.</p>
<p><strong>Examples</strong></p>
<pre><code class="language-python">self.sample_average(['X','Y'], [1, 2])
</code></pre>
<p>returns the value and SE of [Δ<sub>47</sub>(X) + 2 Δ<sub>47</sub>(Y)]/3,
where Δ<sub>47</sub>(X) and Δ<sub>47</sub>(Y) are the average Δ<sub>47</sub>
values of samples X and Y, respectively.</p>
<pre><code class="language-python">self.sample_average(['X','Y'], [1, -1], normalize = False)
</code></pre>
<p>returns the value and SE of the difference Δ<sub>47</sub>(X) - Δ<sub>47</sub>(Y).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">        def sample_average(self, samples, weights = &#39;equal&#39;, normalize = True):
                &#39;&#39;&#39;
                Weighted average Δ&lt;sub&gt;47&lt;/sub&gt; value of a group of samples, accounting for covariance.

                Returns the weighed average Δ47 value and associated SE
                of a group of samples. Weights are equal by default. If `normalize` is
                true, `weights` will be rescaled so that their sum equals 1.

                __Examples__

                ```python
                self.sample_average([&#39;X&#39;,&#39;Y&#39;], [1, 2])
                ```

                returns the value and SE of [Δ&lt;sub&gt;47&lt;/sub&gt;(X) + 2 Δ&lt;sub&gt;47&lt;/sub&gt;(Y)]/3,
                where Δ&lt;sub&gt;47&lt;/sub&gt;(X) and Δ&lt;sub&gt;47&lt;/sub&gt;(Y) are the average Δ&lt;sub&gt;47&lt;/sub&gt;
                values of samples X and Y, respectively.

                ```python
                self.sample_average([&#39;X&#39;,&#39;Y&#39;], [1, -1], normalize = False)
                ```

                returns the value and SE of the difference Δ&lt;sub&gt;47&lt;/sub&gt;(X) - Δ&lt;sub&gt;47&lt;/sub&gt;(Y).
                &#39;&#39;&#39;
                if weights == &#39;equal&#39;:
                        weights = [1/len(samples)] * len(samples)

                if normalize:
                        s = sum(weights)
                        if s:
                                weights = [w/s for w in weights]

                try:
#                       indices = [self.standardization.var_names.index(f&#39;D47_{pf(sample)}&#39;) for sample in samples]
#                       C = self.standardization.covar[indices,:][:,indices]
                        C = np.array([[self.sample_D4x_covar(x, y) for x in samples] for y in samples])
                        X = [self.samples[sample][f&#39;D{self._4x}&#39;] for sample in samples]
                        return correlated_sum(X, C, weights)
                except ValueError:
                        return (0., 0.)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.simulate"><code class="name flex">
<span>def <span class="ident">simulate</span></span>(<span>self, samples=None, a=1.0, b=0.0, c=-0.9, rD4x=0.015, seed=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Populate <code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code> instance with simulated analyses from a single session.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>samples</code>: a list of entries; each entry is a dictionary with the following fields:<ul>
<li><code>Sample</code>: the name of the sample</li>
<li>either <code>d47</code> (the δ<sub>47</sub> value of this sample), or <code>d13C_VPDB</code> and <code>d18O_VPDB</code> (its δ<sup>13</sup>C<sub>VPDB</sub> and δ<sup>18</sup>O<sub>VPDB</sub> values)</li>
<li><code>D47</code>: the absolute Δ<sub>47</sub> value of this sample</li>
<li><code>N</code>: how many analyses of this sample should be generated</li>
</ul>
</li>
<li><code>a</code>: scrambling factor)</li>
<li><code>b</code>: compositional nonlinearity</li>
<li><code>c</code>: working gas offset</li>
<li><code>rD47</code>: Δ<sub>47</sub> repeatability</li>
<li><code>seed</code>: explicitly set to a non-zero value to achieve random but repeatable simulations</li>
</ul>
<p>Beware that <code>d47</code> values computed from <code>d13C_VPDB</code> and <code>d18O_VPDB</code> are calculated assuming
a working gas with δ<sup>13</sup>C<sub>VPDB</sub>&nbsp;=&nbsp;0 and δ<sup>18</sup>O<sub>VSMOW</sub>&nbsp;=&nbsp;0.
In the unusual case where simulating a different working gas composition is necessary, <code>d47</code> must be specified explicitly.</p>
<p>Samples already defined in <code><a title="D47crunch.D47data.Nominal_d13C_VPDB" href="#D47crunch.D4xdata.Nominal_d13C_VPDB">D47data.Nominal_d13C_VPDB</a></code>, <code><a title="D47crunch.D47data.Nominal_d18O_VPDB" href="#D47crunch.D4xdata.Nominal_d18O_VPDB">D47data.Nominal_d18O_VPDB</a></code>, and <code><a title="D47crunch.D47data.Nominal_D47" href="#D47crunch.D47data.Nominal_D47">D47data.Nominal_D47</a></code>
do not require explicit <code>d47</code>, <code>D47</code>, <code>d13C_VPDB</code> nor <code>d18O_VPDB</code> (the nominal values will be used by default).</p>
<p>Here is an example of using this method to simulate a given combination of anchors and unknowns:</p>
<pre><code class="language-py">import D47crunch
D = D47crunch.D47data()
D.simulate([
    dict(Sample = 'ETH-1', N = 6),
    dict(Sample = 'ETH-2', N = 6),
    dict(Sample = 'ETH-3', N = 12),
    dict(Sample = 'FOO', d13C_VPDB = -5., d18O_VPDB = -10., D47 = 0.3, N = 4),
    ], rD47 = 0.010)
D.standardize()
D.plot_sessions()
D.verbose = True
D.table_of_samples()
</code></pre>
<p>This should output something like:</p>
<pre><code>[table_of_samples] 
––––––  ––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
Sample   N  d13C_VPDB  d18O_VSMOW     D47      SE    95% CL      SD  p_Levene
––––––  ––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
ETH-1    6        nan         nan  0.2052                    0.0076          
ETH-2    6        nan         nan  0.2085                    0.0089          
ETH-3   12        nan         nan  0.6132                    0.0118          
FOO      4        nan         nan  0.3031  0.0057  ± 0.0118  0.0104     0.572
––––––  ––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulate(self,
        samples = None,
        a = 1.,
        b = 0.,
        c = -0.9,
        rD4x = 0.015,
        seed = 0,
        ):
        &#39;&#39;&#39;
        Populate `D47data` instance with simulated analyses from a single session.
        
        __Parameters__

        + `samples`: a list of entries; each entry is a dictionary with the following fields:
            * `Sample`: the name of the sample
            * either `d47` (the δ&lt;sub&gt;47&lt;/sub&gt; value of this sample), or `d13C_VPDB` and `d18O_VPDB` (its δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt; and δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VPDB&lt;/sub&gt; values)
            * `D47`: the absolute Δ&lt;sub&gt;47&lt;/sub&gt; value of this sample
            * `N`: how many analyses of this sample should be generated
        + `a`: scrambling factor)
        + `b`: compositional nonlinearity
        + `c`: working gas offset
        + `rD47`: Δ&lt;sub&gt;47&lt;/sub&gt; repeatability
        + `seed`: explicitly set to a non-zero value to achieve random but repeatable simulations
        
        Beware that `d47` values computed from `d13C_VPDB` and `d18O_VPDB` are calculated assuming
        a working gas with δ&lt;sup&gt;13&lt;/sup&gt;C&lt;sub&gt;VPDB&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;0 and δ&lt;sup&gt;18&lt;/sup&gt;O&lt;sub&gt;VSMOW&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;0.
        In the unusual case where simulating a different working gas composition is necessary, `d47` must be specified explicitly.
        
        Samples already defined in `D47data.Nominal_d13C_VPDB`, `D47data.Nominal_d18O_VPDB`, and `D47data.Nominal_D47`
        do not require explicit `d47`, `D47`, `d13C_VPDB` nor `d18O_VPDB` (the nominal values will be used by default).
        
        Here is an example of using this method to simulate a given combination of anchors and unknowns:

        ````py
        import D47crunch
        D = D47crunch.D47data()
        D.simulate([
            dict(Sample = &#39;ETH-1&#39;, N = 6),
            dict(Sample = &#39;ETH-2&#39;, N = 6),
            dict(Sample = &#39;ETH-3&#39;, N = 12),
            dict(Sample = &#39;FOO&#39;, d13C_VPDB = -5., d18O_VPDB = -10., D47 = 0.3, N = 4),
            ], rD47 = 0.010)
        D.standardize()
        D.plot_sessions()
        D.verbose = True
        D.table_of_samples()
        ````
        
        This should output something like:
        
        ````
        [table_of_samples] 
        ––––––  ––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
        Sample   N  d13C_VPDB  d18O_VSMOW     D47      SE    95% CL      SD  p_Levene
        ––––––  ––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
        ETH-1    6        nan         nan  0.2052                    0.0076          
        ETH-2    6        nan         nan  0.2085                    0.0089          
        ETH-3   12        nan         nan  0.6132                    0.0118          
        FOO      4        nan         nan  0.3031  0.0057  ± 0.0118  0.0104     0.572
        ––––––  ––  –––––––––  ––––––––––  ––––––  ––––––  ––––––––  ––––––  ––––––––
        ````
        &#39;&#39;&#39;
        from numpy import random as nprandom
        if seed:
                rng = nprandom.default_rng(seed)
        else:
                rng = nprandom.default_rng()
        
        if samples is None:
                samples = [dict(Sample = s, N = 4) for s in self.Nominal_D4x]
                samples += [{
                        &#39;Sample&#39;: &#39;FOO&#39;,
                        f&#39;d{self._4x}&#39;: 0.,
                        f&#39;D{self._4x}&#39;: 0.5 if self._4x == &#39;47&#39; else 0.2,
                        &#39;N&#39;: 4,
                        }]

        N = sum([s[&#39;N&#39;] for s in samples])
        errors = rng.normal(loc = 0, scale = 1, size = N) # generate random measurement errors
        errors *= rD4x / stdev(errors) # scale errors to rD47
        
        k = 0
        for s in samples:
        
                if f&#39;D{self._4x}&#39; not in s:
                        if s[&#39;Sample&#39;] not in self.Nominal_D4x:
                                raise KeyError(f&#34;Sample {s[&#39;Sample&#39;]} is missing a D{self._4x} value and it is not defined in Nominal_D{self._4x}&#34;)
                        else:
                                s[f&#39;D{self._4x}&#39;] = self.Nominal_D4x[s[&#39;Sample&#39;]]                                       

                if f&#39;d{self._4x}&#39; not in s:

                        if &#39;d13C_VPDB&#39; not in s:
                                if s[&#39;Sample&#39;] not in self.Nominal_d13C_VPDB:
                                        raise KeyError(f&#34;Sample {s[&#39;Sample&#39;]} is missing d{self._4x} and d13C_VPDB values, and it is not defined in Nominal_d13C_VPDB.&#34;)
                                else:
                                        s[&#39;d13C_VPDB&#39;] = self.Nominal_d13C_VPDB[s[&#39;Sample&#39;]]

                        if &#39;d18O_VPDB&#39; not in s:
                                if s[&#39;Sample&#39;] not in self.Nominal_d18O_VPDB:
                                        raise KeyError(f&#34;Sample {s[&#39;Sample&#39;]} is missing d{self._4x} and d18O_VPDB values, and it is not defined in Nominal_d18O_VPDB.&#34;)
                                else:
                                        s[&#39;d18O_VPDB&#39;] = self.Nominal_d18O_VPDB[s[&#39;Sample&#39;]]

                        i = 2 if self._4x == &#39;47&#39; else 3
                        R4xwg = self.compute_isobar_ratios(self.R13_VPDB, self.R18_VPDB * self.ALPHA_18O_ACID_REACTION)[i]
                        R4xs = self.compute_isobar_ratios(
                                self.R13_VPDB * (1 + s[&#39;d13C_VPDB&#39;]/1000),
                                self.R18_VPDB * (1 + s[&#39;d18O_VPDB&#39;]/1000) * self.ALPHA_18O_ACID_REACTION,
                                )[i]*(1+s[f&#39;D{self._4x}&#39;]/1000)
                        s[f&#39;d{self._4x}&#39;] = (R4xs/R4xwg-1)*1000
                                
                while s[&#39;N&#39;]:
                        self.append({
                                &#39;Sample&#39;: s[&#39;Sample&#39;],
                                &#39;d13Cwg_VPDB&#39;: 0.,
                                &#39;d18Owg_VSMOW&#39;: (self.R18_VSMOW * self.ALPHA_18O_ACID_REACTION - 1) * 1000,
                                &#39;d13C_VPDB&#39;: self[&#39;d13C_VPDB&#39;],
                                &#39;d18O_VSMOW&#39;: self[&#39;d18O_VPDB&#39;],
                                f&#39;d{self._4x}&#39;: s[f&#39;d{self._4x}&#39;],
                                f&#39;D{self._4x}raw&#39;: a * (s[f&#39;D{self._4x}&#39;] + errors[k]) + b * s[f&#39;d{self._4x}&#39;] + c,
                                })
                        s[&#39;N&#39;] -= 1
                        k += 1

        self.refresh()</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.split_samples"><code class="name flex">
<span>def <span class="ident">split_samples</span></span>(<span>self, samples_to_split='all', grouping='by_session')</span>
</code></dt>
<dd>
<div class="desc"><p>Split unknown samples by UID (treat all analyses as different samples)
or by session (treat analyses of a given sample in different sessions as
different samples).</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>samples_to_split</code>: a list of samples to split, e.g., <code>['IAEA-C1', 'IAEA-C2']</code></li>
<li><code>grouping</code>: <code>by_uid</code> | <code>by_session</code></li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split_samples(self, samples_to_split = &#39;all&#39;, grouping = &#39;by_session&#39;):
        &#39;&#39;&#39;
        Split unknown samples by UID (treat all analyses as different samples)
        or by session (treat analyses of a given sample in different sessions as
        different samples).

        __Parameters__

        + `samples_to_split`: a list of samples to split, e.g., `[&#39;IAEA-C1&#39;, &#39;IAEA-C2&#39;]`
        + `grouping`: `by_uid` | `by_session`
        &#39;&#39;&#39;
        if samples_to_split == &#39;all&#39;:
                samples_to_split = [s for s in self.unknowns]
        gkeys = {&#39;by_uid&#39;:&#39;UID&#39;, &#39;by_session&#39;:&#39;Session&#39;}
        self.grouping = grouping.lower()
        if self.grouping in gkeys:
                gkey = gkeys[self.grouping]
        for r in self:
                if r[&#39;Sample&#39;] in samples_to_split:
                        r[&#39;Sample_original&#39;] = r[&#39;Sample&#39;]
                        r[&#39;Sample&#39;] = f&#34;{r[&#39;Sample&#39;]}__{r[gkey]}&#34;
                elif r[&#39;Sample&#39;] in self.unknowns:
                        r[&#39;Sample_original&#39;] = r[&#39;Sample&#39;]
        self.refresh_samples()</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.standardization_error"><code class="name flex">
<span>def <span class="ident">standardization_error</span></span>(<span>self, session, d4x, D4x, t=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute standardization error for a given session and
(δ<sub>47</sub>, Δ<sub>47</sub>) composition.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">        def standardization_error(self, session, d4x, D4x, t = 0):
                &#39;&#39;&#39;
                Compute standardization error for a given session and
                (δ&lt;sub&gt;47&lt;/sub&gt;, Δ&lt;sub&gt;47&lt;/sub&gt;) composition.
                &#39;&#39;&#39;
                a = self.sessions[session][&#39;a&#39;]
                b = self.sessions[session][&#39;b&#39;]
                c = self.sessions[session][&#39;c&#39;]
                a2 = self.sessions[session][&#39;a2&#39;]
                b2 = self.sessions[session][&#39;b2&#39;]
                c2 = self.sessions[session][&#39;c2&#39;]
                CM = self.sessions[session][&#39;CM&#39;]

                x, y = D4x, d4x
                z = a * x + b * y + c + a2 * x * t + b2 * y * t + c2 * t
#               x = (z - b*y - b2*y*t - c - c2*t) / (a+a2*t)
                dxdy = -(b+b2*t) / (a+a2*t)
                dxdz = 1. / (a+a2*t)
                dxda = -x / (a+a2*t)
                dxdb = -y / (a+a2*t)
                dxdc = -1. / (a+a2*t)
                dxda2 = -x * a2 / (a+a2*t)
                dxdb2 = -y * t / (a+a2*t)
                dxdc2 = -t / (a+a2*t)
                V = np.array([dxda, dxdb, dxdc, dxda2, dxdb2, dxdc2])
                sx = (V @ CM @ V.T) ** .5
                return sx</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.standardize"><code class="name flex">
<span>def <span class="ident">standardize</span></span>(<span>self, method='pooled', weighted_sessions=[], consolidate=True, consolidate_tables=False, consolidate_plots=False, constraints={})</span>
</code></dt>
<dd>
<div class="desc"><p>Compute absolute Δ<sub>47</sub> values for all replicate analyses and for sample averages.
If <code>method</code> argument is set to <code>'pooled'</code>, the standardization processes all sessions
in a single step, assuming that all samples (anchors and unknowns alike) are
homogeneous (i.e. that their true Δ<sub>47</sub> value does not change between sessions).
If <code>method</code> argument is set to <code>'indep_sessions'</code>, the standardization processes each
session independently, based only on anchors analyses.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">        @make_verbal
        def standardize(self,
                method = &#39;pooled&#39;,
                weighted_sessions = [],
                consolidate = True,
                consolidate_tables = False,
                consolidate_plots = False,
                constraints = {},
                ):
                &#39;&#39;&#39;
                Compute absolute Δ&lt;sub&gt;47&lt;/sub&gt; values for all replicate analyses and for sample averages.
                If `method` argument is set to `&#39;pooled&#39;`, the standardization processes all sessions
                in a single step, assuming that all samples (anchors and unknowns alike) are
                homogeneous (i.e. that their true Δ&lt;sub&gt;47&lt;/sub&gt; value does not change between sessions).
                If `method` argument is set to `&#39;indep_sessions&#39;`, the standardization processes each
                session independently, based only on anchors analyses.
                &#39;&#39;&#39;

                self.standardization_method = method
                self.assign_timestamps()

                if method == &#39;pooled&#39;:
                        if weighted_sessions:
                                for session_group in weighted_sessions:
                                        X = D4xdata([r for r in self if r[&#39;Session&#39;] in session_group], mass = self._4x)
                                        X.Nominal_D4x = self.Nominal_D4x.copy()
                                        X.refresh()
                                        result = X.standardize(method = &#39;pooled&#39;, weighted_sessions = [], consolidate = False)
                                        w = np.sqrt(result.redchi)
                                        self.msg(f&#39;Session group {session_group} MRSWD = {w:.4f}&#39;)
                                        for r in X:
                                                r[f&#39;wD{self._4x}raw&#39;] *= w
                        else:
                                self.msg(f&#39;All D{self._4x}raw weights set to 1 ‰&#39;)
                                for r in self:
                                        r[f&#39;wD{self._4x}raw&#39;] = 1.

                        params = Parameters()
                        for k,session in enumerate(self.sessions):
                                self.msg(f&#34;Session {session}: scrambling_drift is {self.sessions[session][&#39;scrambling_drift&#39;]}.&#34;)
                                self.msg(f&#34;Session {session}: slope_drift is {self.sessions[session][&#39;slope_drift&#39;]}.&#34;)
                                self.msg(f&#34;Session {session}: wg_drift is {self.sessions[session][&#39;wg_drift&#39;]}.&#34;)
                                s = pf(session)
                                params.add(f&#39;a_{s}&#39;, value = 0.9)
                                params.add(f&#39;b_{s}&#39;, value = 0.)
                                params.add(f&#39;c_{s}&#39;, value = -0.9)
                                params.add(f&#39;a2_{s}&#39;, value = 0., vary = self.sessions[session][&#39;scrambling_drift&#39;])
                                params.add(f&#39;b2_{s}&#39;, value = 0., vary = self.sessions[session][&#39;slope_drift&#39;])
                                params.add(f&#39;c2_{s}&#39;, value = 0., vary = self.sessions[session][&#39;wg_drift&#39;])
                        for sample in self.unknowns:
                                params.add(f&#39;D{self._4x}_{pf(sample)}&#39;, value = 0.5)

                        for k in constraints:
                                params[k].expr = constraints[k]

                        def residuals(p):
                                R = []
                                for r in self:
                                        session = pf(r[&#39;Session&#39;])
                                        sample = pf(r[&#39;Sample&#39;])
                                        if r[&#39;Sample&#39;] in self.Nominal_D4x:
                                                R += [ (
                                                        r[f&#39;D{self._4x}raw&#39;] - (
                                                                p[f&#39;a_{session}&#39;] * self.Nominal_D4x[r[&#39;Sample&#39;]]
                                                                + p[f&#39;b_{session}&#39;] * r[f&#39;d{self._4x}&#39;]
                                                                +       p[f&#39;c_{session}&#39;]
                                                                + r[&#39;t&#39;] * (
                                                                        p[f&#39;a2_{session}&#39;] * self.Nominal_D4x[r[&#39;Sample&#39;]]
                                                                        + p[f&#39;b2_{session}&#39;] * r[f&#39;d{self._4x}&#39;]
                                                                        +       p[f&#39;c2_{session}&#39;]
                                                                        )
                                                                )
                                                        ) / r[f&#39;wD{self._4x}raw&#39;] ]
                                        else:
                                                R += [ (
                                                        r[f&#39;D{self._4x}raw&#39;] - (
                                                                p[f&#39;a_{session}&#39;] * p[f&#39;D{self._4x}_{sample}&#39;]
                                                                + p[f&#39;b_{session}&#39;] * r[f&#39;d{self._4x}&#39;]
                                                                +       p[f&#39;c_{session}&#39;]
                                                                + r[&#39;t&#39;] * (
                                                                        p[f&#39;a2_{session}&#39;] * p[f&#39;D{self._4x}_{sample}&#39;]
                                                                        + p[f&#39;b2_{session}&#39;] * r[f&#39;d{self._4x}&#39;]
                                                                        +       p[f&#39;c2_{session}&#39;]
                                                                        )
                                                                )
                                                        ) / r[f&#39;wD{self._4x}raw&#39;] ]
                                return R

                        M = Minimizer(residuals, params)
                        result = M.least_squares()
                        self.Nf = result.nfree
                        self.t95 = tstudent.ppf(1 - 0.05/2, self.Nf)
#                       if self.verbose:
#                               report_fit(result)

                        for r in self:
                                s = pf(r[&#34;Session&#34;])
                                a = result.params.valuesdict()[f&#39;a_{s}&#39;]
                                b = result.params.valuesdict()[f&#39;b_{s}&#39;]
                                c = result.params.valuesdict()[f&#39;c_{s}&#39;]
                                a2 = result.params.valuesdict()[f&#39;a2_{s}&#39;]
                                b2 = result.params.valuesdict()[f&#39;b2_{s}&#39;]
                                c2 = result.params.valuesdict()[f&#39;c2_{s}&#39;]
                                r[f&#39;D{self._4x}&#39;] = (r[f&#39;D{self._4x}raw&#39;] - c - b * r[f&#39;d{self._4x}&#39;] - c2 * r[&#39;t&#39;] - b2 * r[&#39;t&#39;] * r[f&#39;d{self._4x}&#39;]) / (a + a2 * r[&#39;t&#39;])

                        self.standardization = result

                        for session in self.sessions:
                                self.sessions[session][&#39;Np&#39;] = 3
                                for k in [&#39;scrambling&#39;, &#39;slope&#39;, &#39;wg&#39;]:
                                        if self.sessions[session][f&#39;{k}_drift&#39;]:
                                                self.sessions[session][&#39;Np&#39;] += 1

                        if consolidate:
                                self.consolidate(tables = consolidate_tables, plots = consolidate_plots)
                        return result


                elif method == &#39;indep_sessions&#39;:

                        if weighted_sessions:
                                for session_group in weighted_sessions:
                                        X = D4xdata([r for r in self if r[&#39;Session&#39;] in session_group], mass = self._4x)
                                        X.Nominal_D4x = self.Nominal_D4x.copy()
                                        X.refresh()
                                        # This is only done to assign r[&#39;wD47raw&#39;] for r in X:
                                        X.standardize(method = method, weighted_sessions = [], consolidate = False)
                                        self.msg(f&#39;D{self._4x}raw weights set to {1000*X[0][f&#34;wD{self._4x}raw&#34;]:.1f} ppm for sessions in {session_group}&#39;)
                        else:
                                self.msg(&#39;All weights set to 1 ‰&#39;)
                                for r in self:
                                        r[f&#39;wD{self._4x}raw&#39;] = 1

                        for session in self.sessions:
                                s = self.sessions[session]
                                p_names = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;a2&#39;, &#39;b2&#39;, &#39;c2&#39;]
                                p_active = [True, True, True, s[&#39;scrambling_drift&#39;], s[&#39;slope_drift&#39;], s[&#39;wg_drift&#39;]]
                                s[&#39;Np&#39;] = sum(p_active)
                                sdata = s[&#39;data&#39;]

                                A = np.array([
                                        [
                                                self.Nominal_D4x[r[&#39;Sample&#39;]] / r[f&#39;wD{self._4x}raw&#39;],
                                                r[f&#39;d{self._4x}&#39;] / r[f&#39;wD{self._4x}raw&#39;],
                                                1 / r[f&#39;wD{self._4x}raw&#39;],
                                                self.Nominal_D4x[r[&#39;Sample&#39;]] * r[&#39;t&#39;] / r[f&#39;wD{self._4x}raw&#39;],
                                                r[f&#39;d{self._4x}&#39;] * r[&#39;t&#39;] / r[f&#39;wD{self._4x}raw&#39;],
                                                r[&#39;t&#39;] / r[f&#39;wD{self._4x}raw&#39;]
                                                ]
                                        for r in sdata if r[&#39;Sample&#39;] in self.anchors
                                        ])[:,p_active] # only keep columns for the active parameters
                                Y = np.array([[r[f&#39;D{self._4x}raw&#39;] / r[f&#39;wD{self._4x}raw&#39;]] for r in sdata if r[&#39;Sample&#39;] in self.anchors])
                                s[&#39;Na&#39;] = Y.size
                                CM = linalg.inv(A.T @ A)
                                bf = (CM @ A.T @ Y).T[0,:]
                                k = 0
                                for n,a in zip(p_names, p_active):
                                        if a:
                                                s[n] = bf[k]
#                                               self.msg(f&#39;{n} = {bf[k]}&#39;)
                                                k += 1
                                        else:
                                                s[n] = 0.
#                                               self.msg(f&#39;{n} = 0.0&#39;)

                                for r in sdata :
                                        a, b, c, a2, b2, c2 = s[&#39;a&#39;], s[&#39;b&#39;], s[&#39;c&#39;], s[&#39;a2&#39;], s[&#39;b2&#39;], s[&#39;c2&#39;]
                                        r[f&#39;D{self._4x}&#39;] = (r[f&#39;D{self._4x}raw&#39;] - c - b * r[f&#39;d{self._4x}&#39;] - c2 * r[&#39;t&#39;] - b2 * r[&#39;t&#39;] * r[f&#39;d{self._4x}&#39;]) / (a + a2 * r[&#39;t&#39;])
                                        r[f&#39;wD{self._4x}&#39;] /= (a + a2 * r[&#39;t&#39;])

                                s[&#39;CM&#39;] = np.zeros((6,6))
                                i = 0
                                k_active = [j for j,a in enumerate(p_active) if a]
                                for j,a in enumerate(p_active):
                                        if a:
                                                s[&#39;CM&#39;][j,k_active] = CM[i,:]
                                                i += 1

                        if not weighted_sessions:
                                w = self.rmswd()[&#39;rmswd&#39;]
                                for r in self:
                                                r[f&#39;wD{self._4x}&#39;] *= w
                                                r[f&#39;wD{self._4x}raw&#39;] *= w
                                for session in self.sessions:
                                        self.sessions[session][&#39;CM&#39;] *= w**2

                        for session in self.sessions:
                                s = self.sessions[session]
                                s[&#39;SE_a&#39;] = s[&#39;CM&#39;][0,0]**.5
                                s[&#39;SE_b&#39;] = s[&#39;CM&#39;][1,1]**.5
                                s[&#39;SE_c&#39;] = s[&#39;CM&#39;][2,2]**.5
                                s[&#39;SE_a2&#39;] = s[&#39;CM&#39;][3,3]**.5
                                s[&#39;SE_b2&#39;] = s[&#39;CM&#39;][4,4]**.5
                                s[&#39;SE_c2&#39;] = s[&#39;CM&#39;][5,5]**.5

                        if not weighted_sessions:
                                self.Nf = len(self) - len(self.unknowns) - np.sum([self.sessions[s][&#39;Np&#39;] for s in self.sessions])
                        else:
                                self.Nf = 0
                                for sg in weighted_sessions:
                                        self.Nf += self.rmswd(sessions = sg)[&#39;Nf&#39;]

                        self.t95 = tstudent.ppf(1 - 0.05/2, self.Nf)

                        avgD4x = {
                                sample: np.mean([r[f&#39;D{self._4x}&#39;] for r in self if r[&#39;Sample&#39;] == sample])
                                for sample in self.samples
                                }
                        chi2 = np.sum([(r[f&#39;D{self._4x}&#39;] - avgD4x[r[&#39;Sample&#39;]])**2 for r in self])
                        rD4x = (chi2/self.Nf)**.5
                        self.repeatability[f&#39;sigma_{self._4x}&#39;] = rD4x

                        if consolidate:
                                self.consolidate(tables = consolidate_tables, plots = consolidate_plots)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.standardize_d13C"><code class="name flex">
<span>def <span class="ident">standardize_d13C</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform δ<sup>13</sup>C standadization within each session <code>s</code> according to
<code>self.sessions[s]['d13C_standardization_method']</code>, which is defined by default
by <code><a title="D47crunch.D47data.refresh_sessions" href="#D47crunch.D4xdata.refresh_sessions">D4xdata.refresh_sessions()</a></code>as equal to <code>self.d13C_STANDARDIZATION_METHOD</code>, but
may be redefined abitrarily at a later stage.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def standardize_d13C(self):
        &#39;&#39;&#39;
        Perform δ&lt;sup&gt;13&lt;/sup&gt;C standadization within each session `s` according to
        `self.sessions[s][&#39;d13C_standardization_method&#39;]`, which is defined by default
        by `D47data.refresh_sessions()`as equal to `self.d13C_STANDARDIZATION_METHOD`, but
        may be redefined abitrarily at a later stage.
        &#39;&#39;&#39;
        for s in self.sessions:
                if self.sessions[s][&#39;d13C_standardization_method&#39;] in [&#39;1pt&#39;, &#39;2pt&#39;]:
                        XY = [(r[&#39;d13C_VPDB&#39;], self.Nominal_d13C_VPDB[r[&#39;Sample&#39;]]) for r in self.sessions[s][&#39;data&#39;] if r[&#39;Sample&#39;] in self.Nominal_d13C_VPDB]
                        X,Y = zip(*XY)
                        if self.sessions[s][&#39;d13C_standardization_method&#39;] == &#39;1pt&#39;:
                                offset = np.mean(Y) - np.mean(X)
                                for r in self.sessions[s][&#39;data&#39;]:
                                        r[&#39;d13C_VPDB&#39;] += offset                                
                        elif self.sessions[s][&#39;d13C_standardization_method&#39;] == &#39;2pt&#39;:
                                a,b = np.polyfit(X,Y,1)
                                for r in self.sessions[s][&#39;data&#39;]:
                                        r[&#39;d13C_VPDB&#39;] = a * r[&#39;d13C_VPDB&#39;] + b</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.standardize_d18O"><code class="name flex">
<span>def <span class="ident">standardize_d18O</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform δ<sup>18</sup>O standadization within each session <code>s</code> according to
<code>self.ALPHA_18O_ACID_REACTION</code> and <code>self.sessions[s]['d18O_standardization_method']</code>,
which is defined by default by <code><a title="D47crunch.D47data.refresh_sessions" href="#D47crunch.D4xdata.refresh_sessions">D4xdata.refresh_sessions()</a></code>as equal to
<code>self.d18O_STANDARDIZATION_METHOD</code>, but may be redefined abitrarily at a later stage.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def standardize_d18O(self):
        &#39;&#39;&#39;
        Perform δ&lt;sup&gt;18&lt;/sup&gt;O standadization within each session `s` according to
        `self.ALPHA_18O_ACID_REACTION` and `self.sessions[s][&#39;d18O_standardization_method&#39;]`,
        which is defined by default by `D47data.refresh_sessions()`as equal to
        `self.d18O_STANDARDIZATION_METHOD`, but may be redefined abitrarily at a later stage.
        &#39;&#39;&#39;
        for s in self.sessions:
                if self.sessions[s][&#39;d18O_standardization_method&#39;] in [&#39;1pt&#39;, &#39;2pt&#39;]:
                        XY = [(r[&#39;d18O_VSMOW&#39;], self.Nominal_d18O_VPDB[r[&#39;Sample&#39;]]) for r in self.sessions[s][&#39;data&#39;] if r[&#39;Sample&#39;] in self.Nominal_d18O_VPDB]
                        X,Y = zip(*XY)
                        Y = [(1000+y) * self.R18_VPDB * self.ALPHA_18O_ACID_REACTION / self.R18_VSMOW - 1000 for y in Y]
                        if self.sessions[s][&#39;d18O_standardization_method&#39;] == &#39;1pt&#39;:
                                offset = np.mean(Y) - np.mean(X)
                                for r in self.sessions[s][&#39;data&#39;]:
                                        r[&#39;d18O_VSMOW&#39;] += offset                               
                        elif self.sessions[s][&#39;d18O_standardization_method&#39;] == &#39;2pt&#39;:
                                a,b = np.polyfit(X,Y,1)
                                for r in self.sessions[s][&#39;data&#39;]:
                                        r[&#39;d18O_VSMOW&#39;] = a * r[&#39;d18O_VSMOW&#39;] + b</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.summary"><code class="name flex">
<span>def <span class="ident">summary</span></span>(<span>self, dir='output', filename=None, save_to_file=True, print_out=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Print out an/or save to disk a summary of the standardization results.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>dir</code>: the directory in which to save the table</li>
<li><code>filename</code>: the name to the csv file to write to</li>
<li><code>save_to_file</code>: whether to save the table to disk</li>
<li><code>print_out</code>: whether to print out the table</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@make_verbal
def summary(self,
        dir = &#39;output&#39;,
        filename = None,
        save_to_file = True,
        print_out = True,
        ):
        &#39;&#39;&#39;
        Print out an/or save to disk a summary of the standardization results.

        __Parameters__

        + `dir`: the directory in which to save the table
        + `filename`: the name to the csv file to write to
        + `save_to_file`: whether to save the table to disk
        + `print_out`: whether to print out the table
        &#39;&#39;&#39;

        out = []
        out += [[&#39;N samples (anchors + unknowns)&#39;, f&#34;{len(self.samples)} ({len(self.anchors)} + {len(self.unknowns)})&#34;]]
        out += [[&#39;N analyses (anchors + unknowns)&#39;, f&#34;{len(self)} ({len([r for r in self if r[&#39;Sample&#39;] in self.anchors])} + {len([r for r in self if r[&#39;Sample&#39;] in self.unknowns])})&#34;]]
        out += [[&#39;Repeatability of δ13C_VPDB&#39;, f&#34;{1000 * self.repeatability[&#39;r_d13C_VPDB&#39;]:.1f} ppm&#34;]]
        out += [[&#39;Repeatability of δ18O_VSMOW&#39;, f&#34;{1000 * self.repeatability[&#39;r_d18O_VSMOW&#39;]:.1f} ppm&#34;]]
        out += [[f&#39;Repeatability of Δ{self._4x} (anchors)&#39;, f&#34;{1000 * self.repeatability[f&#39;r_D{self._4x}a&#39;]:.1f} ppm&#34;]]
        out += [[f&#39;Repeatability of Δ{self._4x} (unknowns)&#39;, f&#34;{1000 * self.repeatability[f&#39;r_D{self._4x}u&#39;]:.1f} ppm&#34;]]
        out += [[f&#39;Repeatability of Δ{self._4x} (all)&#39;, f&#34;{1000 * self.repeatability[f&#39;r_D{self._4x}&#39;]:.1f} ppm&#34;]]
        out += [[&#39;Model degrees of freedom&#39;, f&#34;{self.Nf}&#34;]]
        out += [[&#39;Student\&#39;s 95% t-factor&#39;, f&#34;{self.t95:.2f}&#34;]]
        out += [[&#39;Standardization method&#39;, self.standardization_method]]

        if save_to_file:
                if not os.path.exists(dir):
                        os.makedirs(dir)
                if filename is None:
                        filename = f&#39;D{self._4x}_summary.csv&#39;
                with open(f&#39;{dir}/{filename}&#39;, &#39;w&#39;) as fid:
                        fid.write(make_csv(out))
        if print_out:
                self.msg(&#39;\n&#39; + pretty_table(out, header = 0))</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.table_of_analyses"><code class="name flex">
<span>def <span class="ident">table_of_analyses</span></span>(<span>self, dir='output', filename=None, save_to_file=True, print_out=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Print out an/or save to disk a table of analyses.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>dir</code>: the directory in which to save the table</li>
<li><code>filename</code>: the name to the csv file to write to</li>
<li><code>save_to_file</code>: whether to save the table to disk</li>
<li><code>print_out</code>: whether to print out the table</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@make_verbal
def table_of_analyses(
        self,
        dir = &#39;output&#39;,
        filename = None,
        save_to_file = True,
        print_out = True,
        ):
        &#39;&#39;&#39;
        Print out an/or save to disk a table of analyses.

        __Parameters__

        + `dir`: the directory in which to save the table
        + `filename`: the name to the csv file to write to
        + `save_to_file`: whether to save the table to disk
        + `print_out`: whether to print out the table
        &#39;&#39;&#39;

        out = [[&#39;UID&#39;,&#39;Session&#39;,&#39;Sample&#39;]]
        extra_fields = [f for f in [(&#39;SampleMass&#39;,&#39;.2f&#39;),(&#39;ColdFingerPressure&#39;,&#39;.1f&#39;),(&#39;AcidReactionYield&#39;,&#39;.3f&#39;)] if f[0] in {k for r in self for k in r}]
        for f in extra_fields:
                out[-1] += [f[0]]
        out[-1] += [&#39;d13Cwg_VPDB&#39;,&#39;d18Owg_VSMOW&#39;,&#39;d45&#39;,&#39;d46&#39;,&#39;d47&#39;,&#39;d48&#39;,&#39;d49&#39;,&#39;d13C_VPDB&#39;,&#39;d18O_VSMOW&#39;,&#39;D47raw&#39;,&#39;D48raw&#39;,&#39;D49raw&#39;,f&#39;D{self._4x}&#39;]
        for r in self:
                out += [[f&#34;{r[&#39;UID&#39;]}&#34;,f&#34;{r[&#39;Session&#39;]}&#34;,f&#34;{r[&#39;Sample&#39;]}&#34;]]
                for f in extra_fields:
                        out[-1] += [f&#34;{r[f[0]]:{f[1]}}&#34;]
                out[-1] += [
                        f&#34;{r[&#39;d13Cwg_VPDB&#39;]:.3f}&#34;,
                        f&#34;{r[&#39;d18Owg_VSMOW&#39;]:.3f}&#34;,
                        f&#34;{r[&#39;d45&#39;]:.6f}&#34;,
                        f&#34;{r[&#39;d46&#39;]:.6f}&#34;,
                        f&#34;{r[&#39;d47&#39;]:.6f}&#34;,
                        f&#34;{r[&#39;d48&#39;]:.6f}&#34;,
                        f&#34;{r[&#39;d49&#39;]:.6f}&#34;,
                        f&#34;{r[&#39;d13C_VPDB&#39;]:.6f}&#34;,
                        f&#34;{r[&#39;d18O_VSMOW&#39;]:.6f}&#34;,
                        f&#34;{r[&#39;D47raw&#39;]:.6f}&#34;,
                        f&#34;{r[&#39;D48raw&#39;]:.6f}&#34;,
                        f&#34;{r[&#39;D49raw&#39;]:.6f}&#34;,
                        f&#34;{r[f&#39;D{self._4x}&#39;]:.6f}&#34;
                        ]
        if save_to_file:
                if not os.path.exists(dir):
                        os.makedirs(dir)
                if filename is None:
                        filename = f&#39;D{self._4x}_analyses.csv&#39;
                with open(f&#39;{dir}/{filename}&#39;, &#39;w&#39;) as fid:
                        fid.write(make_csv(out))
        if print_out:
                self.msg(&#39;\n&#39; + pretty_table(out))
        return out</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.table_of_samples"><code class="name flex">
<span>def <span class="ident">table_of_samples</span></span>(<span>self, dir='output', filename=None, save_to_file=True, print_out=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Print out an/or save to disk a table of samples.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>dir</code>: the directory in which to save the table</li>
<li><code>filename</code>: the name to the csv file to write to</li>
<li><code>save_to_file</code>: whether to save the table to disk</li>
<li><code>print_out</code>: whether to print out the table</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@make_verbal
def table_of_samples(
        self,
        dir = &#39;output&#39;,
        filename = None,
        save_to_file = True,
        print_out = True,
        ):
        &#39;&#39;&#39;
        Print out an/or save to disk a table of samples.

        __Parameters__

        + `dir`: the directory in which to save the table
        + `filename`: the name to the csv file to write to
        + `save_to_file`: whether to save the table to disk
        + `print_out`: whether to print out the table
        &#39;&#39;&#39;

        out = [[&#39;Sample&#39;,&#39;N&#39;,&#39;d13C_VPDB&#39;,&#39;d18O_VSMOW&#39;,f&#39;D{self._4x}&#39;,&#39;SE&#39;,&#39;95% CL&#39;,&#39;SD&#39;,&#39;p_Levene&#39;]]
        for sample in self.anchors:
                out += [[
                        f&#34;{sample}&#34;,
                        f&#34;{self.samples[sample][&#39;N&#39;]}&#34;,
                        f&#34;{self.samples[sample][&#39;d13C_VPDB&#39;]:.2f}&#34;,
                        f&#34;{self.samples[sample][&#39;d18O_VSMOW&#39;]:.2f}&#34;,
                        f&#34;{self.samples[sample][f&#39;D{self._4x}&#39;]:.4f}&#34;,&#39;&#39;,&#39;&#39;,
                        f&#34;{self.samples[sample][f&#39;SD_D{self._4x}&#39;]:.4f}&#34; if self.samples[sample][&#39;N&#39;] &gt; 1 else &#39;&#39;, &#39;&#39;
                        ]]
        for sample in self.unknowns:
                out += [[
                        f&#34;{sample}&#34;,
                        f&#34;{self.samples[sample][&#39;N&#39;]}&#34;,
                        f&#34;{self.samples[sample][&#39;d13C_VPDB&#39;]:.2f}&#34;,
                        f&#34;{self.samples[sample][&#39;d18O_VSMOW&#39;]:.2f}&#34;,
                        f&#34;{self.samples[sample][f&#39;D{self._4x}&#39;]:.4f}&#34;,
                        f&#34;{self.samples[sample][f&#39;SE_D{self._4x}&#39;]:.4f}&#34;,
                        f&#34;± {self.samples[sample][f&#39;SE_D{self._4x}&#39;] * self.t95:.4f}&#34;,
                        f&#34;{self.samples[sample][f&#39;SD_D{self._4x}&#39;]:.4f}&#34; if self.samples[sample][&#39;N&#39;] &gt; 1 else &#39;&#39;,
                        f&#34;{self.samples[sample][&#39;p_Levene&#39;]:.3f}&#34; if self.samples[sample][&#39;N&#39;] &gt; 2 else &#39;&#39;
                        ]]
        if save_to_file:
                if not os.path.exists(dir):
                        os.makedirs(dir)
                if filename is None:
                        filename = f&#39;D{self._4x}_samples.csv&#39;
                with open(f&#39;{dir}/{filename}&#39;, &#39;w&#39;) as fid:
                        fid.write(make_csv(out))
        if print_out:
                self.msg(&#39;\n&#39;+pretty_table(out))</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.table_of_sessions"><code class="name flex">
<span>def <span class="ident">table_of_sessions</span></span>(<span>self, dir='output', filename=None, save_to_file=True, print_out=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Print out an/or save to disk a table of sessions.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>dir</code>: the directory in which to save the table</li>
<li><code>filename</code>: the name to the csv file to write to</li>
<li><code>save_to_file</code>: whether to save the table to disk</li>
<li><code>print_out</code>: whether to print out the table</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@make_verbal
def table_of_sessions(self,
        dir = &#39;output&#39;,
        filename = None,
        save_to_file = True,
        print_out = True,
        ):
        &#39;&#39;&#39;
        Print out an/or save to disk a table of sessions.

        __Parameters__

        + `dir`: the directory in which to save the table
        + `filename`: the name to the csv file to write to
        + `save_to_file`: whether to save the table to disk
        + `print_out`: whether to print out the table
        &#39;&#39;&#39;
        include_a2 = any([self.sessions[session][&#39;scrambling_drift&#39;] for session in self.sessions])
        include_b2 = any([self.sessions[session][&#39;slope_drift&#39;] for session in self.sessions])
        include_c2 = any([self.sessions[session][&#39;wg_drift&#39;] for session in self.sessions])

        out = [[&#39;Session&#39;,&#39;Na&#39;,&#39;Nu&#39;,&#39;d13Cwg_VPDB&#39;,&#39;d18Owg_VSMOW&#39;,&#39;r_d13C&#39;,&#39;r_d18O&#39;,f&#39;r_D{self._4x}&#39;,&#39;a ± SE&#39;,&#39;1e3 x b ± SE&#39;,&#39;c ± SE&#39;]]
        if include_a2:
                out[-1] += [&#39;a2 ± SE&#39;]
        if include_b2:
                out[-1] += [&#39;b2 ± SE&#39;]
        if include_c2:
                out[-1] += [&#39;c2 ± SE&#39;]
        for session in self.sessions:
                out += [[
                        session,
                        f&#34;{self.sessions[session][&#39;Na&#39;]}&#34;,
                        f&#34;{self.sessions[session][&#39;Nu&#39;]}&#34;,
                        f&#34;{self.sessions[session][&#39;d13Cwg_VPDB&#39;]:.3f}&#34;,
                        f&#34;{self.sessions[session][&#39;d18Owg_VSMOW&#39;]:.3f}&#34;,
                        f&#34;{self.sessions[session][&#39;r_d13C_VPDB&#39;]:.4f}&#34;,
                        f&#34;{self.sessions[session][&#39;r_d18O_VSMOW&#39;]:.4f}&#34;,
                        f&#34;{self.sessions[session][f&#39;r_D{self._4x}&#39;]:.4f}&#34;,
                        f&#34;{self.sessions[session][&#39;a&#39;]:.3f} ± {self.sessions[session][&#39;SE_a&#39;]:.3f}&#34;,
                        f&#34;{1e3*self.sessions[session][&#39;b&#39;]:.3f} ± {1e3*self.sessions[session][&#39;SE_b&#39;]:.3f}&#34;,
                        f&#34;{self.sessions[session][&#39;c&#39;]:.3f} ± {self.sessions[session][&#39;SE_c&#39;]:.3f}&#34;,
                        ]]
                if include_a2:
                        if self.sessions[session][&#39;scrambling_drift&#39;]:
                                out[-1] += [f&#34;{self.sessions[session][&#39;a2&#39;]:.1e} ± {self.sessions[session][&#39;SE_a2&#39;]:.1e}&#34;]
                        else:
                                out[-1] += [&#39;&#39;]
                if include_b2:
                        if self.sessions[session][&#39;slope_drift&#39;]:
                                out[-1] += [f&#34;{self.sessions[session][&#39;b2&#39;]:.1e} ± {self.sessions[session][&#39;SE_b2&#39;]:.1e}&#34;]
                        else:
                                out[-1] += [&#39;&#39;]
                if include_c2:
                        if self.sessions[session][&#39;wg_drift&#39;]:
                                out[-1] += [f&#34;{self.sessions[session][&#39;c2&#39;]:.1e} ± {self.sessions[session][&#39;SE_c2&#39;]:.1e}&#34;]
                        else:
                                out[-1] += [&#39;&#39;]

        if save_to_file:
                if not os.path.exists(dir):
                        os.makedirs(dir)
                if filename is None:
                        filename = f&#39;D{self._4x}_sessions.csv&#39;
                with open(f&#39;{dir}/{filename}&#39;, &#39;w&#39;) as fid:
                        fid.write(make_csv(out))
        if print_out:
                self.msg(&#39;\n&#39; + pretty_table(out))
        return out</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.unsplit_samples"><code class="name flex">
<span>def <span class="ident">unsplit_samples</span></span>(<span>self, tables=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Reverse the effects of <code><a title="D47crunch.D47data.split_samples" href="#D47crunch.D4xdata.split_samples">D4xdata.split_samples()</a></code>.</p>
<p>This should only be used after <code><a title="D47crunch.D4xdata.standardize" href="#D47crunch.D4xdata.standardize">D4xdata.standardize()</a></code> with <code>method='pooled'</code>.</p>
<p>After <code><a title="D47crunch.D4xdata.standardize" href="#D47crunch.D4xdata.standardize">D4xdata.standardize()</a></code> with <code>method='indep_sessions'</code>, one should
probably use <code><a title="D47crunch.D4xdata.combine_samples" href="#D47crunch.D4xdata.combine_samples">D4xdata.combine_samples()</a></code> instead to reverse the effects of
<code><a title="D47crunch.D47data.split_samples" href="#D47crunch.D4xdata.split_samples">D4xdata.split_samples()</a></code> with <code>grouping='by_uid'</code>, or <code><a title="D47crunch.w_avg" href="#D47crunch.w_avg">w_avg()</a></code> to reverse the
effects of <code><a title="D47crunch.D47data.split_samples" href="#D47crunch.D4xdata.split_samples">D4xdata.split_samples()</a></code> with <code>grouping='by_sessions'</code> (because in
that case session-averaged Δ<sub>4x</sub> values are statistically independent).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unsplit_samples(self, tables = False):
        &#39;&#39;&#39;
        Reverse the effects of `D47data.split_samples()`.
        
        This should only be used after `D4xdata.standardize()` with `method=&#39;pooled&#39;`.
        
        After `D4xdata.standardize()` with `method=&#39;indep_sessions&#39;`, one should
        probably use `D4xdata.combine_samples()` instead to reverse the effects of
        `D47data.split_samples()` with `grouping=&#39;by_uid&#39;`, or `w_avg()` to reverse the
        effects of `D47data.split_samples()` with `grouping=&#39;by_sessions&#39;` (because in
        that case session-averaged Δ&lt;sub&gt;4x&lt;/sub&gt; values are statistically independent).
        &#39;&#39;&#39;
        unknowns_old = sorted({s for s in self.unknowns})
        CM_old = self.standardization.covar[:,:]
        VD_old = self.standardization.params.valuesdict().copy()
        vars_old = self.standardization.var_names

        unknowns_new = sorted({r[&#39;Sample_original&#39;] for r in self if &#39;Sample_original&#39; in r})

        Ns = len(vars_old) - len(unknowns_old)
        vars_new = vars_old[:Ns] + [f&#39;D{self._4x}_{pf(u)}&#39; for u in unknowns_new]
        VD_new = {k: VD_old[k] for k in vars_old[:Ns]}

        W = np.zeros((len(vars_new), len(vars_old)))
        W[:Ns,:Ns] = np.eye(Ns)
        for u in unknowns_new:
                splits = sorted({r[&#39;Sample&#39;] for r in self if &#39;Sample_original&#39; in r and r[&#39;Sample_original&#39;] == u})
                if self.grouping == &#39;by_session&#39;:
                        weights = [self.samples[s][f&#39;SE_D{self._4x}&#39;]**-2 for s in splits]
                elif self.grouping == &#39;by_uid&#39;:
                        weights = [1 for s in splits]
                sw = sum(weights)
                weights = [w/sw for w in weights]
                W[vars_new.index(f&#39;D{self._4x}_{pf(u)}&#39;),[vars_old.index(f&#39;D{self._4x}_{pf(s)}&#39;) for s in splits]] = weights[:]

        CM_new = W @ CM_old @ W.T
        V = W @ np.array([[VD_old[k]] for k in vars_old])
        VD_new = {k:v[0] for k,v in zip(vars_new, V)}

        self.standardization.covar = CM_new
        self.standardization.params.valuesdict = lambda : VD_new
        self.standardization.var_names = vars_new

        for r in self:
                if r[&#39;Sample&#39;] in self.unknowns:
                        r[&#39;Sample_split&#39;] = r[&#39;Sample&#39;]
                        r[&#39;Sample&#39;] = r[&#39;Sample_original&#39;]

        self.refresh_samples()
        self.consolidate_samples()
        self.repeatabilities()

        if tables:
                self.table_of_analyses()
                self.table_of_samples()</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.vmsg"><code class="name flex">
<span>def <span class="ident">vmsg</span></span>(<span>self, txt)</span>
</code></dt>
<dd>
<div class="desc"><p>Log a message to <code>self.logfile</code> and print it out</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vmsg(self, txt):
        &#39;&#39;&#39;
        Log a message to `self.logfile` and print it out
        &#39;&#39;&#39;
        self.log(txt)
        print(txt)</code></pre>
</details>
</dd>
<dt id="D47crunch.D4xdata.wg"><code class="name flex">
<span>def <span class="ident">wg</span></span>(<span>self, samples=None, a18_acid=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute bulk composition of the working gas for each session based on
the carbonate standards defined in both <code>self.Nominal_d13C_VPDB</code> and
<code>self.Nominal_d18O_VPDB</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">        @make_verbal
        def wg(self, samples = None, a18_acid = None):
                &#39;&#39;&#39;
                Compute bulk composition of the working gas for each session based on
                the carbonate standards defined in both `self.Nominal_d13C_VPDB` and
                `self.Nominal_d18O_VPDB`.
                &#39;&#39;&#39;

                self.msg(&#39;Computing WG composition:&#39;)

                if a18_acid is None:
                        a18_acid = self.ALPHA_18O_ACID_REACTION
                if samples is None:
                        samples = [s for s in self.Nominal_d13C_VPDB if s in self.Nominal_d18O_VPDB]

                assert a18_acid, f&#39;Acid fractionation factor should not be zero.&#39;

                samples = [s for s in samples if s in self.Nominal_d13C_VPDB and s in self.Nominal_d18O_VPDB]
                R45R46_standards = {}
                for sample in samples:
                        d13C_vpdb = self.Nominal_d13C_VPDB[sample]
                        d18O_vpdb = self.Nominal_d18O_VPDB[sample]
                        R13_s = self.R13_VPDB * (1 + d13C_vpdb / 1000)
                        R17_s = self.R17_VPDB * ((1 + d18O_vpdb / 1000) * a18_acid) ** self.lambda_17
                        R18_s = self.R18_VPDB * (1 + d18O_vpdb / 1000) * a18_acid

                        C12_s = 1 / (1 + R13_s)
                        C13_s = R13_s / (1 + R13_s)
                        C16_s = 1 / (1 + R17_s + R18_s)
                        C17_s = R17_s / (1 + R17_s + R18_s)
                        C18_s = R18_s / (1 + R17_s + R18_s)

                        C626_s = C12_s * C16_s ** 2
                        C627_s = 2 * C12_s * C16_s * C17_s
                        C628_s = 2 * C12_s * C16_s * C18_s
                        C636_s = C13_s * C16_s ** 2
                        C637_s = 2 * C13_s * C16_s * C17_s
                        C727_s = C12_s * C17_s ** 2

                        R45_s = (C627_s + C636_s) / C626_s
                        R46_s = (C628_s + C637_s + C727_s) / C626_s
                        R45R46_standards[sample] = (R45_s, R46_s)
                
                for s in self.sessions:
                        db = [r for r in self.sessions[s][&#39;data&#39;] if r[&#39;Sample&#39;] in samples]
                        assert db, f&#39;No sample from {samples} found in session &#34;{s}&#34;.&#39;
#                       dbsamples = sorted({r[&#39;Sample&#39;] for r in db})

                        X = [r[&#39;d45&#39;] for r in db]
                        Y = [R45R46_standards[r[&#39;Sample&#39;]][0] for r in db]
                        x1, x2 = np.min(X), np.max(X)

                        if x1 &lt; x2:
                                wgcoord = x1/(x1-x2)
                        else:
                                wgcoord = 999

                        if wgcoord &lt; -.5 or wgcoord &gt; 1.5:
                                # unreasonable to extrapolate to d45 = 0
                                R45_wg = np.mean([y/(1+x/1000) for x,y in zip(X,Y)])
                        else :
                                # d45 = 0 is reasonably well bracketed
                                R45_wg = np.polyfit(X, Y, 1)[1]

                        X = [r[&#39;d46&#39;] for r in db]
                        Y = [R45R46_standards[r[&#39;Sample&#39;]][1] for r in db]
                        x1, x2 = np.min(X), np.max(X)

                        if x1 &lt; x2:
                                wgcoord = x1/(x1-x2)
                        else:
                                wgcoord = 999

                        if wgcoord &lt; -.5 or wgcoord &gt; 1.5:
                                # unreasonable to extrapolate to d46 = 0
                                R46_wg = np.mean([y/(1+x/1000) for x,y in zip(X,Y)])
                        else :
                                # d46 = 0 is reasonably well bracketed
                                R46_wg = np.polyfit(X, Y, 1)[1]

                        d13Cwg_VPDB, d18Owg_VSMOW = self.compute_bulk_delta(R45_wg, R46_wg)

                        self.msg(f&#39;Session {s} WG:   δ13C_VPDB = {d13Cwg_VPDB:.3f}   δ18O_VSMOW = {d18Owg_VSMOW:.3f}&#39;)

                        self.sessions[s][&#39;d13Cwg_VPDB&#39;] = d13Cwg_VPDB
                        self.sessions[s][&#39;d18Owg_VSMOW&#39;] = d18Owg_VSMOW
                        for r in self.sessions[s][&#39;data&#39;]:
                                r[&#39;d13Cwg_VPDB&#39;] = d13Cwg_VPDB
                                r[&#39;d18Owg_VSMOW&#39;] = d18Owg_VSMOW</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="D47crunch.SessionPlot"><code class="flex name class">
<span>class <span class="ident">SessionPlot</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SessionPlot():
        def __init__(self):
                pass</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#usage">Usage</a><ul>
<li><a href="#1-create-d47data-object">1. Create D47data object</a><ul>
<li><a href="#11-nominal-13cvpdb-18ovpdb-and-47-values-of-carbonate-standards">1.1 Nominal δ13CVPDB, δ18OVPDB, and Δ47 values of carbonate standards</a></li>
<li><a href="#12-oxygen-17-correction-parameters">1.2 Oxygen-17 correction parameters</a></li>
<li><a href="#13-default-method-for-carbon-13-and-oxygen-18-standardization">1.3 Default method for carbon-13 and oxygen-18 standardization</a></li>
<li><a href="#14-oxygen-18-acid-fractionation-factor">1.4 Oxygen-18 acid fractionation factor</a></li>
</ul>
</li>
<li><a href="#2-import-data">2. Import data</a><ul>
<li><a href="#21-sessions">2.1 Sessions</a></li>
<li><a href="#22-samples-anchors-and-unknowns">2.2 Samples, anchors and unknowns</a></li>
</ul>
</li>
<li><a href="#3-working-gas-composition">3. Working gas composition</a><ul>
<li><a href="#31-option-1-explicit-definition">3.1 Option 1: explicit definition</a></li>
<li><a href="#32-option-2-based-on-the-known-composition-of-a-sample">3.2 Option 2: based on the known composition of a sample:</a></li>
</ul>
</li>
<li><a href="#4-crunch-the-data">4. Crunch the data</a></li>
<li><a href="#6-standardization">6. Standardization</a><ul>
<li><a href="#61-default-approach-pooled">6.1 Default approach (pooled)</a></li>
<li><a href="#62-d47datasessions">6.2 D47data().sessions</a></li>
<li><a href="#63-d47datasamples-d47dataanchors-and-d47dataunknowns">6.3 D47data().samples, D47data().anchors, and D47data().unknowns</a></li>
<li><a href="#64-d47datarepeatability">6.4 D47data.()repeatability</a></li>
<li><a href="#65-d47dataresult">6.5 D47data.()result</a></li>
<li><a href="#66-combining-information-from-carbonate-anchors-and-equilibrated-gases">6.6 Combining information from carbonate anchors and equilibrated gases</a></li>
<li><a href="#67-legacy-standardization-approach-indep_sessions">6.7 Legacy standardization approach (indep_sessions)</a></li>
</ul>
</li>
<li><a href="#7-viewing-and-saving-the-results">7. Viewing and saving the results</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="D47crunch.correlated_sum" href="#D47crunch.correlated_sum">correlated_sum</a></code></li>
<li><code><a title="D47crunch.fCO2eqD47_Petersen" href="#D47crunch.fCO2eqD47_Petersen">fCO2eqD47_Petersen</a></code></li>
<li><code><a title="D47crunch.fCO2eqD47_Wang" href="#D47crunch.fCO2eqD47_Wang">fCO2eqD47_Wang</a></code></li>
<li><code><a title="D47crunch.make_csv" href="#D47crunch.make_csv">make_csv</a></code></li>
<li><code><a title="D47crunch.pf" href="#D47crunch.pf">pf</a></code></li>
<li><code><a title="D47crunch.pretty_table" href="#D47crunch.pretty_table">pretty_table</a></code></li>
<li><code><a title="D47crunch.read_csv" href="#D47crunch.read_csv">read_csv</a></code></li>
<li><code><a title="D47crunch.smart_type" href="#D47crunch.smart_type">smart_type</a></code></li>
<li><code><a title="D47crunch.transpose_table" href="#D47crunch.transpose_table">transpose_table</a></code></li>
<li><code><a title="D47crunch.w_avg" href="#D47crunch.w_avg">w_avg</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="D47crunch.D47data" href="#D47crunch.D47data">D47data</a></code></h4>
<ul class="">
<li><code><a title="D47crunch.D47data.D47fromTeq" href="#D47crunch.D47data.D47fromTeq">D47fromTeq</a></code></li>
<li><code><a title="D47crunch.D47data.Nominal_D47" href="#D47crunch.D47data.Nominal_D47">Nominal_D47</a></code></li>
<li><code><a title="D47crunch.D47data.Nominal_D4x" href="#D47crunch.D47data.Nominal_D4x">Nominal_D4x</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="D47crunch.D48data" href="#D47crunch.D48data">D48data</a></code></h4>
<ul class="">
<li><code><a title="D47crunch.D48data.Nominal_D48" href="#D47crunch.D48data.Nominal_D48">Nominal_D48</a></code></li>
<li><code><a title="D47crunch.D48data.Nominal_D4x" href="#D47crunch.D48data.Nominal_D4x">Nominal_D4x</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="D47crunch.D4xdata" href="#D47crunch.D4xdata">D4xdata</a></code></h4>
<ul class="">
<li><code><a title="D47crunch.D4xdata.ALPHA_18O_ACID_REACTION" href="#D47crunch.D4xdata.ALPHA_18O_ACID_REACTION">ALPHA_18O_ACID_REACTION</a></code></li>
<li><code><a title="D47crunch.D4xdata.LEVENE_REF_SAMPLE" href="#D47crunch.D4xdata.LEVENE_REF_SAMPLE">LEVENE_REF_SAMPLE</a></code></li>
<li><code><a title="D47crunch.D4xdata.Nominal_d13C_VPDB" href="#D47crunch.D4xdata.Nominal_d13C_VPDB">Nominal_d13C_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.Nominal_d18O_VPDB" href="#D47crunch.D4xdata.Nominal_d18O_VPDB">Nominal_d18O_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.R13_VPDB" href="#D47crunch.D4xdata.R13_VPDB">R13_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.R17_VPDB" href="#D47crunch.D4xdata.R17_VPDB">R17_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.R17_VSMOW" href="#D47crunch.D4xdata.R17_VSMOW">R17_VSMOW</a></code></li>
<li><code><a title="D47crunch.D4xdata.R18_VPDB" href="#D47crunch.D4xdata.R18_VPDB">R18_VPDB</a></code></li>
<li><code><a title="D47crunch.D4xdata.R18_VSMOW" href="#D47crunch.D4xdata.R18_VSMOW">R18_VSMOW</a></code></li>
<li><code><a title="D47crunch.D4xdata.assign_timestamps" href="#D47crunch.D4xdata.assign_timestamps">assign_timestamps</a></code></li>
<li><code><a title="D47crunch.D4xdata.combine_samples" href="#D47crunch.D4xdata.combine_samples">combine_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.compute_bulk_and_clumping_deltas" href="#D47crunch.D4xdata.compute_bulk_and_clumping_deltas">compute_bulk_and_clumping_deltas</a></code></li>
<li><code><a title="D47crunch.D4xdata.compute_bulk_delta" href="#D47crunch.D4xdata.compute_bulk_delta">compute_bulk_delta</a></code></li>
<li><code><a title="D47crunch.D4xdata.compute_isobar_ratios" href="#D47crunch.D4xdata.compute_isobar_ratios">compute_isobar_ratios</a></code></li>
<li><code><a title="D47crunch.D4xdata.compute_r" href="#D47crunch.D4xdata.compute_r">compute_r</a></code></li>
<li><code><a title="D47crunch.D4xdata.consolidate" href="#D47crunch.D4xdata.consolidate">consolidate</a></code></li>
<li><code><a title="D47crunch.D4xdata.consolidate_samples" href="#D47crunch.D4xdata.consolidate_samples">consolidate_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.consolidate_sessions" href="#D47crunch.D4xdata.consolidate_sessions">consolidate_sessions</a></code></li>
<li><code><a title="D47crunch.D4xdata.crunch" href="#D47crunch.D4xdata.crunch">crunch</a></code></li>
<li><code><a title="D47crunch.D4xdata.d13C_STANDARDIZATION_METHOD" href="#D47crunch.D4xdata.d13C_STANDARDIZATION_METHOD">d13C_STANDARDIZATION_METHOD</a></code></li>
<li><code><a title="D47crunch.D4xdata.d18O_STANDARDIZATION_METHOD" href="#D47crunch.D4xdata.d18O_STANDARDIZATION_METHOD">d18O_STANDARDIZATION_METHOD</a></code></li>
<li><code><a title="D47crunch.D4xdata.fill_in_missing_info" href="#D47crunch.D4xdata.fill_in_missing_info">fill_in_missing_info</a></code></li>
<li><code><a title="D47crunch.D4xdata.input" href="#D47crunch.D4xdata.input">input</a></code></li>
<li><code><a title="D47crunch.D4xdata.lambda_17" href="#D47crunch.D4xdata.lambda_17">lambda_17</a></code></li>
<li><code><a title="D47crunch.D4xdata.log" href="#D47crunch.D4xdata.log">log</a></code></li>
<li><code><a title="D47crunch.D4xdata.make_verbal" href="#D47crunch.D4xdata.make_verbal">make_verbal</a></code></li>
<li><code><a title="D47crunch.D4xdata.msg" href="#D47crunch.D4xdata.msg">msg</a></code></li>
<li><code><a title="D47crunch.D4xdata.plot_distribution_of_analyses" href="#D47crunch.D4xdata.plot_distribution_of_analyses">plot_distribution_of_analyses</a></code></li>
<li><code><a title="D47crunch.D4xdata.plot_residuals" href="#D47crunch.D4xdata.plot_residuals">plot_residuals</a></code></li>
<li><code><a title="D47crunch.D4xdata.plot_sessions" href="#D47crunch.D4xdata.plot_sessions">plot_sessions</a></code></li>
<li><code><a title="D47crunch.D4xdata.plot_single_session" href="#D47crunch.D4xdata.plot_single_session">plot_single_session</a></code></li>
<li><code><a title="D47crunch.D4xdata.read" href="#D47crunch.D4xdata.read">read</a></code></li>
<li><code><a title="D47crunch.D4xdata.refresh" href="#D47crunch.D4xdata.refresh">refresh</a></code></li>
<li><code><a title="D47crunch.D4xdata.refresh_samples" href="#D47crunch.D4xdata.refresh_samples">refresh_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.refresh_sessions" href="#D47crunch.D4xdata.refresh_sessions">refresh_sessions</a></code></li>
<li><code><a title="D47crunch.D4xdata.repeatabilities" href="#D47crunch.D4xdata.repeatabilities">repeatabilities</a></code></li>
<li><code><a title="D47crunch.D4xdata.report" href="#D47crunch.D4xdata.report">report</a></code></li>
<li><code><a title="D47crunch.D4xdata.rmswd" href="#D47crunch.D4xdata.rmswd">rmswd</a></code></li>
<li><code><a title="D47crunch.D4xdata.sample_D4x_correl" href="#D47crunch.D4xdata.sample_D4x_correl">sample_D4x_correl</a></code></li>
<li><code><a title="D47crunch.D4xdata.sample_D4x_covar" href="#D47crunch.D4xdata.sample_D4x_covar">sample_D4x_covar</a></code></li>
<li><code><a title="D47crunch.D4xdata.sample_average" href="#D47crunch.D4xdata.sample_average">sample_average</a></code></li>
<li><code><a title="D47crunch.D4xdata.simulate" href="#D47crunch.D4xdata.simulate">simulate</a></code></li>
<li><code><a title="D47crunch.D4xdata.split_samples" href="#D47crunch.D4xdata.split_samples">split_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.standardization_error" href="#D47crunch.D4xdata.standardization_error">standardization_error</a></code></li>
<li><code><a title="D47crunch.D4xdata.standardize" href="#D47crunch.D4xdata.standardize">standardize</a></code></li>
<li><code><a title="D47crunch.D4xdata.standardize_d13C" href="#D47crunch.D4xdata.standardize_d13C">standardize_d13C</a></code></li>
<li><code><a title="D47crunch.D4xdata.standardize_d18O" href="#D47crunch.D4xdata.standardize_d18O">standardize_d18O</a></code></li>
<li><code><a title="D47crunch.D4xdata.summary" href="#D47crunch.D4xdata.summary">summary</a></code></li>
<li><code><a title="D47crunch.D4xdata.table_of_analyses" href="#D47crunch.D4xdata.table_of_analyses">table_of_analyses</a></code></li>
<li><code><a title="D47crunch.D4xdata.table_of_samples" href="#D47crunch.D4xdata.table_of_samples">table_of_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.table_of_sessions" href="#D47crunch.D4xdata.table_of_sessions">table_of_sessions</a></code></li>
<li><code><a title="D47crunch.D4xdata.unsplit_samples" href="#D47crunch.D4xdata.unsplit_samples">unsplit_samples</a></code></li>
<li><code><a title="D47crunch.D4xdata.vmsg" href="#D47crunch.D4xdata.vmsg">vmsg</a></code></li>
<li><code><a title="D47crunch.D4xdata.wg" href="#D47crunch.D4xdata.wg">wg</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="D47crunch.SessionPlot" href="#D47crunch.SessionPlot">SessionPlot</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>